# 2.1 数据集的划分方法

## 数据集划分的基本要求

### 数据集划分

分为训练集和测试集两部分。

- 原则：测试集应尽可能与训练集互斥，测试样本不在训练集中出现
- 目标：将数据集**_D_**划分为训练集**_S_**和测试集**_T_**两部分，在训练集上建立模型，在测试集上评估性能
- 假设：测试样本从原样本真实分布中独立同分布采样得到

### 性能度量

模型在测试集（新样本）上进行度量，也叫**泛化性能**。

## 数据集的划分方法

### 留出法（Hold-out）

从数据集 $D$ 中随机采样一部分数据作为训练集 $S$（通常约 2/3），剩余部分作为测试集 $T$（通常约 1/3），在训练集上训练模型，在测试集上评估性能。

**适用场景**：数据量充足的场景。

**关键特点**：操作简单、计算成本低；但受随机划分影响大，数据利用效率较低（测试集未参与模型训练）。

### 随机子抽样（Random Sub-sampling）

重复执行留出法 $k$ 次，每次随机划分不同的训练集和测试集，最终泛化性能取 $k$ 次评估结果（如准确率）的均值。

**适用场景**：数据量充足，需要减少单次随机划分带来的评估误差的场景。

**关键特点**：通过多次随机划分取均值，能够降低单次随机划分带来的评估误差，提高评估结果的稳定性；但计算成本随 $k$ 增大而增加。

### 自助法（Bootstrapping）

从原始数据集 $D$（样本数为 $|D|$）中有放回地均匀抽样，共抽样 $|D|$ 次组成训练集 $S$，原始数据中未被抽中的样本作为测试集 $T$。

**适用场景**：数据量较少，难以通过留出法 / 交叉验证划分的场景。

**关键概率特性**：单个样本在 $|D|$ 次抽样中始终未被选中的概率为 $(1-\frac{1}{|D|})^{|D|}$，当 $|D|$ 趋近于无穷大时，该概率趋近于 $e^{-1} \approx 0.368$，即测试集约占原始数据的 36.8%。

**关键特点**：充分利用有限数据，训练集包含重复数据，大小与原始数据一致。

### k 折交叉验证（k-fold Cross-Validation）

将原始数据集 $D$ 划分为 $k$ 个大小相似、互不相交的子集（称为 "折"，记为 $S_1, S_2, ..., S_k$），迭代训练与测试 $k$ 次：第 $i$ 次迭代中，以第 $i$ 折 $S_i$ 作为测试集，剩余 $k-1$ 折合并作为训练集，最终泛化性能取 $k$ 次测试结果的均值。

**适用场景**：数据量相对较少，需平衡数据利用效率与评估稳定性的场景。

**特殊形式**：留一法（Leave-one-out, LOO）。当 $k$ 等于原始数据集的样本总数 $|D|$ 时，每个样本单独作为一 "折"（测试集），其余 $|D|-1$ 个样本作为训练集。特点：无随机划分误差，评估结果最稳定；但计算成本极高（需训练 $|D|$ 次模型），仅适用于极小数据集。

**关键特点**：数据利用效率高（几乎所有数据均参与训练），评估结果稳定；但计算成本随 $k$ 增大而增加（常用 $k=5$ 或 $k=10$ 平衡效率与稳定性）。

### 训练 - 测试集（Train-Test）二分

仅将数据集划分为训练集 $S$ 和测试集 $T$ 两部分。

**适用场景**：无需调整超参数的简单模型或数据量极大的场景。

**局限性**：无法单独验证超参数（如正则化系数、模型复杂度），易因超参数选择不当导致过拟合。

### 训练 - 验证 - 测试集（Train-Val-Test）三分

将数据集划分为三部分：训练集 $S$ 用于模型核心参数的训练，验证集 $V$ 用于调整模型超参数（如 k 近邻的 $k$ 值、正则化系数 $\lambda$）并筛选最优模型结构，测试集 $T$ 仅用于评估最终最优模型的泛化性能且不参与任何模型调整过程。

超参数调优流程：在训练集上训练不同超参数配置的模型，在验证集上评估并选择最优超参数，用最优超参数训练最终模型后在测试集上评估。

**适用场景**：大规模数据集（如 ImageNet、COCO）或需要精细调整超参数的复杂模型（如深度学习模型）。

# 2.2 模型的性能度量

## 性能度量核心定义

模型的性能度量以泛化性能为核心，即模型在测试集（新样本）上的表现，不同任务（分类、回归）因输出类型不同，采用差异化的评估指标。

## 分类任务性能度量

分类任务的输出为“正确”或“错误”（二分类场景为主），仅定性评价分类结果是否准确，无法直接量化预测值与真实值的数值差异。

### 混淆矩阵

- 假负例（FN，False Negative）：实际为正类但被错误地判为负类的样本。
- 真负例（TN，True Negative）：实际为负类且被正确判为负类的样本。
- 假正例（FP，False Positive）：实际为负类却被错误地判为正类的样本。
- 真阳性（TP，True Positive）：实际为正类且被正确判为正类的样本。

### 常用性能指标及公式

- 准确率：分类器预测正确的样本数量占总样本数量的比例，体现模型整体的预测准确性。
- 错误率：分类器预测错误的样本数量占总样本数量的比例，与准确率互为补充。
- 查全率（Recall）/敏感性（Sensitivity）：被分类器成功识别为正类的真实正类样本数量，占所有真实正类样本的比例，衡量模型发现正类的全面性。
- 查准率（Precision）：分类器预测为正类的所有样本中，真实为正类的比例，即在所有被判为正类的样本里，真正属于正类的有多少。
- 特异性（Specificity）：分类器预测为负类的所有样本中，真实为负类的比例，衡量模型对负类的识别能力，即避免把负类误判成正类的能力。
- F1度量：查准率与查全率的调和平均值，用于平衡两者权重，常见于推荐系统等需兼顾“精准”与“全面”的场景，公式为：
  $$F1 = \frac{2×precision×recall}{precision + recall}$$
- Fβ度量：F1度量的一般形式，通过参数β控制查全率对查准率的相对重要性（β>1时更侧重查全率，β<1时更侧重查准率），公式为：
  $$Fβ = \frac{(β² + 1)×precision×recall}{β²×precision + recall}$$

## 回归任务性能度量

回归任务的输出为连续数值，需定量反馈指标评估“预测值与真实值的数值差异精度”，核心关注预测结果的数值准确性。

### 平均绝对误差（Mean Absolute Error，MAE）

通过计算预测值与实际值之间的平均绝对差异，衡量回归模型的性能。
公式为：

$$MAE = \frac{1}{N}\sum_{i=1}^{N} |y_i - \hat{y}_i|$$

其中，N为样本总数，y_i为真实值，$\hat{y}_i$为预测值。

- 计算简单：直接测量预测值与实际值之间的绝对差异
- 敏感性不足：对于一些相对较小的误差不够敏感
- 不可微分：MAE在零误差点处不可微分，影响优化过程

### 均方误差（Mean Squared Error，MSE）

衡量预测值与实际值之间的平均平方差异，平方差的计算使得误差被放大。

公式为：

$$MSE = \frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

- 对误差敏感：MSE会放大误差，受错误预测影响更大
- 可微分：MSE是可微的，有利于使用优化算法（如梯度下降）
- 对离群点敏感：MSE对离群点非常敏感，较大误差的平方会显著影响整体误差度量

### 均方根误差（Root Mean Squared Error，RMSE）

RMSE是MSE的平方根，表示预测值与实际值之间的标准偏差。

公式为：

$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}$$

- 对误差敏感：与MSE类似，RMSE对误差有一定程度的敏感性
- 依旧对离群点敏感：由于RMSE是MSE的平方根，仍然对离群点敏感
- 鲁棒性不足：对于包含大量离群点的数据集，RMSE依旧不如MAE鲁棒
- 单位与样本一致

# 2.3 概率统计基础

## 基本概念

- 概率：对随机事件发生可能性的度量，常用符号 $P$ 表示
- 联合概率：表示事件 $A$ 和 $B$ 同时发生的概率，记作 $P(A, B)$ 或 $P(A \cap B)$
- 条件概率：在事件 $B$ 已发生的条件下事件 $A$ 发生的概率，记作 $P(A|B)$
- 独立事件：事件 $A$ 的发生对事件 $B$ 的发生概率没有影响，反之亦然
- 条件独立：在给定 $C$ 的情况下，$A$ 和 $B$ 满足 $P(A, B|C) = P(A|C)P(B|C)$，常记作 $A \perp\!\!\!\perp B \mid C$（使用两个并排的竖线符号 $\perp\!\!\!\perp$）

## 核心公式

- 乘法原理：$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$
- 全概率公式：若$V_1,V_2,…,V_n$是样本空间Ω的一个划分，两两互斥且$P(V_i)>0$（i=1,2,…,n），则$P(A) = \sum_{i=1}^{n}P(A|V_i)P(V_i)$

## 连续型随机变量

### 概率密度函数与累积分布函数

**概率密度函数**是描述随机变量输出值在某确定取值点附近可能性的函数，随机变量的取值落在某个区域之内的概率为概率密度函数在这个区域上的积分。

**累积分布函数**是概率密度函数的积分。

## 期望

期望是一个随机变量所取值的概率平均。

- 离散变量期望：$E[X] = \sum_{x}xP(X=x)$
- 连续变量期望：$E[X] = \int_{-\infty}^{+\infty}xP(x)dx$

## 方差与标准差

方差描述的是随机变量的值偏离其期望值的程度。

- 离散变量方差：$Var(X) = \sum_{x}(x-E[X])^2P(X=x) = E[X^2] - (E[X])^2$
- 连续变量方差：$Var(X) = \int_{-\infty}^{+\infty}(x-E[X])^2P(x)dx$

标准差是方差的平方根，记作$\sigma(X) = \sqrt{Var(X)}$

## 贝叶斯公式

贝叶斯公式给出了“结果”事件A已经发生的条件下，“原因”事件B的条件概率，对结果的任何观测都将增加对原因事件B的真正分布的知识。

$$P(B_i|A) = \frac{P(B_i)P(A|B_i)}{\sum_{j=1}^{n} P(B_j)P(A|B_j)}$$

- 后验概率：对应公式中的$P(B_i|A)$，指给定观测数据后，某事件发生的概率
- 先验概率：对应公式中的$P(B_i)$，指在没有观测数据之前，某事件发生的初始概率
- 似然概率：对应公式中的$P(A|B_i)$，指给定事件发生的情况下，观测数据出现的概率
- 证据因子：对应公式中的$\sum_{j=1}^{n} P(B_j)P(A|B_j)$，指观测数据的边际概率，即所有可能事件下观测数据的总概率

# 2.4 贝叶斯决策理论

## 基本概念

贝叶斯决策是统计决策理论中的一个基本方法，用于解决分类问题。

已知条件：

- 属于一定数量类别的数据，类别标签为：$\omega_i, i=1,2,...,c$
- 各类别$\omega_i$的类先验概率$P(\omega_i)$和类条件概率密度$P(x|\omega_i)$

基本思想：

根据贝叶斯公式计算后验概率，基于最大后验概率进行判决。

判决函数：

最大化后验概率，即：$x \in \omega_k$ 当且仅当 $k = arg \max_{i}\{P(\omega_i|x)\}$
其中后验概率的计算式为：
$$P(\omega_i|x) = \frac{P(x|\omega_i)P(\omega_i)}{\sum_{j=1}^{c} P(x|\omega_j)P(\omega_j)}$$

## 最小错误率贝叶斯决策

- 核心思路：最小化分类错误概率，基于最大后验概率原则，在分类中选择后验概率最大的类别
- 错误概率目标函数：$min P(e) = \int P(e|x)p(x)dx$
- 条件错误概率特性：因$P(e | x) ≥0$、$p(x) ≥0$，决策目标可简化为$min _{x} P(e | x)$
- 决策规则：对于样本x和类别$\omega_1$、$\omega_2$，若$P(\omega_1 | x) > P(\omega_2 | x)$，则将样本x判别为$\omega_1$，反之判别为$\omega_2$，即选择后验概率更大的类别，对应错误概率$P(e | x)$更小

## 最小风险贝叶斯决策

- 核心思路：在考虑错误率的基础上，进一步考虑不同错误的损失，通过最小化期望风险进行决策
- 期望风险公式：$R = \int R(\alpha(x)|x)p(x)dx$，其中$R(\alpha(x)|x)$为条件期望损失
- 条件期望损失计算：$R(\alpha_i|x) = \sum_{j} \lambda(\alpha_i, \omega_j)P(\omega_j|x)$，$\lambda(\alpha_i, \omega_j)$为决策$\alpha_i$对应真实类别$\omega_j$的损失
- 决策规则：选择使条件期望损失最小的决策$\alpha_i$，即$\alpha_i = arg min R(\alpha_i|x)$

## 朴素贝叶斯决策

- 核心思路：在类先验概率和类条件概率密度未知，需要估计时，通过假设所有属性相互独立，实现降低样本集大小需求、降低复杂度的目的
- 适用场景：样本量有限，难以直接估计复杂类条件概率的场景
- 决策逻辑：已知类条件概率$P(X|\omega_i)$和类先验概率$P(\omega_i)$，通过属性独立假设简化计算，进而得到后验概率$P(\omega_i|X)$，基于后验概率进行决策

# 2.5 参数化概率密度估计方法

## 概率密度估计的概念

概率密度估计的任务：

根据样本数据估计类条件概率密度函数 $P(x | \omega_{i})$ 和类先验概率 $P(\omega_{i})$。

为什么需要估计概率密度：

概率密度估计可以建模原始数据分布，帮助我们更精细地了解数据特性，进而帮助我们识别数据中的异常值、甚至用于生成新数据。

概率密度估计的方法：

- 参数化方法：已知概率密度函数的形式，其中几个参数未知
- 非参数化方法：概率密度函数的形式未知

## 参数化概率密度估计方法的概念

参数化概率密度估计的任务：

- 已知：已知概率密度函数的形式，只是其中一个或几个参数未知
- 目标：依据样本估计这些未知参数的值

典型方法：

- 极大似然估计：把待估计参数看做是确定的量，只是其取值未知。最佳估计就是使产生已观测到样本的概率最大的那个值
- 贝叶斯估计：把待估计参数看做是符合某种先验概率分布的随机变量。对样本进行观测的过程，就是把先验概率密度转化为后验概率密度，从而利用样本信息修正参数的初始估计值的过程

## 极大似然估计方法

极大似然估计的假设条件：

- $P(x | \omega_{i})$ 具有某种确定的解析函数形式，只有部分参数 $\theta$ 未知；参数 $\theta$ 通常为向量，如一维正态分布 $N(\mu, \sigma^{2})$ 中的 $\mu$、$\sigma$
- 参数 $\theta$ 是确定的未知量，不是随机量
- 各类样本集 $x_{i}$（$i=1,2, ..., c$）满足独立同分布条件（i.i.d.），即 $x_{i}$ 均为从密度为 $P(x | \omega_{i})$ 的总体中独立抽取出来的
- 各类样本只包含本类分布的信息；因此，$P(x | \omega_{i})$ 可记为 $P(x | \omega_{i} ; \theta_{i})$ 或 $P(x ; \theta_{i})$
- 基于上述假设，各类条件概率密度可根据各类样本分别估计

似然函数：

针对一类已知样本 $X=\{x_{i}, i=1,2, ..., N\}$，定义参数 $\theta$ 下观测到样本集 $X$ 的联合分布概率密度，称为相对于样本集 $X$ 的 $\theta$ 的似然函数，公式为：

$$l(\theta) = P(x_{1}, x_{2}, ..., x_{n}; \theta) = \prod_{i=1}^{n} P(x_{i}; \theta)$$

基本思想：

在 $\theta$ 可能的取值范围内选择使似然函数达到最大的参数值作为参数 $\theta$ 的估计值。

- 形式化描述为：求 $\hat{\theta}$，使得 $l(\hat{\theta}) = \max_{\theta} l(\theta)$
- 如果参数 $\theta = \hat{\theta}$ 时，$l(\theta)$ 最大，则 $\hat{\theta}$ 是最可能的参数估计值。它是样本集的函数，记作：$\hat{\theta} = d(x_{1}, x_{2}, ..., x_{N}) = d(X)$，称为极大似然估计量
- 为便于分析求解，实际运用中往往采用对数似然函数：$H(\theta) = \ln l(\theta)$

求解方式：

通过求导令导数为 0 求解，若参数 $\theta$ 为多维向量，需构造方程组，例如：

$$\left\{\begin{array}{l}\frac{\partial H}{\partial \mu} = 0 \\ \frac{\partial H}{\partial \sigma^{2}} = 0\end{array}\right.$$

即：

$$\left\{\begin{array}{c}\frac{1}{\sigma^{2}}\left[\sum_{i=1}^{N} x_{i} - N \mu\right] = 0 \\ -\frac{n}{2 \sigma^{2}} + \frac{1}{2 \sigma^{4}} \sum_{i=1}^{N}\left(x_{i} - \mu\right)^{2} = 0\end{array}\right.$$

## 贝叶斯估计方法

贝叶斯估计的基本思想：

把待估计参数 $\theta$ 看作是具有先验分布 $P(\theta)$ 的随机变量，其取值与样本集 $X$ 有关，贝叶斯估计利用样本集 $X$ 将 $\theta$ 的先验概率分布修正为后验概率分布。

- 贝叶斯决策用于分类，计算离散形式的后验概率值，而贝叶斯估计则用于回归，计算连续形式的后验概率密度函数

贝叶斯估计损失函数：

- 把 $\theta$ 估计为 $\hat{\theta}$ 所造成的损失，记为 $\lambda(\hat{\theta}, \theta)$
- 不同于离散形式贝叶斯决策的损失表，由于参数化概率密度估计为连续值估计，因此常采用损失函数，常用平方误差损失函数 $\lambda(\hat{\theta}, \theta) = (\theta - \hat{\theta})^{2}$

贝叶斯估计步骤：

1. 确定参数 $\theta$ 所遵从的先验分布：$P(\theta)$
2. 求样本集的联合分布：$P(X | \theta) = \prod_{i=1}^{N} P(x_{i} | \theta)$
3. 求 $\theta$ 的后验概率分布：$P(\theta | X) = \frac{P(X | \theta) P(\theta)}{\int_{\Theta} P(X | \theta) P(\theta) d \theta}$
4. 求 $\theta$ 的贝叶斯估计量（定理 2.5.1）：$\hat{\theta} = \int_{\Theta} \theta P(\theta | x) d \theta$

贝叶斯估计相关概念：

- 条件期望损失：$R(\hat{\theta} | x) = \int_{\Theta} \lambda(\hat{\theta}, \theta) P(\theta | x) d \theta$，其中，$x \in E^{d}$，$\theta \in \Theta$，$E^{d}$ 为样本集，$\Theta$ 为待估参数集
- 期望风险：$R = \int_{E^{d}} \int_{\Theta} \lambda(\hat{\theta}, \theta) P(\theta | x) P(x) d \theta d x = \int_{E^{d}} R(\hat{\theta} | x) P(x) d x$
- 贝叶斯估计量：使条件期望损失最小的估计量 $\hat{\theta}$，即

$$\hat{\theta} = \text{argmin}(R(\hat{\theta} | x)) = \text{argmin}\left(\int_{\theta} \lambda(\hat{\theta}, \theta) P(\theta | x) d \theta\right)$$

- 定理 2.5.1：若采用平方误差损失函数，则 $\theta$ 的贝叶斯估计量是在给定样本集 $X$ 时 $\theta$ 的条件期望，即 $\hat{\theta} = E(\theta | x) = \int_{\Theta} \theta P(\theta | x) d \theta$
- 定理 2.5.1 证明过程，可参考《模式识别》-边肇祺张学工 P52

贝叶斯估计示例（单变量正态分布）：

已知：

- 参数：$\theta = [\theta_{1}, \theta_{2}]$，$\theta_{1} = \mu$，$\theta_{2} = \sigma^{2}$，其中 $\theta_{2}$ 已知
- 概率密度函数：$P(x | \theta) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left[-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right]$
- 样本集：$X = \{x_{1}, x_{2}, ..., x_{N}\}$

目标：估计均值 $\theta_{1}$。

解（仅保留关键步骤）：

1. 确定概率密度函数形式：$P(x | \theta) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left[-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right]$
2. 设估计量 $\mu$ 遵从以下分布：$\mu \sim N(\mu_0, \sigma_0^2)$
3. 根据观测样本 $X$ 求得 $\mu$ 的后验分布：$\mu|X \sim N\left(\frac{N \sigma_0^2 \bar{x} + \sigma^2 \mu_0}{N \sigma_0^2 + \sigma^2}, \frac{\sigma^2 \sigma_0^2}{N \sigma_0^2 + \sigma^2}\right)$，其中 $\bar{x} = \frac{1}{N}\sum_{i=1}^N x_i$，$\tau = \frac{1}{\sigma^2}$，$\tau_0 = \frac{1}{\sigma_0^2}$
4. $\mu$ 的贝叶斯估计量为 $\hat{\mu} = E(\mu | X) = \frac{N \sigma_0^2 \bar{x} + \sigma^2 \mu_0}{N \sigma_0^2 + \sigma^2} = \alpha \mu_0 + (1 - \alpha) \bar{x}$，其中 $N$ 为样本数，$\alpha = \frac{\sigma^2}{N \sigma_0^2 + \sigma^2}$
5. 当 $N = 0$ 时，估计量 $\hat{\mu} = \mu_0$；当 $N \to \infty$ 时，估计量 $\hat{\mu} = \bar{x}$

具体可参考《模式识别》-边肇祺张学工 P56-57

# 2.6 非参数概率密度估计方法
## 什么是非参数估计?

**非参数估计定义**：非参数估计是概率密度函数的形式未知的模型，直接依赖于数据本身来进行推断和估计。

**常见的非参数估计方法**：

- Parzen窗法
- k近邻法

## 非参数估计方法核心思路与流程

**基本思路**：要估计$x_0$点的密度，可把相关样本在该点的"贡献"相加近似作为其概率密度，进而估计每个点的概率密度。

**具体流程**：

1. 确定计算$x_0$点处概率密度的相关贡献点
2. 确定贡献点对$x_0$点处的贡献
3. 重复1-2计算所有点处的概率密度

**核心假设与收敛条件**：

假设$N$为样本总数，以$x_0$为中心的区域$R$（足够小，体积为$V$）内的$k$个点对估计$x_0$的概率密度$p(x)$有贡献，则：

- 区域$R$中落入$k$个样本的概率为：$P_k = \binom{N}{k} [p(x_0)V]^k [1-p(x_0)V]^{N-k}$
- 估计得到的概率密度$\hat{p}(x_0)$为：$\hat{p}(x_0) = \frac{k}{NV}$

当满足以下条件时，估计概率$\hat{p}(x_0)$收敛于$p(x_0)$：

- $\lim_{N \to \infty} k_N = 0$（贡献点与总样本数比例越小越好）
- $\lim_{N \to \infty} k_N = \infty$（贡献点越多越好）
- $\lim_{N \to \infty} \frac{k_N}{N} = 0$（贡献点的区域大小越小越好）

**方法差异（区域与样本数选择）**：

- Parzen窗法：使区域体积序列$V_N$以$N$的某个函数的关系不断缩小，同时限制$k_N$和$\frac{k_N}{N}$
- k近邻法：使落入区域样本数$k_N$为$N$的某个函数，选择不同的$V_N$使区域包含$x$的$k_N$个近邻

## Parzen窗算法

**核心原理**：Parzen窗法使用窗函数对贡献点进行选择，通过窗函数反映$x_i$对$p(x)$的贡献，同时实现区域选择。

**窗函数要求**：

- 形式：$\phi(x, x_0)$，反映$x_i$对$p(x)$的贡献并进行区域选择
- 条件：$\phi(x, x_0) \geq 0$，$\int \phi(x, x_0) dx = 1$

**窗函数类型**：

- 方窗函数
- 正态窗函数
- 指数窗函数等

**窗宽选择原则**：样本数多则选小些；样本数少则选大些。

**概率密度估计公式**：$\hat{p}(x) = \frac{1}{NV} \sum_{i=1}^N \phi\left( \frac{x - x_i}{h} \right)$，其中$h$为窗宽，$V$为窗体积。

## k近邻算法

**核心原理**：k近邻法使用k近邻算法对贡献点进行选择，选择样本$x_0$一定范围内确定个数的$k$个样本后，根据$\frac{k}{NV}$计算概率密度，更适用于样本分布不均匀的数据。

**k近邻算法流程**：

1. 选择一个正整数$k$，表示需要考虑的邻近样本的数量
2. 对于待预测的样本，计算其与训练集中所有样本之间的距离（常用的距离度量包括欧氏距离、曼哈顿距离、余弦相似度等）
3. 选择$k$个最近：根据计算得到的距离，选择距离待预测样本最近的$k$个邻居

**k值选择**：$k$值决定了决策的局部性，$k$值越大，模型越平滑，越小则越敏感。

**概率密度估计公式**：$\hat{p}(x) = \frac{k}{NV(x)}$，其中$V(x)$为包含$x$的$k$个近邻的区域体积。

## 方法对比与小结

### 核心对比

| 特性                | Parzen窗法                | k近邻法                  |
|---------------------|---------------------------|--------------------------|
| 区域体积$V$         | 固定窗宽，动态调整体积    | 动态调整体积以包含$k$个近邻 |
| 样本分布适应性      | 适用于分布较均匀的数据    | 更适用于分布不均匀的数据  |
| 高维空间适用性      | 应用难度较高              | 更易应用，可缓解维度问题  |
| 边界估计偏差        | 边界附近可能存在估计偏差  | 边界附近可能存在估计偏差  |

### 小结

- Parzen窗法与k近邻法均使用广泛
- 在样本分布不均匀时k近邻法比Parzen窗法表现更好
- 在高维空间中k近邻法比Parzen窗法更易应用，且可通过技术手段缓解维度问题
- k近邻法和Parzen窗法在边界附近都可能会遇到估计偏差