# 4.1 为什么要引入正则化？

## 过拟合问题回顾

过拟合是机器学习训练中常见的问题，模型在训练集上表现优异，但在未见过的测试集上性能大幅下降，泛化能力差。

## 什么是正则化？

### 正则化的定义

正则化(Regularization)是指为缓解过拟合而加入额外先验约束的过程。训练机器学习模型时通过引入正则化来增强模型的泛化性能。

### 作用机制

机器学习训练过程可以简化为训练集 $D$ 损失函数 $L(F)$ 的最小化问题，为了对抗过拟合，我们向损失函数中加入描述模型复杂程度的正则化项 $\Omega(F)$，其一般形式为：

$$Obj(F) = L(F) + \lambda \Omega(F),\ \lambda > 0$$

其中：
- $L(F)$ 为训练集损失，反映模型对训练数据的拟合程度；
- $\lambda$ 为正则化系数，用于控制正则项对总损失的贡献程度；
- $\Omega(F)$ 为正则化项，引入了模型参数的先验约束。

### 核心作用

通过引入表示模型复杂度的正则化项，降低模型的复杂度，避免模型过度拟合到训练集的特定数据，从而增强模型对未知数据的适应能力，提高泛化性能。

## 正则化的形式

### $L_p$ 范数正则化

- 机器学习中广泛使用的正则化形式；
- 计算高效，目标函数可用梯度下降等方式求解最优化问题；
- $L_p$ -范数表示向量空间中的距离，具备非负性、齐次性、三角不等式（次可加性），定义为：
$$L_{p}(\vec{x})=\| \vec{x}\| _{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{1 / p}, \vec{x}=\left\{x_{1}, x_{2}, \cdots, x_{n}\right\}, p \geq 1$$

### $L_p$ 范数的核心性质

- 非负性：$\|\overrightarrow{x}\|_{p} \geq 0$；
- 齐次性：$\| \alpha \vec{x}\| _{p}=\alpha\| \vec{x}\| _{p}, \alpha>0$；
- 三角不等式（次可加性）：$\|\overrightarrow{x}+\overrightarrow{y}\|_{p} \leq\|\overrightarrow{x}\|_{p}+\|\overrightarrow{y}\|_{p}$。

### 作用原理

$L_p$ -范数正则化项通过引入 $L_p$ -范数作为正则化项 $\Omega(F)$，约束了模型参数的范数上界，防止模型过度拟合到训练集 $D$ 的数据。通过引入 $L_p$ -范数正则化来约束模型的复杂度，提高了模型的泛化能力。

# 4.2 L2正则化

## L2正则化的定义与求解

### L2正则化的定义

即在损失函数中添加一个参数的$L_2$范数惩罚项，令$\gamma=\frac{\lambda}{2}$（$\lambda>0$），则$L_2$正则化的目标函数为：

$$L(w)=l(w)+\frac{\lambda}{2}\| w\| _{2}^{2}$$

$L_2$正则化（$p=2$）：$\|\overrightarrow{x}\|_{2}=\sqrt{\sum_{i=1}^{n}|x_{i}|^{2}}$，即欧氏距离。

### L2正则化问题的求解

#### 目标函数

$$L(w)=l(w)+\frac{\lambda}{2}\| w\| _{2}^{2}$$

#### 求导与梯度下降更新

1. 求$L$对$w$的偏导数：
$$\nabla _{w}L(w)=\nabla _{w}l(w)+\lambda w$$
2. 梯度下降更新公式：
$$\begin{aligned} w_{t+1} & =w_{t}-\eta \cdot \nabla_{w} L(w) \\ & =w_{t}-\eta\left(\nabla_{w} l(w)+\lambda w\right) \\ & =(1-\eta \lambda) w_{t}-\eta \nabla_{w} l(w) \end{aligned}$$

其中$\eta$为学习率。

## 岭回归(Ridge Regression)
### 岭回归的定义
岭回归是在线性回归的代价函数中加入$L_2$正则化项的模型。

### 目标函数
1. 一般形式：
$$L(w)=\frac{1}{2} \sum_{i=1}^{n}\left(\left(w^{T} x_{i}+b\right)-y_{i}\right)^{2}+\frac{\lambda}{2}\| w\| _{2}^{2}$$

2. 矩阵形式：
$$\begin{aligned} L(w) & =\frac{1}{2}(X w-Y)^{T}(X w-Y)+\frac{\lambda}{2} w^{T} w \\ & =\frac{1}{2}\left(w^{T} X^{T} X w-w^{T} X^{T} Y-Y^{T} X w+Y^{T} Y\right)+\frac{\lambda}{2} w^{T} w \end{aligned}$$

### 岭回归的求解
1. 对矩阵形式的目标函数求导：
$$\nabla_{w} L(w)=X^{T} X w-X^{T} Y+\lambda w$$

2. 令导数为0，得解：
$$w=\left(X^{T} X+\lambda I\right)^{-1} X^{T} Y$$

此外，岭回归还可以用梯度下降法求解。

### 岭回归的性质特点
岭回归通过添加平方项来收缩系数，但不会将它们缩减至零。

## L2正则化系数$\lambda$的确定
1. 将数据集分成训练集和验证集；
2. 在训练集上训练模型，并在验证集上评估模型的性能；
3. 选择在验证集上性能最好的参数$\lambda$值作为最终的参数设置。

# 4.3 L1正则化

## L0引入背景：高维特征场景下的需求

对于高维特征场景，$L_2$正则化不适用：
- 基因表达数据等通常具有非常高的维度，数以万计的基因表达水平作为特征，大部分基因表达冗余；
- 需要从中提取少数关键的基因特征来预测疾病，即在加入正则项减少过拟合的同时，进行特征选择与提升计算效率；
- $L_2$正则化使权重减小得更均匀，而不是将它们降为0，所有特征都会被保留，无法满足特征选择需求；
- 需要将大部分冗余的参数设置为0，使模型能保留关键特征并减小模型复杂度，$L_0$正则化可处理这类问题，但存在求解困难，因此引入$L_1$正则化作为$L_0$正则化的近似替代。

## L0正则化的定义

### L0范数定义

$L_0$范数：$\|x\|_{0}=\sum_{i=1}^{n} \mathbb{1}_{x_{i} \neq 0}$，即向量中非0元素个数。

### L0正则化的优化问题

对于一般模型，其$L_0$正则化可表示为：

$$min _{w} l(w)+\lambda\|w\|_{0}$$

### L0正则化的局限性

- $L_0$范数实际上不是一个真正的数学范数，因为它不满足次可加性，非凸且不连续；
- 优化目标离散且非凸，导致优化问题十分复杂，属于NP难问题，难以求解；
- 可通过$L_1$范数近似代替$L_0$范数。

## 为什么能使用$L_1$范数近似替代$L_0$范数

### 核心性质对比

- $L_1$范数：$\|x\|_{1}=\sum_{i=1}^{n}|x_{i}|$，被称作曼哈顿距离，连续且凸，但在零点不可导；
- $L_0$范数：$\|x\|_{0}=\sum_{i=1}^{n} \mathbb{1}_{x_{i} \neq 0}$，非凸且不连续，不满足次可加性。

### 关键原因
$L_1$范数是$L_0$范数的最优凸近似，几何上$L_1$是能包住$L_0$的最小凸图形。

*相关证明：Best Convex Lower Approximations of the $L_0$ Pseudonorm on Unit Balls；几何解释：Why $L_1$ is a good approximation to $L_0$ : A geometric explanation*

## L1正则化的定义

### 一般模型的L1正则化形式

$$min _{w} l(w) + \lambda\|w\|_{1}$$

### 示例：带L1正则项的线性回归（Lasso回归）

损失函数为：

$$min _{w} \frac{1}{2}\| X w-y\| _{2}^{2}+\lambda\| w\| _{1}$$

### 求解特性

该问题无法直接使用梯度下降方法求解，目标函数同时包含可微分项与不可微分项时，可使用近端梯度下降（PGD）求解。

## L1正则化的求解

### 核心方法：近端梯度下降（PGD）

近端梯度下降一般用于求解部分带有不可微函数的优化问题，形式为$min\ g(w) + h(w)$（$g(w)$为可微函数，$h(w)$为不可微函数）。

#### 主要步骤

1. 可微部分优化：先求可微部分的解，即先求$g(w)$；
2. 近端映射：再根据可微部分的解更新整体，即求$g(w)+h(w)$；
3. 迭代求解：迭代步骤1和步骤2，直至收敛或达到最大迭代次数。

#### 可微部分优化

1. 优化目标拆解：对$L1$正则化问题的优化目标$min\ l(w) + \lambda\|w\|_{1}$，先考虑可微部分$l(w)$的迭代；
2. 二阶泰勒展开近似：为求$l(w)$的更新方向，对$l(w)$在$w_k$附近通过二阶泰勒展开近似，利用函数曲率信息自适应调整步长，提高收敛速度和稳定性：
$$\begin{aligned} \hat{l}(w) & \simeq l\left(w_{k}\right)+\left<\nabla l\left(w_{k}\right), w-w_{k}\right>+\frac{\nabla^{2} l\left(w_{k}\right)}{2}\left\| w-w_{k}\right\| ^{2} \\ & \leq l\left(w_{k}\right)+\left<\nabla l\left(w_{k}\right), w-w_{k}\right>+\frac{L}{2}\left\| w-w_{k}\right\| ^{2} \end{aligned}$$
3. 约束条件：可微部分$l(w)$需满足$L$-Lipschitz条件，即存在常数$L>0$使得：
$$\| \nabla l(w^{\prime })-\nabla l(w)\| _{2} \leq L\| w^{\prime }-w\| _{2} \quad (\forall w^{\prime }, w)$$
4. 合并展开与最小值求解：
$$\begin{array}{rl}
& l\left( w_{k}\right) +\frac {L}{2}\bigg (\left( w-w_{k}\right) +\frac {1}{L}\nabla l\left( w_{k}\right) \bigg )^{T}\left( \left( w-w_{k}\right) +\frac {1}{L}\nabla l\left( w_{k}\right) \right) -\frac {1}{2 L}\nabla l\left( w_{k}\right) ^{T}\nabla l\left( w_{k}\right) \\
& =\frac {L}{2}\bigg \| \left( w-w_{k}\right) +\frac {1}{L}\nabla l\left( w_{k}\right) \bigg \| _{2}^{2}+C - \frac {1}{2 L}\nabla l\left( w_{k}\right) ^{T} \nabla l\left( w_{k}\right)
\end{array}$$
取上式最小值，令$\left\|(w-w_{k})+\frac{1}{L} \nabla l(w_{k})\right\|_{2}^{2}=0$，则$l(w)$的最小值可由如下迭代求得：
$$w_{k+1}=w_{k}-\frac{1}{L} \nabla l\left(w_{k}\right)$$

#### 近端映射

1. 近端算子（Proximal Operator）定义：
$$prox_{\lambda, h(w)}(z)=argmin \frac{L}{2}\| w-z\| _{2}^{2}+\lambda \cdot h(w)$$
2. 软阈值函数（$L_1$正则化中的近端映射）：
$$S_{\lambda}(z)=\underset{w}{argmin} \frac{L}{2}\| w-z\| _{2}^{2}+\lambda\| w\| _{1}$$
3. 分量求解：$w$的各个分量互不影响，可通过下式求解：
$$w_{k+1}^{i}=\left\{ \begin{array} {cc}
z^{i}-\lambda /L, & \lambda / L<z^{i} \\
0, & \lambda / L \geq\left|z^{i}\right| \\
z^{i}+\lambda / L, & -\lambda / L>z^{i}
\end{array}\right.$$
4. 整体迭代流程：
$$w_{k+1}=\underset{w}{argmin} \frac{L}{2}\left\| \left(w-w_{k}\right)+\frac{1}{L} \nabla l\left(w_{k}\right)\right\| _{2}^{2}+\lambda\| w\| _{1}$$
   - 先计算可微部分：$z=w_{k}-\frac{1}{L} \nabla l(w_{k})$；
   - 再对整体求解：$w_{k+1}=\underset{w}{argmin} \frac{L}{2}\|w-z\|_{2}^{2}+\lambda\|w\|_{1}$；
   - 迭代上述两步，直至收敛。

### 其他L1正则化求解方法

- 次梯度法（Subgradient Method）：通过使用目标函数在不可微处的次梯度代替传统梯度，进行迭代优化以求解不可微的优化问题；
- 坐标下降法（Coordinate Descent）：将多变量优化问题分解为多个单变量问题，逐变量沿坐标轴方向上进行优化，直到收敛；
- 近似法（Approximation Methods）：通过将不可微问题近似为可微问题，利用传统的优化方法进行求解。

## L1正则化的示例：Lasso回归

### 问题模型

$$min _{w} \frac{1}{2}\| X w-y\| _{2}^{2}+\lambda\| w\| _{1}$$

### 求解方法

使用近端梯度下降求解。

### 性质特点

Lasso回归训练后能使部分参数为零，即具有求取稀疏解的能力。

## L1正则化与稀疏解

### 核心结论

$L_1$正则化比$L_2$正则化更易获得稀疏解。

### 几何解释

$L_1$正则化在$L_1$等值线角点上更容易取到最优解（最小值），在角点上取值时，会使权值$w$的部分分量取零值，因此$L_1$正则化可以使模型参数稀疏化。

- $L_2$范数等值线为圆形，最优解多在非坐标轴上，参数不易为零；
- $L_1$范数等值线为菱形，最优解多在坐标轴上，参数易为零。

*L1正则化与稀疏解关系具体说明:《机器学习》周志华著P253*

# 稀疏学习

## 模型的稀疏性

权重$w$，模型的稀疏性：模型中大部分参数都是零或接近零。

### 模型稀疏性的好处

1. 特征选择：自动压缩不重要特征权重，保留关键特征，简化模型；
2. 降低过拟合：限制模型复杂度，提高泛化能力。

## 数据表示的稀疏性

特征向量$X$，数据表示的稀疏性：即稀疏表示，数据表示中只含有少量非零元素。

### 稀疏表示的好处

1. 提升存储计算效率：数据易于存储，模型计算量减少；
2. 提取关键特征：学习数据的稀疏表示，实现对数据的高效分析。

### 稀疏表示的实现方法

如何为普通稠密表达的数据找到合适的“稀疏表示”形式？字典学习是一个代表性方法。

## 字典学习的基本概念

### 核心定义

对于给定的数据集$X=\{x_{1}, ..., x_{n}\}$，通过由原子$b_{i}$构成的字典矩阵$B$和由稀疏向量$\alpha_{i}$构成的稀疏矩阵$A$对数据集样本进行近似表达：
$$X \approx \underbrace{\left(b_{1}\left|b_{2}\right| \cdots | b_{k}\right)}_{B \in \mathbb{R}^{m \times k}} \underbrace{\left(\begin{array}{c} \alpha_{1}^{T} \\ \vdots \\ \alpha_{n}^{T} \end{array}\right)}_{A \in \mathbb{R}^{n \times k}, 稀疏 }$$

### 本质含义

通过为普通稠密表达的数据学习一组过完备的基向量（原子）来作为合适的字典，在表示中尽可能使用较少的原子将样本转化为稀疏表达形式。这个过程通常称之为“字典学习”或是“稀疏编码”。

## 字典学习的求解

### 优化目标

$$min _{B, \alpha_{i}} \sum_{i=1}^{n}\left\|x_{i}-B \alpha_{i}\right\|_{2}^{2}+\lambda\left\|\alpha_{i}\right\|_{1}$$

目标：准确表示样本且$\alpha_{i}$尽量稀疏（$\lambda$为正则化系数）。

### 算法基本思路
式子中包含字典$B$和稀疏向量$\alpha_{i}$两项，直接求解较为困难。考虑采用变量交替优化的策略来求解。

### 算法流程
1. 初始化字典矩阵$B$；
2. 通过固定字典$B$，转化为LASSO形式进行求解，更新稀疏矩阵$A$；
3. 通过固定稀疏矩阵$A$，更新字典$B$；
4. 反复迭代上述两步，最终可求得字典$B$和稀疏矩阵$A$。

### 步骤2：更新稀疏矩阵$A$
原优化目标固定字典$B$后，更新稀疏向量$\alpha_{i}$，转化为如下形式：
$$min _{\alpha_{i}}\left\| x_{i}-B \alpha_{i}\right\| _{2}^{2}+\lambda\left\| \alpha_{i}\right\| _{1}$$

该形式符合Lasso回归的损失函数$min _{w} \frac{1}{2}\| X w - y\| _{2}^{2}+\lambda\| w\| _{1}$，可直接沿用Lasso的求解方法。

### 步骤3：更新字典$B$
#### 优化目标转化
固定稀疏矩阵$A \in \mathbb{R}^{n \times k}$，更新字典$B$，优化目标转化为：
$$min _{B}\| X - B A\| _{2}^{2}$$

#### 求解思路：逐列更新策略
1. 将优化目标按列拆分，每次只更新一个原子$b_{i}$和对应的稀疏向量$\alpha_{i}$：
$$\begin{aligned} min _{B}\| X - B A\| _{2}^{2} & =min _{b_{i}}\left\| X - \sum_{j=1}^{k} b_{j} \alpha_{j}^{T}\right\| _{2}^{2} \\ & =min _{b_{i}}\left\| \left(X - \sum_{j \neq i} b_{j} \alpha_{j}^{T}\right) - b_{i} \alpha_{i}^{T}\right\| _{2}^{2} \\ & =min _{b_{i}}\left\| E_{i} - b_{i} \alpha_{i}^{T}\right\| _{2}^{2} \end{aligned}$$

其中$E_{i} = X - \sum_{j \neq i} b_{j} \alpha_{j}^{T}$（逐列更新时其他列固定，该项为已知残差矩阵）。

2. 奇异值分解（SVD）求解：
通过最小化$E_{i}$的重建误差，对$E_{i}$进行奇异值分解$E_{i}=U \Sigma V^{T}$，可得：
$$b_{i}=U_{\cdot, 1}, \quad \alpha_{i}=\Sigma_{1,1} V_{\cdot, 1}^{T}$$

其中$U$、$V$为$E_{i}$的左右奇异矩阵，$b_{i}$和$\alpha_{i}^{T}$分别为最大奇异值对应的$U$的第一列和$V$的第一行向量。

3. 稀疏性保持：
通过保留$\alpha_{i}$中已有的0值来保留已有系数的稀疏性，仅取出$\alpha_{i}$中非零的元素记为$\alpha_{i}'$，保留$E_{i}$中对应$b_{i}$和$\alpha_{i}'$的乘积项为$E_{i}'$，再进行上述最小化误差步骤。

*奇异值分解参考：https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3*

## 字典学习的应用

### 核心应用场景：图像去噪

#### 应用目标

从受到噪声干扰的图像中恢复出原始图像。

#### 去噪原理

字典学习是一种常用的图像去噪方法，它通过学习自然图像的稀疏表示，实现对图像的去噪处理：自然图像的相似结构和纹理具有局部冗余性，易被稀疏表示；而噪声（随机、高频）难以稀疏表示。因此二者可通过字典学习分离。

#### 字典学习与去噪步骤

1. 字典学习步骤：
   - 图像分块：将原始图像分割为多个图像块$\{I_{k}\}$；
   - 最小化目标函数：通过图像块训练，求解$min _{B, \alpha_{k}} \sum_{k}\left\|I_{k}-B \alpha_{k}\right\|_{2}^{2}+\lambda\left\|\alpha_{k}\right\|_{1}$，得到字典矩阵$B$和稀疏向量$\{\alpha_{k}\}$。

2. 去噪步骤：
   - 对带噪图像分块，使用训练好的字典矩阵$B$求解各块的稀疏向量$\{\alpha_{k}\}$；
   - 重建图像块：通过$I_{k}' = B \alpha_{k}$得到去噪后的图像块；
   - 拼接：将去噪后的图像块拼接为完整的重建图像$\{I_{k}'\}$。