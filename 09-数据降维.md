# 9.1 什么是降维？

## 降维的基本定义

### 维数（又称维度）

- 数学中：独立参数的数目；
- 物理中：独立时空坐标的数目。

### 不同维度的直观理解

- 点是0维：在点上定位一个点，不需要参数；
- 直线是1维：在直线上定位一个点，需要1个参数；
- 平面是2维：在平面上定位一个点，需要2个参数；
- 体是3维：在体上定位一个点，需要3个参数。

### 维数灾难

在高维情形下出现的数据样本稀疏，距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难”。

### 降维的定义

降维（Dimensionality Reduction）是将高维数据转换为低维数据的技术，同时尽量保留原始数据的重要信息。

### 高维数据的低维嵌入

观测收集的数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维嵌入。例如，三维空间中观察到的样本点，可能对应二维空间中的曲面。

## 为什么要降维？

在原始的高维空间中，包含冗余信息和噪声信息，会在实际应用中引入误差，影响准确率；
- 降维可以提取数据内部的本质结构，减少冗余信息和噪声信息造成的误差，提高算法效率。

## 降维的典型应用

### 高维特征可视化分析

- 人类无法直观理解高维度特征嵌入，通过降维算法将高维特征投影至二维空间可视化分析，观察到相似内容的图像、文本在低维可视化空间中具备更相近的空间位置，便于高维特征的直观理解与科学分析。

### 金融数据分析

- 从大量的金融数据中，降维算法可以识别和提取出主要的、潜在的驱动因子，这些因子能够解释资产价格变动的大部分原因。通过该分析方法，投资者可以更准确地理解市场动态，预测未来趋势，以及优化投资策略。

## 代表性降维方法

### 线性方法

- 主成分分析 (Principal Component Analysis, PCA)
- 因子分析 (Factor Analysis, FA)
- 独立成分分析 (Independent Component Analysis, ICA)

### 非线性方法

- 局部线性嵌入 (Locally Linear Embedding, LLE)

# 9.2 主成分分析 PCA

## 主成分分析的概述

- 【1901年K. Pearson提出，针对非随机变量】
- 【1933年H. Hotelling提出，推广到随机向量】
- 主成分分析(Principal Component Analysis, PCA)是最常用的一种降维方法。PCA将原有众多具有一定相关性的指标重新组合成一组少量互相无关的综合指标。
- 核心目标：使用较少的数据维度保留住较多的原数据特性，将$D$维数据集$\{x_{n}\}$（$n=1,2, ..., N$）降为$M$维（$M<D$）。

## 主成分分析的推导-最大方差思想

- 核心思想：使得降维后数据的方差尽可能大，即最大化投影后数据的方差。
- 基础设定：首先考虑$M=1$，定义投影方向为$D$维向量$u_{1}$，令$u_{1}^{T} u_{1}=1$；每个数据点$x_{n}$在新空间中表示为标量$u_{1}^{T} x_{n}$，样本均值$\bar{x}=\frac{1}{N} \sum_{n=1}^{N} x_{n}$，样本均值在新空间中表示为$u_{1}^{T} \bar{x}$。
- 投影后样本方差：
$$\frac{1}{N} \sum_{n=1}^{N}{u_{1}^{T} x_{n}-u_{1}^{T} \bar{x}}^{2}=u_{1}^{T} S u_{1}$$
其中原样本协方差矩阵$S=\frac{1}{N} \sum_{n=1}^{N}(x_{n}-\bar{x})(x_{n}-\bar{x})^{T}$。
- 优化目标：最大化$u_{1}^{T} S u_{1}$，约束条件$u_{1}^{T} u_{1}=1$。
- 拉格朗日乘子法：构造函数$u_{1}^{T} S u_{1}+\lambda_{1}(1-u_{1}^{T} u_{1})$，对$u_{1}$求导并置零得$S u_{1}=\lambda_{1} u_{1}$，即$u_{1}$是$S$的特征向量。
- 结论：$u_{1}^{T} S u_{1}=\lambda_{1}$，当$u_{1}$是$S$最大特征值对应的特征向量时，方差取到极大值，称$u_{1}$为第一主成分。
- 一般情况（$M>1$）：新空间中数据方差最大的最佳投影方向由协方差矩阵$S$的$M$个特征向量$u_{1}, u_{2}, ..., u_{M}$定义，分别对应$M$个最大的特征值$\lambda_{1}, \lambda_{2}, ..., \lambda_{M}$。通过依次在补空间中获取方差最大的维度，最终得到$M$维空间。

## 主成分分析的推导-最小均方误差思想

- 核心思想：使原数据与降维后的数据（在原空间中的重建）的误差最小。
- 基础设定：定义一组正交的$D$维基向量$\{u_{i}\}$（$i=1,2, ..., D$），满足$u_{i}^{T} u_{j}=\delta_{i j}$（$i=j$时$\delta_{i j}=1$；$i \neq j$时$\delta_{i j}=0$），每个数据点可表示为基向量的线性组合$x_{n}=\sum_{i=1}^{D} \alpha_{n i} u_{i}$，相当于坐标变换$\{x_{n 1}, x_{n 2}, ..., x_{n D}\}$变换到$\{\alpha_{n 1}, \alpha_{n 2}, ..., \alpha_{n D}\}$。
- 重建数据：$\tilde{x}_{n}=\sum_{i=1}^{M}\left(z_{n j} u_{i}+\sum_{i=M+1}^{D} b_{i}\right) u_{i}$，其中$z_{n j}=x_{n}^{T} u_{j}$（$j=1,2, ..., M$），$b_{j}=\overline{x}^{T} u_{j}$（$j=M+1, M+2, ..., D$）。
- 失真度（均方误差）：
$$J=\frac{1}{N} \sum_{n=1}^{N}\left\|x_{n}-\tilde{x}_{n}\right\|^{2}$$
- 推导过程：$x_{n}-\tilde{x}_{n}=\sum_{i=M+1}^{D}{(x_{n}-\bar{x})^{T} u_{i}} u_{i}$，代入失真度公式得：
$$J=\frac{1}{N} \sum_{n=1}^{N} \sum_{i=M+1}^{D}\left(x_{n}^{T} u_{i}-\overline{x}^{T} u_{i}\right)^{2}=\sum_{i=M+1}^{D} u_{i}^{T} S u_{i}$$
- 拉格朗日乘子法：构造函数$\tilde{J}=\sum_{i=M+1}^{D} u_{i}^{T} S u_{i}+\sum_{i=M+1}^{D} \lambda_{i}(1-u_{i}^{T} u_{i})$，求导并置零得$S u_{i}=\lambda_{i} u_{i}$。
- 结论：$J=\sum_{i=M+1}^{D} \lambda_{i}$，当$J$最小时，取$D-M$个最小的特征值，主子空间对应$M$个最大特征值。

这两种思想是等价的，只是两种不同的理解方法。

## 主成分分析的计算步骤

1. 计算给定样本$\{x_{n}\}$（$n=1,2, ..., N$）的均值$\bar{x}$和协方差矩阵$S$；
2. 计算$S$的特征向量与特征值；
3. 将特征值从大到小排列，前$M$个特征值$\lambda_{1}, \lambda_{2}, ..., \lambda_{M}$所对应的特征向量$u_{1}, u_{2}, ..., u_{M}$构成**投影矩阵**。

## 主成分分析的应用

- 特征脸（Eigenfaces）：通过PCA提取人脸图像的主成分，得到特征脸，用于人脸识别等任务。
- 高维数据处理：当样本维数$D$远大于样本个数$N$（如1000张100×100像素的人脸图像，$D=10000$，$N=1000$），利用奇异值分解（SVD）简化计算：
  - 协方差矩阵$S=N^{-1} X^{T} X$（$X$为$N \times D$维数据矩阵，行向量为$(X_{n}-\overline{X})^{T}$）；
  - 对$\frac{1}{N} X X^{T}$求特征值$\lambda_{i}$和特征向量$v_{i}$，调整$u_{i} \propto X^{T} v_{i}$的尺度，令$\left\|u_{i}\right\|=1$，得$u_{i}=\frac{1}{N \lambda_{i}^{1 / 2}} X^{T} v_{i}$；
  - 令$v_{i}=X u_{i}$，则$\frac{1}{N} X X^{T} v_{i}=\lambda_{i} v_{i}$（$N \times N$维矩阵运算，简化计算）。

## 概率主成分分析

### PCA的概率表示

- 模型设定：隐藏变量$Z$产生$D$维观测变量$X$，$X=W z+\mu+\varepsilon$，其中：
  - $z$为$M$维隐藏变量，满足$p(z)=\mathcal{N}(z | 0, I)$；
  - $X$以$Z$为条件的分布满足$p(x | z)=\mathcal{N}\left(x | W z+\mu, \sigma^{2} I\right)$；
  - $\varepsilon$为高斯噪声，均值为0，协方差为$\sigma^{2} I$。
- 边缘分布：$p(x)=\int p(x | z) p(z) d z=\mathcal{N}(x | \mu, C)$，其中$C=W W^{T}+\sigma^{2} I$。
- 期望与协方差：
$$\mathbb{E}[X]=\mathbb{E}[W z+\mu+\varepsilon]=\mu$$
$$\begin{aligned} cov[X] & =\mathbb{E}\left[(W z+\varepsilon)(W z+\varepsilon)^{T}\right] \\ & =\mathbb{E}\left[W z z^{T} W^{T}\right]+\mathbb{E}\left[\varepsilon \varepsilon^{T}\right]=W W^{T}+\sigma^{2} I \end{aligned}$$

### 最大似然估计求解

- 对数似然函数：
$$ln p\left(X | \mu, W, \sigma^{2}\right)=\sum_{n=1}^{N} ln p\left(x_{n} | \mu, W, \sigma^{2}\right)$$
$$=-\frac{N D}{2} ln (2 \pi)-\frac{N D}{2} ln |C|-\frac{1}{2} \sum_{n=1}^{N}\left(x_{n}-\mu\right)^{T} C^{-1}\left(x_{n}-\mu\right)$$
$$=-\frac{N}{2}\{D ln (2 \pi)-ln |C|-Tr\left(C^{-1} S\right)\}$$
（其中$S$为协方差矩阵）
- 对$\mu$求导置零并代回，得噪声方差的最大似然估计：
$$\sigma_{ML}^{2}=\frac{1}{D-M} \sum_{i=M+1}^{D} \lambda_{i}$$

## 核主成分分析

### 核心思想

将主成分分析的线性假设一般化，使之适应非线性数据，通过非线性映射$\phi(X)$将样本$x_{n}$映射到高维空间，再在高维空间中进行PCA。

### 数学推导

- 传统PCA：$D$维样本$\{x_{n}\}$（$n=1,2, ..., N$），中心化$\sum_{n} x_{n}=0$，协方差矩阵$S=\frac{1}{N} \sum_{n=1}^{N} x_{n} x_{n}^{T}$，特征方程$S u_{i}=\lambda_{i} u_{i}$（$u_{i}^{T} u_{i}=1$）。
- 核PCA：非线性映射$\phi(x_{n})$，中心化$\sum_{n} \phi(x_{n})=0$，协方差矩阵$S_{\phi}=\frac{1}{N} \sum_{n=1}^{N} \phi(x_{n}) \phi(x_{n})^{T}$，特征方程$S_{\phi} v_{i}=\lambda_{i} v_{i}$，其中$v_{i}=\sum_{n=1}^{N} \alpha_{i n} \phi(x_{n})$。
- 核函数替代：$k(x_{n}, x_{m})=\phi(x_{n})^{T} \phi(x_{m})$，代入特征方程得：
$$\frac{1}{N} \sum_{n=1}^{N} \phi\left(x_{n}\right) \phi\left(x_{n}\right)^{T} \sum_{m=1}^{N} \alpha_{i m} \phi\left(x_{m}\right)=\lambda_{i} \sum_{n=1}^{N} \alpha_{i n} \phi\left(x_{n}\right)$$
$$\stackrel{\phi\left(x_{l}\right)^{T}}{\to} \frac{1}{N} \sum_{n=1}^{N} k\left(x_{l}, x_{n}\right) \sum_{m=1}^{N} \alpha_{i m} k\left(x_{n}, x_{m}\right)=\lambda_{i} \sum_{n=1}^{N} \alpha_{i n} k\left(x_{l}, x_{n}\right)$$
$$\to K^{2} \alpha_{i}=\lambda_{i} N K \alpha_{i} \to K \alpha_{i}=\lambda_{i} N \alpha_{i}$$
（其中$K$为核矩阵，$K_{nm}=k(x_n,x_m)$）

## 讨论：PCA v.s. LDA（线性判别分析）

- PCA：追求降维后能够最大化保持数据内在信息，通过衡量投影方向上的数据方差判断重要性，但对数据的区分作用不大，可能使数据点混杂在一起。
- LDA：不追求保持数据最多信息，而是希望数据在降维后能够很容易地被区分开（LDA数据有标注）。

## PCA的优点与局限性

### 优点

- 普适性高，最大程度保持原有数据的信息；
- 可对主成分重要性排序，略去部分维数实现降维、简化模型或数据压缩；
- 完全无参数限制，计算过程无需人为设定参数或经验模型干预，结果仅与数据相关。

### 局限性

- 假设模型是线性的，仅能处理主成分间线性关系的数据；
- 假设数据具有较高信噪比，将最高方差的一维向量视为主成分，方差较小的变化被认为是噪声。

# 9.3 因子分析

## 因子分析的定义

- 【1904年C. Spearman提出】
- 因子分析是指从数据内部的关联关系出发，将多个可观测变量归结为少数几个因子（隐变量），从而达到降维目的的一种技术。
- 隐变量：是指不能直接观测到的变量，例如逻辑能力、语言能力等。

## 因子模型

### 因子模型（Factor Model）

- 设有$p$维随机向量$X$，$E(X)=\mu$，$Var(X)=\sum$。
- 重要的因子模型：正交因子模型
$$\left\{\begin{array}{l} X_{1}=\mu_{1}+a_{11} F_{1}+a_{12} F_{2}+...+a_{1 m} F_{m}+\varepsilon_{1} \\ X_{2}=\mu_{2}+a_{21} F_{1}+a_{22} F_{2}+...+a_{2 m} F_{m}+\varepsilon_{2} \\ \vdots \\ X_{p}=\mu_{p}+a_{p 1} F_{1}+a_{p 2} F_{2}+...+a_{p m} F_{m}+\varepsilon_{p} \end{array}\right.$$
- 化简形式：$X=\mu+A F+\varepsilon$
- 术语说明：
  - $F$称为公共因子；
  - $\varepsilon$称为特殊因子；
  - $a_{i j}$称为第$i$个变量在第$j$个因子上的因子载荷；
  - $A=(a_{i j})_{p \times m}$矩阵称为**载荷矩阵**。

## 正交因子模型

### 正交因子模型假设

1. $E(F)=0$，$Var(F)=E\left(F F^{T}\right)=I$（正交性）；
2. $E(\varepsilon)=0$，$Var(\varepsilon)=E\left(\varepsilon \varepsilon^{T}\right)=\Phi=diag\left\{\phi_{1}, ..., \phi_{p}\right\}$；
3. $Cov(F, \varepsilon)=E\left(F \varepsilon^{T}\right)=0$。

### 模型推导结论

- $Cov(X,F)=Cov(AF+\varepsilon ,F)=Cov(AF,F)+Cov(\varepsilon ,F)=A$；
- $\sum=Var(X)=Var(A F+\varepsilon)=A A^{T}+\Phi$；
- $\sigma_{i i}=Var(X_{i})=h_{i}^{2}+\phi_{i}$，其中$h_{i}^{2}=a_{i 1}^{2}+a_{i 2}^{2}+\cdots+a_{i m}^{2}$；
  - $a_{i j}^{2}$表示第$j$个公共因子对$x_{i}$的方差贡献；
  - $\phi_{i}$表示第$i$个特殊因子对$x_{i}$的方差贡献。

## 因子提取

- 核心目标：根据数据的协方差矩阵$\sum$求解载荷矩阵$A$和特殊因子$\varepsilon$。
- 求解算法：主成分法、极大似然法、主因子法等。

### 主成分法

- 设$\sum$的特征值为$\lambda_{1} ≥\cdots ≥\lambda_{p}$，$u_{1}, ..., u_{p}$为对应的标准正交化特征向量，有$\sum ≈A A^{T}$，其中$A=(\sqrt{\lambda_{1}} u_{1}, ..., \sqrt{\lambda_{p}} u_{p})$；
- 忽略特征值较小的$p-m$个特征向量，正交因子分析的载荷矩阵$A$可写为$A=(\sqrt{\lambda_{1}} u_{1}, \sqrt{\lambda_{2}} u_{2}, ..., \sqrt{\lambda_{m}} u_{m})$；
- 用$\Phi$补全$\sum$的对角线部分，即$\phi_{i}=\sqrt{\sigma_{i i}-h_{i}^{2}}$。

## 模型求解步骤

1. 因子提取：求解因子模型中的载荷矩阵$A$和特殊因子$\varepsilon$，旨在将原始高维数据归纳为较少的因子，以解释数据的方差；
2. 因子旋转：通过旋转因子载荷矩阵$A$，对提取的因子进行变换，增强因子的解释性；
3. 因子得分：基于旋转后的载荷矩阵$A$，计算每个样本在这些因子上的得分值，使原本的高维数据能够在较低维度上被解释和分析。

## 因子旋转

### 旋转前提

载荷矩阵不唯一，即载荷矩阵$A$与任意正交阵$T$相乘仍符合正交因子模型假设。

### 旋转目标

确定一个结构简单（每个可观测变量仅在一个公共因子上有较大的载荷）的载荷矩阵$A$。

### 旋转原理

设$T$为$m \times m$正交阵，则$X=\mu+A F+\varepsilon=\mu+A T T^{T} F+\varepsilon$，记$\tilde{A}=A T$，$\tilde{F}=T^{T} F$，则因子模型可写为$X=\mu+\tilde{A} \tilde{F}+\varepsilon$，其中：
- $\tilde{F}$仍满足$E(\tilde{F})=0$，$Var(\tilde{F})=I$，$Cov(\tilde{F}, \varepsilon)=0$；
- 方差结构不变：$\sum =A A^{T}+\Phi=\tilde{A} \tilde{A}^{T}+\Phi$。

### 常见因子旋转算法

- 最大方差旋转法（Varimax）；
- 最大四次方法（Quartimax）。

### 最大方差旋转法（Varimax）

- 核心思想：使得可观测变量在每个因子上的负荷尽可能分散，即在每个因子上的方差最大。
- 两因子正交旋转示例：
  - 设旋转角度为$\varphi$，旋转矩阵为$\Gamma=\left[\begin{array}{cc} cos \varphi & -sin \varphi \\ sin \varphi & cos \varphi \end{array}\right]$；
  - 旋转后载荷矩阵$B=A\Gamma$：
$$B=\left[\begin{array}{cc}a_{11} cos \varphi+a_{12} sin \varphi & -a_{11} sin \varphi+a_{12} cos \varphi \\ \vdots & \vdots \\ a_{p 1} cos \varphi+a_{p 2} sin \varphi & -a_{p 1} sin \varphi+a_{p 2} cos \varphi \end{array}\right]=\left[\begin{array}{cc} b_{11} & b_{12} \\ \vdots & \vdots \\ b_{p 1} & b_{p 2} \end{array}\right]$$
  - 优化目标：
$$max _{\varphi} V=\left[\frac{1}{p} \sum_{i=1}^{p}\left(b_{i 1}^{2}\right)^{2}-\left(\frac{1}{p} \sum_{i=1}^{p} b_{i 1}^{2}\right)^{2}\right]+\left[\frac{1}{p} \sum_{i=1}^{p}\left(b_{i 2}^{2}\right)^{2}-\left(\frac{1}{p} \sum_{i=1}^{p} b_{i 2}^{2}\right)^{2}\right]$$
  - 求解：根据求极值原理，求得令导数$\frac{d V}{d \varphi}=0$的$\varphi$，即为因子旋转角度。

## 因子得分

### 核心目标

根据$p$维可观测变量$X$和载荷矩阵$A$，计算每个样本在$m$个公共因子$F$上的得分值$\widehat{F}$。

### 特点

$m$维因子得分$\widehat{F}$比$p$维可观测变量$X$更能体现样本的特点。

### 求解算法

- 最小二乘法、极大似然估计法等。

### 最小二乘法求解

对于式$X-\mu=A F+\varepsilon$，视特殊因子$\varepsilon$为随机误差，公共因子$F$为回归系数，由于$Var(\varepsilon)=\Phi$，通过加权最小二乘法，得到：
$$\hat{F}=\left(A^{T} \Phi^{-1} A\right)^{-1} A^{T} \Phi^{-1}(X-\mu)$$

## 因子分析的应用

### 示例背景

给定52个学生的6科成绩（数学、物理、化学、语文、历史和英语），使用正交因子模型分析科目间的相关性，将6科成绩归结为2个因子（文科因子、理科因子）。

### 协方差矩阵

$$\begin{aligned} Var(X)=\sum= \left[\begin{array}{cccccc} 1.000 & 0.647 & 0.696 & -0.561 & -0.456 & -0.439 \\ 0.647 & 1.000 & 0.573 & -0.503 & -0.351 & -0.458 \\ 0.696 & 0.573 & 1.000 & -0.380 & -0.274 & -0.244 \\ -0.561 & -0.503 & -0.380 & 1.000 & 0.813 & 0.835 \\ -0.456 & -0.351 & -0.274 & 0.813 & 1.000 & 0.819 \\ -0.439 & -0.458 & -0.244 & 0.835 & 0.819 & 1.000 \end{array}\right] \end{aligned}$$

### 因子提取结果（载荷矩阵）

| 因子       | 数学   | 物理   | 化学   | 语文   | 历史   | 英语   |
| ---        | ---    | ---    | ---    | ---    | ---    | ---    |
| 文科因子   | −0.676 | −0.676 | −0.676 | 0.917  | 0.917  | 0.883  |
| 理科因子   | 0.562  | 0.427  | 0.656  | 0.104  | 0.239  | 0.266  |
| 特殊因子$\phi_i$ | 0.228  | 0.459  | 0.333  | 0.148  | 0.210  | 0.150  |

### 因子旋转结果（旋转后载荷矩阵）

| 因子       | 数学   | 物理   | 化学   | 语文   | 历史   | 英语   |
| ---        | ---    | ---    | ---    | ---    | ---    | ---    |
| 文科因子   | -0.089 | -0.132 | 0.156  | 0.845  | 0.909  | 0.954  |
| 理科因子   | 0.833  | 0.657  | 0.890  | -0.133 | 0.046  | 0.098  |
| 特殊因子$\phi_i$ | 0.228  | 0.459  | 0.333  | 0.148  | 0.210  | 0.150  |

### 因子得分应用

根据因子得分可将学生划分为四类：文科优秀&理科优秀、文科较差&理科优秀、文科较差&理科较差、文科优秀&理科较差，实现学生成绩的低维分类分析。

# 9.4 独立成分分析 ICA

## 独立成分分析的定义

- 独立成分分析 (Independent Component Analysis, ICA) 是一种用于信号分离的统计方法，它通过提取数据中的独立成分来识别和分离出数据内部的本质结构，这些独立成分不仅减少了冗余信息和噪声对数据分析的干扰，还能保留关键信息。
- 典型示例：以鸡尾酒派对为例，通过ICA从混合的观测信号中分离出各个独立的声源信号。

## 模型假设

### ICA模型变量定义

- $x$：观测信号；
- $A$：混合矩阵；
- $s$：独立成分；
- 模型表达式：$x = A s$。

### ICA模型求解目标

通过估计一个权重向量 $w$，使得从观测信号 $x$ 中分离出的信号 $s = W x$ 是相互独立的。

### 模型先验假设

- 独立成分统计独立；
- 独立成分服从非高斯分布。

### 典型算法

JADE、Infomax ICA、FastICA等。

## FastICA算法

### 原理

- 基于非高斯性最大化原理的独立成分分析方法，旨在从多维观测数据中提取出统计独立的信号源。
- 分离过程中，可通过对分离结果的非高斯性度量来表示分离结果间的相互独立性，当非高斯性度量达到最大时，则表明已完成对各独立分量的分离。

### 整体过程

1. 数据预处理，包含去中心化和白化；
2. 寻找最大非高斯方向并计算独立成分，提取独立成分后进行正交化处理；
3. 重复步骤2对其他独立成分进行提取。

### 步骤1：数据预处理

#### 去中心化

- 对每个观测向量 $x$ 计算均值 $\mu$，变换为 $x' = x - \mu$；
- 目的：调整数据均值为零，方便后续的白化和独立成分提取。

#### 白化

1. 计算中心化后数据的协方差矩阵 $C = E[x' x'^T]$；
2. 对协方差矩阵进行特征值分解 $C = E D E^T$（$D$ 是特征值对角矩阵，$E$ 是对应的特征向量矩阵）；
3. 使用特征值和特征向量对数据进行变换，得到白化数据 $x'' = E D^{-1/2} E^T x'$；
- 目的：使数据协方差为单位矩阵（协方差为零、方差为1），简化后续的独立成分提取过程。

### 步骤2：寻找最大非高斯方向并计算独立成分

1. 初始化：选择一个随机的单位向量 $w$ 作为权重向量的初始值；
2. 非线性变换：为了最大化信号的非高斯性，采用非线性函数 $g(•)$ 对数据进行变换；
3. 权重更新：根据固定点迭代法，使用输入数据 $x''$ 和当前的权重向量 $w$ 进行权重更新：
$$W^+ = E\left[x'' g\left(W^T x''\right)\right] - E\left[g'\left(W^T x''\right)\right] W$$
   - 其中，$E[x'' g(W^T x'')]$ 表示每个样本应用非线性函数后与输入数据的期望值，$E[g'(W^T x'')]$ 是用于调整的校正项；
   - 更新后，需要对 $w^+$ 进行归一化；
4. 计算独立成分：使用得到的权重向量 $w$ 将白化后的数据投影到独立成分上，即 $s = w^T x''$；
5. 正交化：如果提取多个独立成分，需要对新的权重向量进行正交化处理，以确保它们相互独立。

### 步骤3：重复步骤2，对其他独立成分进行提取

依次提取所有独立成分，直至完成所有信号源的分离。

## 独立成分分析的应用

### 脑电图信号分析

#### 应用背景

- 脑电图 (EEG) 信号分析在神经科学研究和临床诊断中至关重要。然而，EEG 信号常常受到伪影干扰（例如眼动和心跳），这些干扰信号会掩盖和混淆真正的脑电活动，使得准确分析变得困难。
- 需求：提取脑电信号中的关键成分，去除冗余信息和噪声，从而揭示大脑的真实活动模式并提高分析的精度。

#### 信号处理流程

1. 输入原始信号（包含MEG、EEG等多通道信号）；
2. 通过ICA分离出独立成分，可视化独立成分以确定噪声信号；
3. 剔除噪声对应的独立成分，将剩余独立成分重新组合，得到去噪后的信号。

#### 效果示例

- 原始信号：包含MEG 1411、MEG 1421、MEG 1431等多通道混合信号，含伪影干扰；
- 去噪后的信号：剔除伪影成分后，保留纯净的脑电/磁信号。

## 讨论：ICA vs. PCA & 因子分析

| 对比维度 | ICA | PCA | 因子分析 |
| --- | --- | --- | --- |
| 优化目标 | 最大化信号的非高斯性，以提取独立成分 | 侧重于数据的降维和方差最大化 | 提取因子解释数据中的结构关系 |
| 解释性 | 有助于揭示数据中潜藏的独立源信号或独立成分 | 解释数据中的方差，本身没有明确含义 | 有助于理解变量之间的潜在关系 |
| 应用场景 | 常用于盲信号分离，也可用于降维 | 数据的降维和可视化 | 从复杂的数据中提炼决定性因素 |

## 小结：优点和局限性

### ICA的优点

- 灵活性：可通过不同的非线性函数来调整算法的行为，以适应不同的数据分布特征；
- 特征提取能力：可识别数据中重要特征，应用于分类等其他任务；
- 物理解释性：提取的独立成分通常具有明确的物理或现实意义，尤其在信号处理领域。

### ICA的局限性

- 假设原信号是非高斯分布的，也就决定了它对高斯假设的数据效果不佳；
- 假设原信号是统计独立的；
- 算法处理复杂数据集和多源信号时，可能会出现不收敛问题。

# 9.5 局部线性嵌入 LLE

## 局部线性嵌入的定义

- 局部线性嵌入 (Local Linear Embedding, LLE) 是一种基于流形假设的非线性数据降维方法。
- 前提假设：该方法假设采样数据所在的低维流形在局部是线性的，每个采样点可以用它的近邻点线性表示。
- 学习目标：在低维空间中保持每个邻域中的权值不变，即假设嵌入映射在局部是线性的条件下，最小化重构误差。
- 参考文献：Nonlinear Dimensionality Reduction by Locally Linear Embedding, Sam T.Roweis and Lawrence K.Saul, SCIENCE VOL 290, 22 DECEMBER 2000。

## 局部线性嵌入的流程

### 计算步骤

1. K近邻重构：基于局部线性假设，对于任意一个样本，可用其距离最近的K个样本加权表示；
2. 重构系数求解：通过最小化重构误差，计算所有样本的重构系数；
3. 低维坐标求解：保持样本重构系数不变，通过最小化降维后的重构误差，求解低维空间坐标。

### 步骤1：K近邻重构

- 前提假设：基于局部线性假设，每个点都能用其周围的点线性表示出来。
- 具体操作：对于给定数据集 $X=\{x_{1}, x_{2}, \cdots, x_{N}\}$ 中的样本 $x_{i}$，寻找其K近邻，表示为 $\{x_{i j}\}$（其中 $j=1,2, \cdots, K$）；使用 $\{x_{i j}\}$ 对 $x_{i}$ 进行重构，即求一组权值 $\{w_{i j}\}$，使得 $x_{i}=\sum_{j} w_{i j} x_{i j}$，其中 $\{w_{i j}\}$ 满足 $\sum _{j} w_{i j}=1$。

### 步骤2：重构系数求解

- 优化目标：
$$min \sum_{i}|x_{i}-\sum_{j} w_{i j} x_{i j}|^{2}$$
$$s. t. \sum_{j} w_{i j}=1$$
- 其中 $C_{j k}^{i}=(x_{i}-x_{j}) \cdot(x_{i}-x_{k})$，可求得：
$$w_{i j}=\frac{\sum_{k}(C^{i})_{j k}^{-1}}{\sum_{j k}(C^{i})_{j k}^{-1}}$$

### 步骤3：低维坐标求解

- 目标：将 $X=\{x_{1}, x_{2}, \cdots, x_{N}\}$ 降维为 $Y=\{y_{1}, y_{2}, \cdots, y_{N}\}$，使原本的重构系数 $W=\{w_{ij}\}$ 尽量不变。
- 优化目标：
$$min \sum_{i}|y_{i}-\sum_{j} w_{i j} y_{i j}|^{2}$$
- 转化形式：可转化为 $min \sum_{i j} M_{i j}(y_{i} \cdot y_{j})$，其中：
$$M=(I-W)^{T}(I-W)$$
- 求解结果：选择 $M$ 的最小 $d$ 个非零特征值对应的特征向量构建 $N \times d$ 的矩阵，即可得到局部线性嵌入的降维结果。

## 局部线性嵌入的效果

- 局部线性嵌入与主成分分析降维效果比较，有效避免了线性方法降维后局部结构改变的问题。
- 直观对比：降维前数据的局部结构，经PCA降维后局部结构被破坏，经局部线性嵌入降维后局部结构得以保留。

## 局部线性嵌入优缺点

### 优点

- 非线性：可以捕捉数据中复杂的非线性信息；
- 非迭代：可以一次运算得到降维后的效果，较为高效；
- 保持局部结构：在降维后，数据点之间的相对位置和距离也能得到较好的保留；
- 参数较少：仅涉及调节K近邻的数量，使得模型的调整较为直观。

### 缺点

- 噪声点可能会破坏数据点之间的局部线性关系，从而影响降维结果的准确性；
- 当数据集由多个不连通的流形组成时，可能无法正确地进行降维；
- 目标函数是一个非凸优化问题，这意味着它可能陷入局部最优解。

## 局部线性嵌入的应用

### 应用场景：社交网络数据可视化

- 应用背景：社交网络的发展逐渐催生出不同的用户类群，同一类群内的用户具有相似行为模型。通过数据降维，将有助于了解各个用户类群的特点并加以区分。
- 应用优势：局部线性嵌入能够在降维后保留样本的局部结构，清晰展示出用户类群内的一致性，以及不同用户类群之间的差异性。
- 数据输入：关注数、点赞数、评论数、观看数、收藏数等多维度用户行为数据。