# 11.1 什么是概率图模型？

## 概率图模型的定义

- 概率图模型(Probabilistic Graphical Model, PGM)是一种用图结构来表示和推断多元随机变量之间关系的概率模型。
- 结点：随机变量或一组随机变量；
- 连接弧：随机变量之间的关系。

## 概率图模型的分类

- 根据图模型的结构不同，概率图模型主要分为贝叶斯网络和马尔可夫场。
- 贝叶斯网络由有向无环图组成；
- 马尔可夫场由无向图组成。

## 贝叶斯网络的概率分布

- 贝叶斯网络是一种有向无环概率图模型，表示了随机变量之间的直接依赖关系。每个结点定义了一个关于其父结点的条件概率分布。
- 父结点：影响子结点的前驱结点；
- 子结点：受父结点影响的后继结点。
- 考虑含有K个随机变量 $X=\{x_{1}, ..., x_{K}\}$ 的贝叶斯网络，其联合概率分布为每个结点上的条件概率分布的乘积：
$$p(X)=\prod_{k=1}^{K} p\left(x_{k} | pa_{k}\right)$$
其中，$pa_{k}$ 代表子节点 $x_{k}$ 的父节点集合。
- 例子：考虑包含随机变量 $X=\{x_{1}, ..., x_{7}\}$ 的贝叶斯网络，联合分布为：
$$p\left(x_{1}, ..., x_{7}\right)=p\left(x_{1}\right) p\left(x_{2}\right) p\left(x_{3}\right) p\left(x_{4} | x_{1}, x_{2}, x_{3}\right) p\left(x_{5} | x_{1}, x_{3}\right) p\left(x_{6} | x_{4}\right) p\left(x_{7} | x_{4}, x_{5}\right)$$
其中，$x_{1}$、$x_{2}$、$x_{3}$ 没有父节点；$x_{4}$ 父节点为 $x_{1}$、$x_{2}$、$x_{3}$；$x_{5}$ 父节点为 $x_{1}$、$x_{3}$；$x_{6}$ 父节点为 $x_{4}$；$x_{7}$ 父节点为 $x_{4}$、$x_{5}$。

## 马尔可夫场的概率分布

- 马尔可夫场是一种无向概率图模型，通过对变量间联合概率分布的建模描述变量间关系。马尔可夫场的概率分布通过团上的势函数定义。
- 无向图的团(clique)：任意两个结点之间都有连接的子集。
  - 两个结点的团：$\{x_{1}, x_{2}\}$、$\{x_{2}, x_{3}\}$、$\{x_{3}, x_{4}\}$、$\{x_{2}, x_{4}\}$、$\{x_{1}, x_{3}\}$；
  - 三个结点的团：$\{x_{1}, x_{2}, x_{3}\}$、$\{x_{2}, x_{3}, x_{4}\}$；
- 最大团：不能被其他团所包含的团。
- 势函数：对于团 $Q$，$Q$ 中的元素为 $x_{Q}$，定义函数 $\psi_{Q}(x_{Q})$ 为 $x_{Q}$ 的势函数，表示了团内局部变量的偏好，例如：
$$\psi\left(x_{1}, x_{2}\right)= \begin{cases}1, & if x_{1}=x_{2} \\ 0.1, & otherwise \end{cases}$$
- 马尔可夫场的联合概率分布基于最大团分解为多个因子的乘积，设所有最大团构成的集合为 $C$，则联合概率分布为：
$$P(X)=\frac{1}{Z} \prod_{Q \in C} \psi_{Q}\left(X_{Q}\right)$$
其中，$Z$ 为归一化因子：
$$Z=\sum_{X} \prod_{Q \in C} \psi_{Q}\left(X_{Q}\right)$$
- 例子：考虑包含随机变量 $X=\{x_{1}, x_{2}, x_{3}, x_{4}\}$ 的马尔可夫随机场，其联合概率分布为：
$$P(x_{1}, x_{2}, x_{3}, x_{4})=\frac{1}{Z} \psi_{1}(x_{1}, x_{2}, x_{3}) \psi_{2}(x_{2}, x_{3}, x_{4})$$
其中，最大团为 $\{x_{1}, x_{2}, x_{3}\}$、$\{x_{2}, x_{3}, x_{4}\}$。

# 11.2 贝叶斯网络

## 认识贝叶斯网络

### 贝叶斯网络的定义

- 贝叶斯网络(Bayesian Network)中，一个贝叶斯网络 $B$ 由结构 $G$ 和参数 $\Theta$ 两部分构成，即 $B=\langle G, \Theta \rangle$。
  - 网络结构 $G$ 是一个有向无环图，图中每个节点对应一个随机变量。若两个随机变量间有直接依赖关系，则它们由一条边连接起来。
  - 参数 $\Theta$ 定量描述了上述依赖关系。假设随机变量 $a$ 在 $G$ 中的父节点集合为 $\pi_{a}$，则 $\Theta$ 中包含了每个随机变量的条件概率 $\theta_{a | \pi_{a}}=P(a | \pi_{a})$。
- 贝叶斯网络通过有向无环图结构提供了对联合分布中随机变量条件独立性的紧凑表示。

## 条件独立性

### 随机变量的独立性

- 考虑两个随机变量 $\{a, b\}$，若满足 $p(a | b)=p(a)$ 且 $p(a, b)=p(a) p(b)$，称 $a$ 与 $b$ 独立，记为 $a \perp\!\!\!\perp b$。
- 含义：事件 $b$ 发生与否对 $a$ 不产生影响。

### 随机变量的条件独立性

- 考虑三个随机变量 $\{a, b, c\}$，若满足 $p(a|b,c)=p(a|c) \leftrightarrow p(a,b|c)=p(a|c)p(b|c)$，称在给定 $c$ 的条件下，$a$ 与 $b$ 条件独立，记为 $a \perp\!\!\!\perp b | c$。
- 含义：在事件 $c$ 发生的条件下，事件 $b$ 发生与否对 $a$ 不产生影响。

### 贝叶斯网络中的三种结构及条件独立性

#### 尾尾相连(tail-to-tail)

- $a$ 与 $b$ 的独立性关系：
$$p(a, b)=\sum_{c} p(a, b, c)=\sum_{c} p(a | c) p(b | c) p(c) \neq p(a) p(b)$$
即 $a \not\perp\!\!\!\perp b | \emptyset$。
- $a, b, c$ 之间的条件独立性关系：
$$\begin{array}{rlrl}
p(a,b,c)&=p(a | c) p(b | c) p(c) & \text{因子分解} \\
& =p(a | b, c) p(b | c) p(c) & \text{链式法则}
\end{array}$$
推导得 $p(a | c)=p(a | b, c)$，即 $a \perp\!\!\!\perp b | c$。

#### 头尾相连(head-to-tail)

- $a$ 与 $b$ 的独立性关系：
$$\begin{aligned}
p(a, b)&= \sum_{c} p(a, b, c) \\
& =\sum_{c} p(a) p(c | a) p(b | c) \\
& \neq p(a) p(b)
\end{aligned}$$
即 $a \not\perp\!\!\!\perp b | \emptyset$。
- $a, b, c$ 之间的条件独立性关系：
$$\begin{aligned}
p(a, b, c) & =p(a | c) p(c | b) p(b) \\
& =p(a | b, c) p(c | b) p(b)
\end{aligned}$$
推导得 $p(a | c)=p(a | b, c)$，即 $a \perp\!\!\!\perp b | c$。

#### 头头相连(head-to-head)

- $a$ 与 $b$ 的独立性关系：
$$\begin{aligned}
p(a, b) & =\sum_{c} p(a, b, c) \\
& =\sum_{c} p(a) p(b) p(c | a, b) \\
& =p(a) p(b)
\end{aligned}$$
即 $a \perp\!\!\!\perp b | \emptyset$。
- $a, b, c$ 之间的条件独立性关系：
$$\begin{aligned}
p(a, b, c)&= p(a) p(b) p(c | a, b) \\
& =p(a | b, c) p(c | b) p(b)
\end{aligned}$$
推导得 $p(a | c) \neq p(a | b, c)$，即 $a \not\perp\!\!\!\perp b | c$。

## D-分离

- D-分离(Directed-Separation)是一种用于判断概率图中节点集合间条件独立性的准则。
- 具体而言，对于概率图中的三个节点集合$\{A, B, C\}$，如果满足如下任一条件，则$A$与$B$关于$C$条件独立：
  1. $C$中的结点满足"头尾相连"或"尾尾相连"；
  2. "头头相连"的节点和它的任何后裔节点都不在$C$中。

### D-分离示例1

- 问题：$a$ 与 $b$ 是否关于 $c$ 条件独立？
- 分析：两个条件均不满足。
- 结论：$a$ 与 $b$ 关于 $c$ 不条件独立。

### D-分离示例2

- 问题：$a$ 与 $b$ 是否关于 $f$ 条件独立？
- 分析：节点 $f$ 满足"尾尾相连"，满足第一个条件。
- 结论：$a$ 与 $b$ 关于 $f$ 条件独立。

# 11.3 贝叶斯推理

## 什么是贝叶斯推理?

- 贝叶斯推理是基于贝叶斯定理来求解某些假设概率的一种方法，通常用于根据已知的数据来推断未知的模型参数或预测结果。
- 假设贝叶斯网络对应的随机变量集合 $X=\{x_{1}, x_{2}, ..., x_{N}\}$ 能分为 $X_{E}$ 和 $X_{F}$ 两个不相交的变量集，贝叶斯推理的目标就是计算目标变量 $X_{F}$ 的边缘概率 $P(X_{F})$ 或条件概率 $P(X_{F} | X_{E})$。
- 根据条件概率定义：
$$P\left(X_{F} | X_{E}\right)=\frac{P\left(X_{F}, X_{E}\right)}{P\left(X_{E}\right)}=\frac{P\left(X_{F}, X_{E}\right)}{\sum_{X_{F}} P\left(X_{F}, X_{E}\right)}$$
其中，联合概率 $P(X_{F}, X_{E})$ 可基于贝叶斯网络的因子分解获得。

## 贝叶斯推理方法分类

- 精确推理方法：希望计算出目标变量的边缘分布或条件分布的精确值，适用于模型简单或数据维度较小的情况。
  - 变量消除(Variable Elimination)
  - 信念传播(Belief Propagation)
- 近似推理方法：希望在较低的时间开销下获得概率的近似估计值，适用于模型较为复杂或数据维度较高的情况。
  - 马尔可夫链蒙特卡洛法 (Markov Chain Monte Carlo)
  - 变分推理(Variational Inference)

## 变量消除

- 变量消除(Variable Elimination)【1994年Nevin L. Zhang提出】。
  - 是一种动态规划算法，它利用贝叶斯网络所描述的条件独立性来削减计算目标概率值所需的计算量。
  - 是最直观的精确推理算法，也是构建其他精确推理算法的基础。

### 算法基本流程

1. 确定变量消除顺序；
2. 合并变量；
3. 消除变量；
4. 得到目标变量。

### 变量消除-示例

给定贝叶斯网络，求目标随机变量 $x_{5}$ 的边缘概率分布 $p(x_{5})$：
- 联合概率因子分解：
$$\begin{aligned}
p\left(x_{5}\right) & =\sum_{x_{4}} \sum_{x_{3}} \sum_{x_{2}} \sum_{x_{1}} p\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right) \\
& =\sum_{x_{4}} \sum_{x_{3}} \sum_{x_{2}} \sum_{x_{1}} p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) p\left(x_{3} | x_{2}\right) p\left(x_{4} | x_{3}\right) p\left(x_{5} | x_{3}\right)
\end{aligned}$$
- 选用变量消除顺序 $x_{1} \to x_{2} \to x_{4} \to x_{3}$，逐步化简：
$$\begin{aligned}
p\left(x_{5}\right) & =\sum_{x_{4}} \sum_{x_{3}} \sum_{x_{2}} p\left(x_{5} | x_{3}\right) p\left(x_{4} | x_{3}\right) p\left(x_{3} | x_{2}\right) \sum_{x_{1}} p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) \\
& =\sum_{x_{4}} \sum_{x_{3}} \sum_{x_{2}} p\left(x_{5} | x_{3}\right) p\left(x_{4} | x_{3}\right) p\left(x_{3} | x_{2}\right) m_{12}\left(x_{2}\right) \\
& =\sum_{x_{4}} \sum_{x_{3}} p\left(x_{5} | x_{3}\right) p\left(x_{4} | x_{3}\right) \sum_{x_{2}} p\left(x_{3} | x_{2}\right) m_{12}\left(x_{2}\right) \\
& =\sum_{x_{4}} \sum_{x_{3}} p\left(x_{5} | x_{3}\right) p\left(x_{4} | x_{3}\right) m_{23}\left(x_{3}\right) \\
& =\sum_{x_{3}} p\left(x_{5} | x_{3}\right) m_{23}\left(x_{3}\right) \sum_{x_{4}} p\left(x_{4} | x_{3}\right) \\
& =\sum_{x_{3}} p\left(x_{5} | x_{3}\right) m_{23}\left(x_{3}\right) m_{43}\left(x_{3}\right)
\end{aligned}$$
其中，$m_{12}(x_{2})$ 是对 $x_{1}$ 求和的结果（关于 $x_{2}$ 的函数），$m_{43}(x_{3})$ 是对 $x_{4}$ 求和的结果（关于 $x_{3}$ 的函数），最终结果是关于 $x_{5}$ 的函数。

### 变量消除的局限性

- 变量消除顺序难以确定：最优的变量消除顺序是一个NP-Hard问题【1999年Rina Dechter】。
  示例：计算 $p(e)=\sum_{a, b, c, d} p(a, b, c, d, e)$ 与 $p(c)=\sum_{d} p(d | c) \sum_{e} p(e | d) \sum_{b} p(c | b) \sum_{a} p(a) p(b | a)$ 时，消除顺序影响计算量。
- 重复计算：若计算多个边缘分布，变量消除法存在大量重复计算。

## 信念传播

- 信念传播(Belief Propagation)是一种精确推理算法，其将变量消除中的求和操作看作节点间消息传递的过程，通过预先存储每个节点的消息，较好地解决了求解多个边缘分布时的重复计算问题。
- 变量消除法中，消去变量 $x_{i}$ 的求和操作，在信念传播算法中被看作从 $x_{i}$ 向 $x_{j}$ 传递消息 $m_{i j}(x_{j})$，公式为：
$$m_{i j}\left(x_{j}\right)=\sum_{x_{i}} p\left(x_{j} | x_{i}\right) \prod_{k \in n(i) \backslash j} m_{k i}\left(x_{i}\right)$$
其中，$n(i)$ 表示结点 $x_{i}$ 的邻居结点。

### 消息传递在信念传播中的表示

变量消除示例中，变量消除过程可描述为节点间的消息传递过程，包括 $m_{12}(x_{2})$、$m_{23}(x_{3})$、$m_{43}(x_{3})$、$m_{35}(x_{5})$ 等消息的传递。

### 信念传播的消息传递过程

1. 指定一个根节点(例如 $x_{1}$)，所有叶结点向根节点传递消息，直到根节点收到所有邻接结点的消息；
2. 从根结点开始向叶结点传递消息，直到所有叶结点均收到消息。

### 边缘概率计算

- 在结束消息传播之后，对于节点 $x_{i}$ 的边缘概率分布 $p(x_{i})$，其正比于所接收消息的乘积，其中 $\alpha$ 为归一化因子：
$$p(x_{i})=\alpha \prod_{j \in n(i)} m_{j i}(x_{i})$$
- 示例：信念传播结束后，$p(x_{3})=\alpha m_{23}(x_{3}) m_{43}(x_{3}) m_{53}(x_{3})$。

# 11.4 隐马尔可夫模型 HMM

## 认识隐马尔可夫模型

### 隐马尔可夫模型 (Hidden Markov Model, HMM) 的定义

- 是一种用于建模时间序列数据的有向概率图模型。由一个隐藏的马尔可夫链随机生成隐藏状态序列 $Y=\{y_{1}, y_{2}, ..., y_{T}\}$，随后由状态产生观测随机序列 $X=\{x_{1}, x_{2}, ..., x_{T}\}$。
- 核心假设：
  - 齐次马尔可夫假设：任意时刻的隐藏状态只依赖于它前一个隐藏状态；
  - 独立观测假设：任意时刻的观察状态仅依赖于当前时刻的隐藏状态。

### 隐马尔可夫模型的参数定义

设隐藏状态集合 $Q=\{q_{1}, q_{2}, ..., q_{N}\}$ 和可能的观测状态集合 $V=\{v_{1}, v_{2}, ..., v_{M}\}$，则定义：
- 状态转移矩阵 $A=[a_{i j}]_{N \times N}$，其中 $a_{ij}=P(y_{t+1}=q_{j} | y_{t}=q_{i})$ 表示任意时刻t，从状态 $q_{i}$ 转移到 $q_{j}$ 的概率；
- 观测概率矩阵 $B=[b_{i j}]_{N \times M}$，其中 $b_{i j}=P(x_{t}=v_{i} | y_{t}=q_{j})$ 表示任意时刻t，从状态 $q_{j}$ 产生观测 $v_{i}$ 的概率；
- 初始隐藏状态概率分布 $\Pi=[\pi_{i}]_{N}$，其中 $\pi_{i}=P(y_{1}=q_{i})$。
- 一个HMM模型可由三元组表示：$\lambda=(\Pi, A, B)$。

### 隐马尔可夫模型要解决的三个基本问题

1. 概率计算问题：给定观测序列 $X=\{x_{1}, x_{2}, ..., x_{T}\}$ 以及模型 $\lambda=(\Pi, A, B)$，计算产生该观测序列的概率 $P(X | \lambda)$；
2. 学习问题：给定观测序列 $X=\{x_{1}, x_{2}, ..., x_{T}\}$，估计模型参数 $\lambda=(\Pi, A, B)$，使得该模型下出现观测序列的概率最大；
3. 预测问题：给定观测序列 $X=\{x_{1}, x_{2}, ..., x_{T}\}$ 以及模型 $\lambda=(\Pi, A, B)$，找到最合理的隐藏状态序列 $Y=\{y_{1}, y_{2}, ..., y_{T}\}$。

## 观测序列概率计算

### 问题回顾

给定观测序列 $X$ 和HMM模型 $\lambda$，求 $P(X | \lambda)$。

### 暴力求解的缺陷

遍历所有T个时刻的所有N个可能状态，状态组合有 $N^{T}$ 种，算法复杂度为 $O(T N^{T})$ 阶，隐藏状态多时计算量极大。

### 前向后向算法

利用信念传播法的思想，定义局部概率避免重复计算，以下为前向算法详细流程：

#### 前向概率定义

前向概率 $\alpha_{t}(i)$：当前观测下时刻t时隐藏状态为 $q_{i}$ 的概率。

#### 前向算法流程

1. 计算 $t=1$ 时刻下各隐藏状态前向概率：
$$\alpha _{1}(i)=\pi _{i}b_{i}(x_{1})$$
2. 对 $t=2, ..., T-1$ 进行递推：
$$\alpha_{t}(i)=\left[\sum_{j=1}^{N} \alpha_{t-1}(j) a_{ji}\right] b_{i}\left(x_{t}\right)$$
3. $t=T$ 时算法终止，观测序列概率为：
$$P(X | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$$

#### 后向算法说明

后向算法和前向算法基本相同但方向相反，核心思想一致，均通过局部概率递推减少计算量。

## 模型参数学习

### 问题回顾

给定观测序列数据集 $X$，估计HMM参数 $\lambda=(\Pi, A, B)$。

### 学习方法

将观测序列看做观测数据 $O$，状态序列看做隐变量 $I$，HMM可建模为含有隐变量的概率模型：
$$P(O | \lambda)=\sum_{I} P(O | I, \lambda) P(I | \lambda)$$
通过EM算法实现迭代优化，最大化观测数据的似然函数，对应算法为鲍姆-韦尔奇（Baum-Welch）算法。

#### 鲍姆-韦尔奇算法流程

1. E-step：基于当前HMM参数估计值 $\bar{\lambda}$，计算隐变量期望：
$$\begin{aligned} Q(\lambda, \overline{\lambda}) & =E_{I}[log P(O | I, \lambda) | O, \overline{\lambda}] \\ & =\sum_{I} log P(O, I | \lambda) P(O, I | \overline{\lambda}) \end{aligned}$$
2. M-step：最大化似然函数，更新参数 $\Pi, A, B$：
$$a_{i j}=\frac{\sum_{t=1}^{T-1} P\left(y_{t}=q_{i}, y_{t+1}=q_{j} | O, \lambda\right)}{\sum_{t=1}^{T-1} P\left(y_{t}=q_{i} | O, \lambda\right)}$$
$$b_{i}(k)=\frac{\sum_{t=1, x_{t}=v_{k}}^{T} P\left(y_{t}=q_{i} | O, \lambda\right)}{\sum_{t=1}^{T} P\left(y_{t}=q_{i} | O, \lambda\right)}$$
$$\pi_{i}=P\left(y_{1}=q_{i} | O, \lambda\right)$$

## 隐状态预测

### 问题回顾

给定观测序列 $X$ 和HMM参数 $\lambda$，找到概率最大的隐藏状态序列 $Y^{*}$。

### 核心思想

HMM的预测问题可转化为寻找一条使 $P(Y^{*} | X)$ 最大的路径，采用维特比算法（动态规划思想），在t时刻记录每个状态对应的最优路径和概率值，避免后续重复计算。

#### 维特比算法的局部状态定义

1. $\delta_{t}(i)$：时刻t状态为 $q_{i}$ 的所有单个路径概率最大值：
$$\delta _{t}(i)=\operatorname*{max}_{1\leq j\leq N}\left[\delta _{t-1}(j)a_{ji}\right] b_{i}(x_{t})$$
2. $\psi_{t}(i)$：时刻t获得最大路径概率的前一状态：
$$\psi_{t}(i)=\underset{1 \leq j \leq N}{argmax}\left[\delta_{t-1}(j) a_{j i}\right]$$

#### 维特比算法流程

1. 初始化：
$$\delta_{1}(i)=\pi_{i} b_{i}(x_{1}), \quad \psi_{1}(i)=0$$
2. 正向递推，对 $t=2,3, ..., T$：
$$\delta_{t}(i)=max _{1 \leq j \leq N}\left[\delta_{t-1}(j) a_{j i}\right] b_{i}\left(x_{t}\right)$$
$$\psi_{t}(i)=\underset{1 \leq j \leq N}{argmax}\left[\delta_{t-1}(j) a_{j i}\right]$$
3. T时刻递推终止：
$$P^{*}=max _{1 \leq i \leq N} \delta_{T}(i)$$
$$Y_{T}^{*}=arg max _{1 \leq i \leq N}\left[\delta_{T}(i)\right]$$
4. 路径回溯，对 $t=T-1, T-2, ..., 1$ 得到最优路径：
$$Y_{t}^{*}=\psi_{t+1}\left(Y_{t+1}^{*}\right)$$

## 隐马尔可夫模型的典型应用

### 步态识别

- 应用目标：通过连续行走的图像确定人的身份。
- 建模方式：将步态图像序列建模为观测序列，将人类行走中潜在的姿态作为隐藏状态。
- 实现步骤：
  1. 模型学习：通过学习算法对数据集中每个人的步态用HMM建模；
  2. 概率计算：给定测试步态序列，对每个模型使用前向后向算法计算序列概率；
  3. 身份判定：比较所有HMM模型的概率，选择最大概率的模型作为该序列对应的身份ID。

# 11.5 条件随机场 CRF

## 认识条件随机场

### 条件随机场 (Conditional Random Field, CRF) 的定义

- 是一种判别式概率无向图模型，是在给定观测变量$X$的条件下，随机变量为$Y$的马尔可夫随机场（节点取值只和相邻位置有关）。
- 相比于隐马尔可夫模型，CRF没有对观测变量做马尔可夫假设，因此可以建模观测间的长距离关系。

### 条件随机场的定义

- 在给定随机变量$X$的条件下，若随机变量$Y$构成一个由无向图$G=(V, E)$表示的马尔可夫场，即对任意节点$v$成立$P(Y_v | X, Y_{V \setminus \{v\}}) = P(Y_v | X, Y_{N(v)})$，则称条件概率分布$P(Y | X)$为条件随机场。
- 式中$N(v)$表示在图中和节点$v$有边连接的所有节点，$V \setminus \{v\}$表示节点$v$以外的所有节点。

### 线性链条件随机场

- 在实际情况下，通常考虑$X$和$Y$具有相同结构的线性链条件随机场，即给定随机变量$X=(X_{1}, X_{2}, ..., X_{n})$、$Y=(Y_{1}, Y_{2}, ..., Y_{n})$，满足线性链结构的马尔可夫场假设。

### CRF的参数化形式

- $t_{k}$为转移特征，定义边上的特征函数；
- $s_{l}$为状态特征，定义结点上的特征函数；
- $Z(x)$为归一化因子，$\lambda_{k}$和$\mu_{l}$为不同特征函数对应的权值；
- 类似马尔可夫场的势函数，特征函数取值为1或0，例如：
$$t_{k}\left(y_{i+1}, y_{i}, x, i\right)=\left\{\begin{array}{c}1, if y_{i+1}=[P], y_{i}=[V], and x_{i}='knock' \\ 0, otherwise \end{array}\right.$$

### CRF的参数化形式简化

- 对所有位置的特征函数求和；
- 根据CRF简化形式，在给定$X$时，从$y_{i-1}$转移到$y_{i}$的非规范化概率由特征函数加权和决定。

### 示例-词性标注

- 观测$X=\{x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}\}$对应文本"The boy knocked at the watermelon."；
- 标注目标$Y=\{y_{1}, y_{2}, y_{3}, y_{4}, y_{5}, y_{6}\}$对应词性"[D][N][V][P][D][N]"；
- 特征函数作用：表示第$i$个观测值$x_{i}$为单词knock时，对应的$y_{i}$、$y_{i+1}$词性很可能为动词[V]和介词[P]。

## 条件随机场的概率计算

### 问题回顾

给定条件序列$X$和状态序列$y=\{y_{1}, y_{2}, ..., y_{n}\}$，求基于CRF模型$P(Y | X)$的条件概率$P(Y_{i}=y_{i} | x)$和$P\left(Y_{i}=y_{i}, Y_{i-1}=y_{i-1} | x\right)$。

### 前向后向算法

- 核心思想：通过保留局部概率简化整个序列的概率计算，与HMM不同，CRF是无向图，需计算前向和后向两部分概率。

#### 前向概率

- 定义：$\alpha_{i}(Y_{i} | x)$表示位置$i$的标记为$Y_{i}$，且到位置$i$的前半部分状态序列和$y$相同的概率；
- 递推公式：
$$\alpha_{i+1}\left(Y_{i+1} | x\right)=\sum_{Y_{i} \in y} \alpha_{i}\left(Y_{i} | x\right) M_{i}\left(Y_{i}, Y_{i+1} | x\right)$$

#### 后向概率

- 定义：$\beta_{i}(Y_{i} | x)$表示位置$i$的标记为$Y_{i}$，且位置$i$的后半部分状态序列和$y$相同的概率；
- 递推公式：
$$\beta_{i}\left(Y_{i} | x\right)=\sum_{Y_{i+1} \in y} \beta_{i+1}\left(Y_{i+1} | x\right) M_{i}\left(Y_{i}, Y_{i+1} | x\right)$$

#### 条件概率计算

- 位置$i$标记为$y_{i}$的条件概率：
$$P\left(Y_{i}=y_{i} | x\right)=\frac{\alpha_{i}\left(Y_{i}=y_{i} | x\right) \beta\left(Y_{i}=y_{i} | x\right)}{Z(x)}$$
- 位置$i-1$标记为$y_{i-1}$且位置$i$标记为$y_{i}$的联合概率：
$$P\left(Y_{i-1}=y_{i-1}, Y_{i}=y_{i} | x\right)=\frac{\alpha_{i-1}\left(Y_{i-1}=y_{i-1} | x\right) M_{i}\left(y_{i-1}, y_{i} | x\right) \beta\left(Y_{i}=y_{i} | x\right)}{Z(x)}$$
- 归一化因子：$Z(x)=\sum^{n} \alpha_{n}(Y_{n}=y_{i} | x)=\sum^{n} \beta_{1}(Y_{1}=y_{i} | x)$。

## 模型参数学习

### 问题回顾

给定训练数据集$D=\{(X_{1}, Y_{1}),(X_{2}, Y_{2}), ...,(X_{n}, Y_{n})\}$和$K$个特征函数$\{f_{1}, f_{2}, ..., f_{K}\}$，求基于CRF模型参数$w_{k}$以及条件概率$P_{w}(y | x)$。

### 学习方法

- CRF的学习问题可转化为有标签的极大似然估计问题，最大化基于训练数据$D$的对数似然函数：
$$L(w)=\sum_{x, y} \overline{P}(x, y) log P_{w}(y | x)$$
- 其中$\bar{P}(x, y)$表示基于训练数据$D$的样本经验分布。

### 参数求导与求解

- 对参数$w_{k}$求导可得：
$$\frac{\partial L(w)}{\partial w}=-\sum_{x, y} \overline{P}(x) P_{w}(y | x) f(x, y)+\sum_{x, y} \overline{P}(x) f(x, y)$$
- 可用梯度下降法、迭代尺度法或拟牛顿法求解。

## 条件随机场的预测

### 问题回顾

基于CRF模型$P(Y | X)$和条件序列$X$，求概率最大的状态序列$Y^*$。

### 维特比算法

- 核心思想：基于动态规划思想，通过记录局部状态简化计算，定义两个局部状态。

#### 局部状态定义

1. $\delta_{i}(l)$：到达位置$i$且状态$y_{i}=l$所有可能路径的概率最大值：
$$\delta_{i}(l)=max _{1 \leq j \leq m}\left\{\delta_{i-1}(j)+\sum_{k=1}^{K} w_{k} f_{k}\left(y_{i-1}=j, y_{i}=l, x, i\right)\right\}$$
2. $\Psi_{i}(l)$：到达位置$i$且状态$y_{i}=l$的最优路径中，位置$i-1$的状态取值：
$$\Psi_{i}(l)=\underset{1 \leq j \leq m}{argmax}\left\{\delta_{i-1}(j)+\sum_{k=1}^{K} w_{k} f_{k}\left(y_{i-1}=j, y_{i}=l, x, i\right)\right\}$$

#### 维特比算法流程

1. 初始化：
$$\delta_{1}(l)=\sum_{k=1}^{K} w_{k} f_{k}\left(y_{0}=start, y_{1}=l, x\right), \quad \Psi_{1}(l)=start$$
2. 正向递推，对$i=2,3, ..., n$：
$$\delta_{i}(l)=max _{1 \leq j \leq m}\left\{\delta_{i-1}(j)+\sum_{k=1}^{K} w_{k} f_{k}\left(y_{i-1}=j, y_{i}=l, x, i\right)\right\}$$
$$\Psi_{i}(l)=\underset{1 \leq j \leq m}{argmax}\left\{\delta_{i-1}(j)+\sum_{k=1}^{K} w_{k} f_{k}\left(y_{i-1}=j, y_{i}=l, x, i\right)\right\}$$
3. 递推终止：
$$y_{n}^{*}=arg max _{1 \leq j \leq m} \delta_{n}(j)$$
4. 路径回溯，对$i=n-1, n-2, ..., 1$得到最优路径：
$$y_{i}^{*}=\Psi_{i+1}(y_{i+1}^{*})$$

## 条件随机场的典型应用

### 命名实体识别

- 应用目标：将一串文本中的实体识别出来，比如人名、地名、机构名等。
- 建模方式：经过LSTM网络输出的概率分布作为观测$X$，对应的实体类别作为$Y$构建CRF模型。
- 核心优势：CRF模型通过建模相邻标签间的依赖关系有效提取实体。

# 11.6 概念对比总结与选择建议

## 概念对比总结

### 生成式模型 vs 判别式模型

| 模型类型 | 代表模型 | 特点 | 使用场景 |
|---------|---------|------|---------|
| 生成式 | HMM | 建模 $P(X, Y)$，可以生成数据 | 需要生成数据、需要建模联合分布 |
| 判别式 | CRF | 建模 $P(Y \| X)$，只做预测 | 只需要预测、需要更好的预测性能 |

### 有向图 vs 无向图

| 图类型 | 代表模型 | 特点 | 使用场景 |
|-------|---------|------|---------|
| 有向图 | 贝叶斯网络、HMM | 表示因果关系、有方向性 | 因果关系明确、需要建模有向依赖 |
| 无向图 | 马尔可夫场、CRF | 表示对称依赖、无方向性 | 双向依赖、对称关系 |

### 精确推理 vs 近似推理

| 推理类型 | 代表算法 | 特点 | 使用场景 |
|---------|---------|------|---------|
| 精确推理 | 变量消除、信念传播 | 计算精确值、计算复杂度高 | 模型简单、数据维度小 |
| 近似推理 | MCMC、变分推理 | 计算近似值、计算复杂度低 | 模型复杂、数据维度高 |

## 选择建议

根据问题特点选择合适的模型和算法：

1. **因果关系明确的问题** → 选择**贝叶斯网络**
   - 适用于：医疗诊断、故障分析、风险评估等

2. **时间序列数据 + 隐藏状态** → 选择**HMM**
   - 适用于：语音识别、步态识别等

3. **序列标注任务 + 需要全局考虑** → 选择**CRF**
   - 适用于：命名实体识别、中文分词、词性标注、需要建模长距离依赖的序列任务

4. **对称依赖关系** → 选择**马尔可夫场**
   - 适用于：图像处理、空间数据建模等

5. **需要精确概率值** → 使用**精确推理算法**（变量消除、信念传播）
   - 适用于：模型简单、数据维度较小的情况

6. **模型复杂或数据维度高** → 使用**近似推理算法**（MCMC、变分推理）
   - 适用于：计算资源有限、需要快速推理的情况