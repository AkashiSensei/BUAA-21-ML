# 15.1 什么是半监督学习

## 无监督学习

- 使用未标记样本，基于样本间的相似度，归纳样本的类别。
- 问题：完全基于无标记样本学习，精度难以保证。

## 监督学习
- 利用已标记样本进行学习生成学习器，再对无标记样本进行测试。
- 问题：需要大量已标记的训练样本，标注数据通用性较弱，大规模人工标注耗时、代价高。

## 传统学习面临的问题

### 实际情况

- 只有少量有标记样本，但只使用少量有标记样本，训练出来的学习系统往往难以具有很好的泛化能力。
- 往往有大量未标记样本，仅使用少量“昂贵的”标记样本而不利用“廉价的”未标记样本是对数据资源的浪费。

## 半监督学习的目的

- 在训练样本的部分信息缺失，特别是样本数据的类别标记缺失的情况下，获得具有良好泛化能力的学习器，即利用大量的未标记样本辅助标记样本建立一个更好的学习器。

## 半监督学习的定义

- 给定一个来自某未知分布的样本集 $S=D_{l} \cup D_{u}$，$D_{l}$ 是已标记样本集 $D_{l}=\{(x_{1}, y_{1}), ...,(x_{l}, y_{l})\}$，$D_{u}$ 是未标记样本集 $D_{u}=\{x_{l+1}, ..., x_{l+u}\}$。其中，$x$ 为 $d$ 维向量，$y_{i} \in Y$ 为 $D_{l}$ 中样本 $x_{i}$ 的标记。
- 半监督学习就是在样本集 $S=D_{l} \cup D_{u}$ 上寻找最优学习器，即函数 $f : X \to Y$，可以准确地对样本 $x$ 预测其标记 $y$。
- 这个函数可以是参数方法（如支持向量机等）、非参数方法（如最近邻法等）、数值方法（如决策树分类等）。
- 如何综合利用所有样本（特别是未标记样本）建模，是半监督学习需要解决的关键问题。

## 数据分布假设

- 要利用未标记样本，就需要做一些将未标记样本所揭示的数据分布信息与数据类别标记相联系的假设，即数据分布假设。

### 聚类假设

- 假设数据存在簇结构，同一个簇的样本属于同一个类别，处在相同聚类中的样本有较大可能拥有相同标记。
- 决策边界应尽量通过数据较稀疏的地方，避免把稠密聚类中的数据分到决策边界两侧。
- 大量未标记样本的作用：帮助探明样本空间中数据分布的稠密和稀疏区域，指导对决策边界的调整。

### 流形假设
- 假设数据分布在一个流形结构上，处于很小局部区域内的样本有相似性质，因此标记也相似。
- 主要考虑模型的局部特性，反映决策函数的局部平滑性。
- 大量未标记示例的作用：让数据空间变得更加稠密，从而有助于更加准确地刻画局部区域的特性，使得决策函数能够更好地进行数据拟合。

## 半监督学习的应用

### 医学图像

- 医学图像的标注专业性高，准确性要求高，标注开销昂贵，所以医疗机构难以大量标注数据。
- 应用流程：标注数据训练通用模型，通用模型为未标注数据生成伪标签，伪标签数据与标注数据共同训练专用模型。

### 自动驾驶
- 自动驾驶公司从数据源（如自动驾驶汽车）收集了大量数据，但由于自动驾驶的数据复杂度高、场景复杂，有限的专家标注难以覆盖所有场景。
- 应用流程：自动驾驶汽车收集未标注数据，识别边界案例并标注，标注数据训练模型，模型部署应用。

# 15.2 自学习

## 什么是自学习？

- 自学习 (Self-training) 是一种启发式的半监督学习方法：用有标签数据训练机器学习模型，然后在无标签数据上进行推理生成伪标签 (Pseudo-label)，通过样本选择策略筛选高质量标签，不断迭代训练模型。
- 自学习方法的优点：模型简单，容易与多种机器学习模型相结合而不改变其内部工作方式，包括从简单的kNN模型到复杂的神经网络分类器等。
- 自学习方法的样本选择策略：每次递归仅将满足设定置信度阈值，即置信度高的样本纳入已标记样本集中，参与递归拟合。

## 自学习的核心步骤
1. 用已标记的样本来训练得到一个初始分类器；
2. 用初始分类器对未标记样本进行分类，将标记置信度高的未标记样本进行标记；
3. 对所有样本进行重新训练，直到将所有未标记样本都标记为止。

## 自学习的典型方法

### 最近邻自学习算法

- 核心策略：在自学习算法中采用最近邻的样本筛选策略，每次选择离已标记样本最近的无标记样本进行标记。
- 算法步骤：
  1. 用已标记样本 $D_{l}$ 生成分类器 $f$；
  2. 选择 $x=\arg\min d(x, x_{0})$（其中 $x \in D_{u}$，$x_{0} \in D_{l}$），即选择离已标记样本最近的无标记样本；
  3. 用 $f$ 给 $x$ 确定一个类别 $f(x)$，并将 $(x, f(x))$ 加入 $D_{l}$ 中；
  4. 重复上述步骤1-3，直到 $D_{u}$ 为空集。
- 其中 $d(x_{1}, x_{2})$ 为两个样本的欧式距离。

### k-近邻算法

- 定义：k-近邻算法是用于分类和回归任务的非参数统计的监督方法，使用邻近度对单个数据点的分组进行分类或预测，是最简单的机器学习算法之一。
- 分类任务规则：样本的分类结果是由其邻居的“多数表决”确定的；
- 回归任务规则：样本的属性值为k个邻居的值的平均值；
- 常用的距离度量：欧氏距离、曼哈顿距离、汉明距离；
- 特殊情况：k=1时，即为最近邻算法。

# 15.3 直推半监督学习

## 定义

- 基于“封闭世界”的假设，仅试图对学习过程中观察到的未标记数据进行预测。
- 经典的直推半监督学习有：直推支持向量机、标签传播算法等。
- 核心特征：训练时的未标记数据=测试数据。

## 直推式支持向量机 (Transductive SVM, T-SVM)

### 基本思想

- 是一种结合了直推学习和支持向量机 (SVM) 思想的分类方法，通过同时考虑已知的训练样本和待分类的未知样本，来优化分类超平面，从而提高分类的准确性和泛化能力。
- 针对二分类问题，同时利用标记和未标记样本，通过尝试将每个未标记样本分别作为正例和反例来寻找最优分类边界，得到在原始数据中具有最大分类间隔的分类超平面。

### 标准SVM回顾
- 标准SVM从线性可分情况下的最优分类面发展而来，最优分类面要求不但能将两类正确分开，且使分类间隔最大。
- 引入松弛变量$\xi$处理噪声和离群点，增强模型的泛化能力，形成有错分但分界区域较大的广义最优分类面，避免无错分但分界区域较小的硬间隔分类面导致的泛化性能差问题。

### 与标准SVM的区别（半监督支持向量机）

- 标准SVM只考虑标记样本，寻求标记样本上的最大分类间隔；
- 半监督SVM考虑所有有标记和无标记样本，其划分超平面不仅分隔两类标记样本，还力求在整体数据上使分类间隔最大。
- 直推式支持向量机是半监督支持向量机中最著名的一种。

### 形式化定义

- 优化目标：
$$\min _{w, b, \hat{y}, \xi_{i}} \frac{1}{2}\| w\| _{2}^{2} + C_{l} \sum_{i=1}^{l} \xi_{i} + C_{u} \sum_{i=l+1}^{m} \xi_{i}$$
- 约束条件：
$$y_{i}\left(w^{T} x_{i} + b\right) \geq 1 - \xi_{i}, \quad i=1, ..., l,$$
$$\hat{y}_{i}\left(w^{T} x_{i} + b\right) \geq 1 - \xi_{i}, \quad i=l+1, ..., m,$$
$$\xi_{i} \geq 0, \quad i=1, ..., m,$$
- 参数说明：$C_{l}$ 和 $C_{u}$ 分别表示已标记样本和未标记样本的惩罚因子，用于调整不同样本的权重；$\xi$ 为松弛变量，用于调整对错分样本的容忍程度；$\hat{y}_{i}$ 为未标记样本分类预测值。

### 算法流程

- 输入：有标记样本集 $D_{l} = \{(x_{1}, y_{1}), …, (x_{l}, y_{l})\}$，未标记样本集 $D_{u} = \{x_{l+1}, …, x_{l+u}\}$，惩罚因子 $C_{l}$、$C_{u}$；
- 过程：
  1. 用 $D_{l}$ 训练一个SVM1；
  2. 用SVM1对 $D_{u}$ 中样本进行预测，得到 $\hat{y} = (\hat{y}_{l+1}, …, \hat{y}_{l+u})$；
  3. 初始化 $C_{u} \ll C_{l}$；
  4. while $C_{u} < C_{l}$ do：
     - 基于 $D_{l}$、$D_{u}$、$\hat{y}$、$C_{l}$、$C_{u}$ 求解优化目标，得到 $w$、$b$、$\xi$；
     - while $\exists i, j$ 满足 $\hat{y}_{i} \neq \hat{y}_{j}$ 且 $\xi_{i} > 0$ 且 $\xi_{j} > 0$ 且 $\xi_{i} + \xi_{j} > 2$ do：
       - $\hat{y}_{i} = -\hat{y}_{i}$；
       - $\hat{y}_{j} = -\hat{y}_{j}$；
       - 基于 $D_{l}$、$D_{u}$、$\hat{y}$、$C_{l}$、$C_{u}$ 重新求解优化目标，得到 $w$、$b$、$\xi$；
     - end while；
     - $C_{u} = \min\{2C_{u}, C_{l}\}$；
  5. end while；
- 输出：未标记样本的预测结果 $\hat{y} = (\hat{y}_{l+1}, …, \hat{y}_{l+u})$。

### 核心问题与求解思路

- 核心问题：对未标记样本的类别 $\hat{y}$ 进行各种可能的指派，寻求在所有样本上间隔最大化的划分超平面；未标记样本数太大时，穷举遍历标记指派无法直接求解。
- 求解思路：采用局部搜索迭代求近似解，通过局部搜索和调整指派为异类且可能错误的标记指派，使目标函数值不断下降，步骤如下：
  1. 局部搜索 $x_{i}, x_{j}$；
  2. 判断 $x_{i}$ 与 $x_{j}$ 是否类别不同（$y_{i} y_{j} < 0$）；
  3. 判断 $x_{i}$ 与 $x_{j}$ 是否发生指派错误（$\xi_{i} + \xi_{j} > 2$）；
  4. 若步骤2、3判断为真，互换 $x_{i}$ 与 $x_{j}$ 的标记。

### 存在的问题

- 大规模优化问题：搜寻标记指派出错的每一对未标记样本并进行调整，后续研究提出高效优化求解算法，如基于图和梯度下降的LDS算法 [O. Chapelle et al., 2005]、基于标记均值估计的MeanS3VM [Y. Li et al., 2009]。
- 类别不均衡问题：无标记样本进行标记指派及调整过程中，可能出现某类样本远多于另一类的情况。
  - 改进方法：将优化目标中的 $C_{u}$ 拆分为 $C_{u}^{+}$ 与 $C_{u}^{-}$ 两项，初始化时令 $C_{u}^{+} = (u_{-} / u_{+}) C_{u}^{-}$（$u_{+}$ 和 $u_{-}$ 分别为未标记样本预测为正例和负例的样本数），减轻类别不均衡的负面影响。

## 应用

### 工业残次品检测

- 应用背景：工厂中每批次产品的数据分布因原材料变化、机器磨损、操作员差异等因素有所不同，无法检测所有产品，且其他批次训练的模型未必适用于当前批次。
- 应用方式：利用少量人工检测的标注数据和大量未检测的未标注数据，通过直推半监督学习方法计算新的分类器，适配当前批次产品的数据分布。

# 15.4 归纳半监督学习

## 形式化描述

- 给定一个来自某未知分布的样本集 $S=D_{l} \cup D_{u}$。$D_{l}$ 是已标记样本集 $D_{l}=\{(x_{1}, y_{1}), ...,(x_{l}, y_{l})\}$，$D_{u}$ 是未标记样本集 $D_{u}=\{x_{l+1}, ..., x_{l+u}\}$。其中，$x$ 为 $d$ 维向量，$y_{i} \in Y$ 为 $D_{l}$ 中样本 $x_{i}$ 的标记。
- 归纳半监督学习就是在样本集 $S=D_{l} \cup D_{u}$ 上学习一个推广的预测函数 $f: X \to Y$，这个函数可以应用于任何未见过的数据。
- 经典的归纳半监督学习方法有：协同学习、图半监督学习等。

## 定义

- 归纳半监督学习基于"开放世界"的假设，构建能够推广到未观察到的新数据上的模型。
- 核心特征：训练时的未标记数据$\neq$测试数据。

## 协同学习算法

### 基本概念
- 协同学习(Co-Training)由【1998年A. Blum和T. Mitchell提出】，是自学习的一种改进方法，通过两个基于不同视图 (view)的分类器来互相促进。
- 基于不同视图的条件独立性假设，在不同视图的数据集上训练出来的模型相当于从不同视图来理解问题，具有一定的互补性，协同训练利用这种互补性来进行学习。
- 视图示例：已标注图片数据集可分为视图1数据集（颜色信息）和视图2数据集（形状信息）。

### 算法流程

- 协同学习的算法流程分为以下几个部分：初始阶段、迭代训练、模型更新、终止阶段。
  1. 初始阶段：将有标签数据集 $D_{l}$ 随机分成两个子集，分别对应视图1和视图2，分别训练分类器1和分类器2；
  2. 迭代训练：在每次迭代中，综合分类器1和分类器2对无标签数据 $D_{u}$ 进行预测，并选择置信度较高的样本加入到相应视图的有标签数据集 $D_{l}$ 中；
  3. 模型更新：使用扩充后的有标签数据集 $D_{l}$ 训练模型1和模型2；
  4. 终止阶段：重复步骤2和步骤3，直到满足停止条件（如达到最大迭代次数或模型性能不再提升）。

## 应用

### 医学领域的辅助疾病诊断
- 皮肤镜图像上的自动皮肤病变分割是计算机辅助诊断黑色素瘤的重要组成部分。
- 基于监督学习的方法需要大量标注数据，标注过程非常耗时且成本高昂；归纳半监督学习仅通过少量标注数据引导模型学习，利用未标注数据增强模型的学习效果。

## 归纳半监督学习 vs. 直推半监督学习

### 直推半监督学习 (Transductive Semi-Supervised Learning)

- 仅为训练集中的未标记数据生成标签；
- 学习到的模型不是预测模型；
- 关注如何有效地利用有限的有标签和无标签数据。

### 归纳半监督学习 (Inductive Semi-Supervised Learning)
- 不仅为训练集中的未标记数据生成标签，还能够为新数据生成标签；
- 学习到的模型是预测模型；
- 需要考虑模型的泛化能力。

# 15.5 图半监督学习

## 基本概念

- 图半监督学习是一类针对图结构数据的半监督学习方法。
- 适合解决数据点间存在复杂关系网络的问题，如社交网络分析、推荐系统等，通过利用少量标记数据和大量未标记数据的图结构信息，有效提升对未标记样本分类的准确性。
- 天然图结构示例：社交网络可表示为图，节点代表用户，边代表用户间的关系（如朋友关系、关注关系等）。

## 核心问题
- 在构建的图中，如何使用少量标记样本信息预测无标记样本。
- 流程：建图→边集矩阵→图及其标记传播算法→无标记样本预测。
- 代表性方法：二分类标记传播算法、多分类标记传播算法。

## 形式化
- 首先需将数据集构建为图 $G=(V, E)$：
  - 带标记数据集为 $D_{l}$，无标记数据集为 $D_{u}$，基于 $D_{l} \cup D_{u}$ 构建图；
  - 结点集合 $V=\{x_{1}, \cdots, x_{l}, x_{l+1}, \cdots, x_{l+u}\}$；
  - 边集合 $E$ 表示为相似性矩阵（Affinity Matrix），基于高斯函数定义，两个样本越近，相似度越大：
$$W_{i j}=\begin{cases}\exp \left(\frac{-\left\| x_{i}-x_{j}\right\| _{2}^{2}}{2 \sigma^{2}}\right), & 如果 i \neq j \\ & 否则\end{cases}$$
- 染色比喻：标记样本 $x \in D_{l}$ 对应的结点为染过色，无标记样本 $x \in D_{u}$ 对应的结点未染色，半监督学习对应"颜色"在图 $G$ 上扩散或传播的过程。
- 图可表示为矩阵形式，可通过矩阵运算推导与分析算法。

## 二分类标记传播

### 形式化

- 目标：求解无标记样本的类别指派函数 $f_{u}$ 与标记样本的类别指派函数 $f_{l}$ 的关系。
- 类别指派函数：实值函数 $f: V \to \mathbb{R}$，分类规则为 $y_{i}=\text{sign}(f(x_{i}))$，$y_{i} \in \{-1,+1\}$。
- 核心思想：相似的样本指派相似的标记，定义“能量函数”，最小化该函数使相近样本的标记更相似：
$$E(f)=\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j}\left(f\left(x_{i}\right)-f\left(x_{j}\right)\right)^{2}$$

### 能量函数化简
- 定义：$f=(f_{l}^{T} ; f_{u}^{T})$，$f_{l}=(f(x_{1}) ; f(x_{2}) ; \cdots ; f(x_{l}))$（有标记样本预测结果），$f_{u}=(f(x_{l+1}) ; f(x_{l+2}) ; \cdots ; f(x_{l+u}))$（无标记样本预测结果），$D=\text{diag}(d_{1}, d_{2}, \cdots, d_{l+u})$。
- 化简过程：
$$\begin{aligned} E(f) & =\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j}\left(f\left(x_{i}\right)-f\left(x_{j}\right)\right)^{2} \\ & =\frac{1}{2}\left(\sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j} f^{2}\left(x_{i}\right)+\sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j} f^{2}\left(x_{j}\right)-2 \sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j} f\left(x_{i}\right) f\left(x_{j}\right)\right) \\ & =\sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j} f^{2}\left(x_{i}\right)-\sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j} f\left(x_{i}\right) f\left(x_{j}\right) \\ & =\sum_{i=1}^{m} d_{i} f^{2}\left(x_{i}\right)-\sum_{i=1}^{m} \sum_{j=1}^{m} W_{i j} f\left(x_{i}\right) f\left(x_{j}\right) \end{aligned}$$

### 类别指派函数 $f_{u}$ 的求解

- 分离 $f_{l}$ 与 $f_{u}$ 后化简得：
$$E(f)=f_{l}^{T}\left(D_{l l}-W_{l l}\right) f_{l}-2 f_{u}^{T} W_{u l} f_{l}+f_{u}^{T}\left(D_{u u}-W_{u u}\right) f_{u}$$
- 令偏导数 $\frac{\partial E(f)}{\partial f_{u}}=0$，解得：
$$(D_{u u}-W_{u u}) f_{u}-2 W_{u l} f_{l}=0$$
- 将已知 $f_{l}$ 代入即可求得 $f_{u}$。

## 多分类标记传播

### 形式化

- 基本思想：将二分类"单步式"标记传播拓展到多分类"迭代式"标记传播，使用相似性矩阵扩散或传播标记。
- 标记矩阵定义：定义 $(l+u) \times |Y|$ 的非负标记矩阵 $F=(F_{1}^{T}, \cdots, F_{|Y|}^{T})$，$|Y|$ 为类别数量；第 $i$ 行元素 $F_{i}=(F_{i1}, \cdots, F_{i|Y|})$ 为样本 $x_{i}$ 的标记向量。
- 分类规则：$y_{i}=\arg\max_{1 \leq j \leq |Y|} F_{i j}$。
- 初始化：定义矩阵 $Y=F(0)$，标记样本的标记向量初始化的为：
$$F(0)= \begin{cases}1, & 如果 (1 \leq i \leq l) \land (y_{i}=j) \\ 0, & 否则\end{cases}$$

### 算法流程

1. 构造标记传播矩阵 $S=D^{-\frac{1}{2}} W D^{-\frac{1}{2}}$，其中 $D^{-\frac{1}{2}}=\left(\frac{1}{\sqrt{d_{1}}}, \cdots, \frac{1}{\sqrt{d_{l+u}}}\right)$，用于归一化 $W$；
2. 迭代式标记传播：
$$\begin{gathered} F(t+1)=\alpha S F(t)+(1-\alpha) Y \\ F^{*}=\alpha S F^{*}+(1-\alpha) Y \\ F^{*}=\lim _{t \to \infty}(1-\alpha)(I-\alpha S)^{-1} Y \end{gathered}$$

### 算法等价性分析

- "迭代式"传播方法是下列正则化框架的解：
$$\min _{F} \frac{1}{2}\left(\sum_{i, j=1}^{l+u} W_{i j}\left\| \frac{1}{\sqrt{d_{i}}} F_{i}-\frac{1}{\sqrt{d_{j}}} F_{j}\right\| ^{2}\right)+\mu \sum_{i=1}^{l+u}\left\| F_{i}-Y_{i}\right\| ^{2}$$
- 当 $\mu=\frac{1-\alpha}{\alpha}$ 时，最优解恰为迭代算法的收敛解 $F^{*}$（最优解推导参考《南瓜书》P174, 13.4.9）。

### 算法输入输出

- 输入：有标记样本集 $D_{l}=\{(x_{1}, y_{1}), ...,(x_{l}, y_{l})\}$，未标记样本集 $D_{u}=\{x_{l+1}, x_{l+2}, ..., x_{l+u}\}$，构图参数 $\sigma$，折中参数 $\alpha$，亲和矩阵 $W$；
- 过程：
  1. 计算 $S=D^{-\frac{1}{2}} W D^{-\frac{1}{2}}$；
  2. 初始化 $F(0)=Y$；
  3. 令 $t=0$；
  4. 迭代：$F(t+1)=\alpha S F(t)+(1-\alpha) Y$，$t=t+1$；
  5. 直至迭代收敛至 $F^{*}$；
  6. 对 $i=l+1, l+2, \cdots, l+u$，计算 $y_{i}=\arg\max_{1 \leq j \leq |Y|}\left(F^{*}\right)_{i j}$；
- 输出：未标记样本集的预测结果 $\hat{y}=\left(\hat{y}_{l+1}, \hat{y}_{l+2}, \cdots, \hat{y}_{l+u}\right)$。

## 小结

### 优势

- 算法性质清晰：标记传播方法在概念上相当清晰，易于理解；
- 易于分析：通过对所涉及的矩阵运算的分析，可以探索算法的性质。

### 缺点

- 存储开销大：如果样本数为 $O(m)$，则算法中涉及的矩阵规模为 $O(m^{2})$，难以处理大规模数据；
- 难以适应新样本：构图过程仅能考虑训练样本集，难以预测新样本在图中的位置，接收到新样本时需重新进行标记传播或引入额外预测机制。

# 15.6 半监督聚类

## 聚类的问题

- 数据方面：数据利用不足，无法利用数据中的少量标记信息；
- 性能方面：性能不如监督学习，准确率较低；
- 训练速度：作为数据处理的一步，需要更高的效率；
- 需求：需要一种能利用额外数据信息提升性能和效率的聚类方法。

## 聚类回顾

- 以K均值算法为例：
  - 核心思想：将无标记样本划分为K个簇，使得每个样本与其所属簇的中心的距离之和最小；
  - 形式化：给定D维空间的数据集 $\{x_{1}, x_{2}, ..., x_{N}\}$，不知道这些样本所对应标签，通过聚类方法将这些数据集划分成K类；
  - 簇中心指派：对于K个聚类中的每一类，分别建立一个代表点 $\mu_{k}$，将每一个样本划归到离该样本最近的 $\mu_{k}$ 所代表的聚类。

## 半监督聚类的定义

- 利用少量标记信息来提升聚类的准确性和效率；
- 少量标记信息的类型：
  1. “必连” (Must-Link) 与“勿连” (Cannot-Link) 约束：
     - “必连”：样本必属于同一个簇；
     - “勿连”：样本必不属于同一个簇；
  2. 少量的有标记样本；
- 代表性算法：约束K均值 (Constrained K-means) 算法、约束种子K均值 (Constrained Seed K-means) 算法。

## 约束K均值算法

### 基本定位

- 【2001年K. Wagstaff等人提出】，是K均值算法的扩展；
- 核心特点：在聚类过程中检查每个样本在“必连”关系集合与“勿连”关系集合中的约束是否得以满足，否则将重新对该样本聚类。

### 算法输入输出

- 输入：样本集 $A = \{x_{1}, x_{2}, … , x_{N}\}$，必连约束集合 $\mathcal{M}$，勿连约束集合 $\mathcal{C}$，聚类簇数 $K$；
- 输出：簇划分 $\{C_{1}, C_{2}, \cdots, C_{K}\}$。

### 基本思路（算法流程）

1. 初始化：选择K个初始聚类中心；
2. 分配：将每个样本分配给最近的聚类中心 $\mu_{k}$；
3. 检查：对有约束信息的样本，若分配到最近的聚类中心不违反任何约束，则执行分配；若违反约束，则尝试分配到次近的聚类中心，重复检查；若没有可分配的聚类中心，则报错；
4. 更新：重新计算聚类中心 $\mu_{k}$；
5. 迭代：重复分配和更新步骤2-4，直到满足终止条件（聚类中心不再发生显著变化或达到最大迭代次数）。

## 约束种子K均值算法

### 基本定位

- 【2002年S. Basu等人提出】，是K均值算法的扩展；
- 核心特点：使用标记样本作为“种子”初始化聚类中心，且迭代更新过程中不改变种子样本的簇隶属关系。

### 算法输入输出

- 输入：样本集 $A = \{x_{1}, x_{2}, … , x_{N}\}$，少量有标记样本集 $\tilde{D} = \bigcup_{k=1}^{K} \tilde{D}_{k}$，聚类簇数 $K$；
- 输出：簇划分 $\{C_{1}, C_{2}, \cdots, C_{K}\}$。

### 基本思路（算法流程）

1. 初始化：使用标记样本初始化K个初始聚类中心，将标记样本作为“种子”，迭代中不改变其簇隶属关系；
2. 标记样本分配：将标记样本分配给对应分类的聚类中心；
3. 分配：将其余未标记样本分配给最近的聚类中心 $\mu_{k}$；
4. 更新：重新计算聚类中心 $\mu_{k}$；
5. 迭代：重复分配和更新步骤2-4，直到满足终止条件（聚类中心不再发生显著变化或达到最大迭代次数）。

## 半监督聚类 VS 聚类

- 准确率：半监督聚类准确率高于传统聚类；
- 迭代次数：约束种子K均值可以有效减少迭代次数，而约束K均值需要的迭代次数较多。