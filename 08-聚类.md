# 8.1 什么是聚类？

## 聚类的定义

- 聚类是一种无监督学习任务，按照某个特定标准（如距离），将数据集中的无标注样本划分为若干个不相交的子集，每个子集称为一个“簇”（Cluster）。
- 核心目标：簇内样本的相似性尽可能大，簇间样本的差异性尽可能大。

### 与分类（监督学习）的对比

| 特征 | 分类 | 聚类 |
| --- | --- | --- |
| 数据类型 | 有标签数据集 | 无标签数据集 |
| 核心逻辑 | 训练分类器，对已知类别进行预测 | 无监督划分簇，挖掘数据内在结构 |
| 性能特点 | 准确性较高 | 准确性通常低于分类，但更灵活，可处理无标签数据 |

## 聚类的形式化描述

- 假定样本集 $X = \{x_1, x_2, \cdots, x_N\}$ 包含 $N$ 个无标记样本，每个样本 $x_n = (x_{n1}, x_{n2}, \cdots, x_{nd})$ 是一个 $d$ 维的特征向量。
- 聚类算法将样本集 $X$ 划分成 $K$ 个不相交的簇 $\{C_k \mid k = 1,2, \cdots, K\}$，其中 $C_{k'} \cap C_k = \emptyset$（$k' \neq k$），且 $X = \bigcup_{k=1}^K C_k$。
- 用 $\lambda_n \in \{1,2, \cdots, K\}$ 表示样本 $x_n$ 的"簇标记"（Cluster Label），即 $x_n \in C_{\lambda_n}$。
- 聚类的结果可用包含 $N$ 个元素的簇标记向量 $\Lambda = \{\lambda_1; \lambda_2; \cdots; \lambda_N\}$ 表示。

## 聚类的性能度量

聚类的“有效性指标”（Validity Index）核心：簇内相似度（Intra-cluster Similarity）高，簇间相似度（Inter-cluster Similarity）低。

### 外部指标

- 定义：将聚类结果与参考模型的簇划分进行比较，评估聚类结果与参考分类的一致性。
- 核心集合定义：
  - $a = |SS|$，其中
    $$SS = \{ ({\pmb x}_i, {\pmb x}_j) \mid \lambda_i = \lambda_j, \lambda_i^* = \lambda_j^*, i < j \}$$
    **解释**：聚类结果和参考模型都认为这两个样本属于同一类，这是正确的聚类结果。
  - $b = |SD|$，其中
    $$SD = \{ ({\pmb x}_i, {\pmb x}_j) \mid \lambda_i = \lambda_j, \lambda_i^* \neq \lambda_j^*, i < j \}$$
    **解释**：聚类结果认为这两个样本属于同一簇，但参考模型认为它们属于不同类，这是聚类错误地将不同类样本合并的情况。
  - $c = |DS|$，其中
    $$DS = \{ ({\pmb x}_i, {\pmb x}_j) \mid \lambda_i \neq \lambda_j, \lambda_i^* = \lambda_j^*, i < j \}$$
    **解释**：聚类结果认为这两个样本属于不同簇，但参考模型认为它们属于同一类，这是聚类错误地将同类样本分开的情况。
  - $d = |DD|$，其中
    $$DD = \{ ({\pmb x}_i, {\pmb x}_j) \mid \lambda_i \neq \lambda_j, \lambda_i^* \neq \lambda_j^*, i < j \}$$
    **解释**：聚类结果和参考模型都认为这两个样本属于不同类，这也是正确的聚类结果。
  - 满足 $a + b + c + d = \frac{m(m-1)}{2}$（所有样本对的总数）
- 常用外部指标（取值在[0,1]区间内，越大越好）：
  - Jaccard系数（Jaccard Coefficient, JC）：
    $$JC = \frac{a}{a+b+c}$$
    **解释**：衡量聚类结果中正确聚类的样本对（$a$）占所有相关样本对（$a+b+c$，即至少在一个划分中属于同一类的样本对）的比例。只关注"应该聚在一起"的样本对，忽略"应该分开"的样本对。
  - FM指数（Fowlkes and Mallows Index, FMI）：
    $$FMI = \sqrt{\frac{a}{a+b} \cdot \frac{a}{a+c}}$$
    **解释**：综合考虑聚类结果的精确率（$\frac{a}{a+b}$，聚类结果中正确聚类的比例）和召回率（$\frac{a}{a+c}$，参考模型中应该聚类的样本对有多少被正确聚类）的几何平均数。平衡了"合并错误"（$b$）和"分离错误"（$c$）的影响。
  - Rand指数（Rand Index, RI）：
    $$RI = \frac{2(a+d)}{m(m-1)}$$
    **解释**：衡量聚类结果中所有正确决策（正确合并的样本对 $a$ 和正确分开的样本对 $d$）占所有样本对的比例。同时考虑了"应该聚在一起"和"应该分开"两种情况，是最全面的评估指标。

### 内部指标

- 定义：无需参考模型，通过聚类结果自身的簇内紧凑性与簇间分离性评估聚类质量。
- 核心术语定义：
  - 簇内平均距离：
    $$\text{avg}(C) = \frac{2}{|C|(|C|-1)} \sum_{x_i,x_j \in C, i<j} \text{dist}(x_i,x_j)$$
    **解释**：计算簇内所有样本对之间距离的平均值，用于衡量簇的紧凑程度。值越小，说明簇内样本越紧密聚集。
  - 簇直径：
    $$\text{diam}(C) = \max_{x_i,x_j \in C, i<j} \text{dist}(x_i,x_j)$$
    **解释**：计算簇内距离最远的两个样本之间的距离，用于衡量簇的"跨度"。值越小，说明簇越紧凑。
  - 簇中心距离：$d_{\text{cent}}(C_i,C_j) = \text{dist}(\mu_i,\mu_j)$（$\mu$ 为簇中心）
    **解释**：计算两个簇的中心点（质心）之间的距离，用于衡量簇之间的分离程度。值越大，说明簇间分离度越高。
  - 簇间最小距离：
    $$d_{\text{min}}(C_i,C_j) = \min_{x_i \in C_i, x_j \in C_j} \text{dist}(x_i,x_j)$$
    **解释**：计算两个簇中最近的两个样本之间的距离，用于衡量簇之间的"边界"距离。值越大，说明簇间分离度越高，边界越清晰。
- 常用内部指标：
  - DB指数（Davies-Bouldin Index, DBI）：
    $$DBI = \frac{1}{k} \sum_{i=1}^k \max_{j \neq i} \left( \frac{\text{avg}(C_i) + \text{avg}(C_j)}{d_{\text{cent}}(\mu_i,\mu_j)} \right)$$
    **解释**：对每个簇，计算它与最相似簇的"分离度"（簇内平均距离之和与簇中心距离的比值），然后取所有簇的平均值。比值越小，说明簇内紧凑（分子小）且簇间分离（分母大），聚类质量越好。**取值越小越好**。
  - Dunn指数（Dunn Index, DI）：
    $$DI = \min_{1 \leq i \leq k} \left\{ \min_{j \neq i} \left( \frac{d_{\text{min}}(C_i,C_j)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} \right) \right\}$$
    **解释**：计算所有簇对中，簇间最小距离与簇内最大直径的比值，然后取最小值。比值越大，说明簇间分离度大（分子大）且簇内紧凑（分母小），聚类质量越好。**取值越大越好**。

## 聚类的应用

### 遥感图像分割

- 处理方式：将每个像素点视为一个无标注样本，每个样本是维度为3的颜色向量；也可使用神经网络提取的特征向量作为样本，利用语义信息聚类。
- 效果：通过聚类可划分出建筑、道路、水体、植被等不同地物（例：$K=6$ 时可区分多种地物类型）。

### 神经网络压缩

- 应用背景：深度神经网络（如ResNet-50）需大量存储和计算资源（95MB+存储空间、38亿+浮点运算），而物联网（IoT）等设备资源有限（仅几MB RAM），需进行模型压缩。
- 核心逻辑：传统压缩算法通过对相似度高的特征图进行聚类，取聚类中心作为替代，再结合滤波器剪枝，减少模型冗余。
- 压缩效果：ResNet-50可压缩至5MB甚至更小。
- 流程：原始模型 → 特征图聚类 → 取聚类中心 → 微调模型结构 → 压缩模型。

# 8.2 K均值算法

## K均值算法的基本概念

K均值算法(K-Means)--最常用的聚类算法

- 定义:给定D维空间上的数据集 $X={x_{1}, ..., x_{N}}$ ,这些数据对应类别未知｡K均值算法将数据集划分成 K 类,各类的聚类中心记为 $\mu_{1}, ..., \mu_{k}$ ,并将每一个样本 $x_{n}$ 划归到离该样本最近的聚类中心。
- 准则函数:各数据点到对应聚类中心距离之和。当 $x_{n}$ 属于第k个聚类时, $r_{n k}=1$，记 $r_{n}$ 为中间变量,可以看作是"隐变量"。

## K均值算法的基本思想

K均值算法通过迭代优化实现聚类：先初始化K个聚类中心，再将样本按距离最近原则分配到对应聚类，随后重新计算聚类中心，重复分配与更新步骤，直至聚类结果稳定，最终使簇内样本相似度高、簇间样本差异度大。

## K均值算法的具体求解

### K均值算法的一般流程

1. 初始化选择K个初始聚类中心；
2. 将每个数据点划分给最近的聚类中心 $\mu_{k}$ ,得到聚类标注 $r_{n}$；
3. 最小化准则函数,重新计算聚类中心 $\mu_{k}$；
4. 迭代步骤2和3,直到满足终止条件：聚类中心不再发生显著变化或达到最大迭代次数。

### 迭代优化策略

- 步骤2:对于给定的 $\mu_{k}$ ,按照最优化准则产生 $r_{n k}$，准则函数为：
$$J=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k}\left\| x_{n}-\mu_{k}\right\| ^{2}$$
- 步骤3:对于给定的 $r_{n k}$ ,按照最优化准则产生 $\mu_{k}$。

### 准则函数变化趋势

准则函数J随迭代次数增加而下降，逐渐趋于稳定，迭代过程中J值从较高水平（如1000）逐步降低至较低水平（如500），直至不再显著变化。

### K均值算法过程可视化

K均值算法流程如下：
- (a): 初始化各类代表点 $\mu_{k}$；
- (b): 第一步更新 $r_{n k}$；
- (c): 第二步计算 $\mu_{k}$；
- (d)-(g):迭代两个步骤直至停止条件，最终求解得聚类中心 $\mu$。

## K均值算法的优缺点

### 优点

- 简单易懂､易于理解和实现；
- 计算成本低､一般可以快速收敛；
- 适应性强，各类大小大致相等且形状为球形时效果较好。

### 缺点

- 需要预先指定聚类中心K的数量，对初始化中心点敏感；
- 欧几里德距离假设所有变量在距离计算中同等重要，限制了能处理的数据变量；
- 不适用于非线性数据集。

## K均值算法的改进

### 选择初始化聚类数量K

- 肘部法则:尝试不同的K值进行实验，收敛后的准则函数可能会呈现一条类似于人的肘部的曲线，曲线拐点对应的K值即为较优聚类数量。

### 迭代自组织数据分析算法 ISODATA

- 改进:增加对聚类结果的“合并”和“分裂”操作，设定算法运行控制参数。
- 基本思想:引入人的决策，1). 两类聚类中心距离小于某阈值时进行合并；2). 当某类标准差大于某一阈值或其样本数目超过某阈值时进行分裂；3). 在某类样本数目少于某阈值时，重新划分其到其他类别。

### K-means++

- 改进: 2007年由Arthur和Vassilvitskii提出的K-means++针对K-means的聚类中心初始化做了改进。
- 基本思想:假设已选取了n个初始聚类中心(0<n<k)，在选取第n+1个中心时：距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心，记候选中心x被选为聚类中心的概率为Q(x)。根据Q(x)用轮盘法选出第n+1个聚类中心。其中，S(x)表示候选中心x到已有中心的最远距离，概率公式为 $Q(x) = \frac{S(x)}{\sum_{x \in K} S(x)}$。

### Kernel K-means

- 改进:传统K-means采用欧式距离进行样本间的相似度度量，显然并不是所有的数据集都适用于这种度量方式。
- 基本思想:参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间(一般为高维)中，希望在这个特征空间中数据可以变得更容易分离或更好的结构化，并在新的特征空间中进行聚类。

# 8.3 高斯混合模型

## 认识高斯混合模型

### 示例

- 统计学校各院系同学的身高数据：从学校的档案馆中随机抽取了$N$（如$N=2000$）名同学的身高信息，但并不知道学生属于哪个院系。假设每个学院同学的身高服从高斯分布。
- 问题：如何从中得到每个学生的院系划分（聚类簇）及各院系的学生身高分布？

### 什么是高斯混合模型（Gaussian Mixture Model, GMM）

- 是一种统计模型，用于表示一组数据是由多个高斯分布混合而成的。具体地，高斯混合模型是多个高斯分布的线性组合，每个高斯分布称为一个混合成分，每个混合成分都有一个对应的混合系数，所有混合系数的和为1。
- 概率密度函数：
$$p(x)=\sum_{i} \alpha_{i} \mathcal{N}\left(x | \mu_{i}, \sum_{i}\right), \sum_{i} \alpha_{i}=1$$
其中，$\alpha_i$为混合系数，$\mathcal{N}(x | \mu_i, \sum_i)$为混合成分。
- 核心特性：GMM能够捕捉数据的多峰特性，即数据集中可能存在多个簇，每个簇的分布可以用一个高斯分布来描述。
- 应用领域：广泛应用于聚类分析、图像分割、语音识别、数据降维等领域。

### 符号定义

- 观测值：每个学生的身高 $X={x_{1}, x_{2}, ..., x_{N}}$；
- 分布参数：第$k$个院系的学生身高服从高斯分布 $N(\mu_{k}, \sigma_{k}^{2})$；
- 求解目标：总体身高分布 $p(x)$。

### 引入中间变量建模GMM

- K-Means使用$r_{n}$表示每个样本$x_{n}$所属的聚类簇。GMM引入隐变量$Z={z_{1}, z_{2}, ..., z_{N}}$表示每个学生所属的院系。

### 隐变量$Z$的含义

- $z$在实际观测中不可知。假设共有$K$个不同的院系，可以用一个$K$维独热向量（one-hot vector）来表示。$z$只在第$k$维为1，其他维均为0，代表学生$x_n$属于第$k$个院系。
- 向量形式：
$$z=\left( \begin{array} {c}{0}\\ {0}\\ {1}\\ {\cdot }\\ {\cdot }\\ {0}\end{array} \right)$$
- 概率分布：$z \sim Multi(\pi)$，满足约束条件：
$$0 \leq \pi_{k} \leq 1, \sum_{k} \pi_{k}=1$$
- 作用：GMM通过引入隐变量$z$来表示样本$x$来自第$k$个高斯分布，用于简化问题的求解。

## 高斯混合模型的极大似然估计

引入隐变量$Z$进行建模，学生身高分布的概率密度函数$p(X ; \pi, \mu, \sum)$的对数似然函数为：
$$\begin{aligned} ln p(X ; \pi, \mu, \sum) & =ln \left(\prod_{n=1}^{N} p\left(x_{n} ; \pi, \mu, \sum\right)\right) \end{aligned}$$
其中$p(X ; \pi, \mu, \sum)$为$N$个数据点概率分布的连乘，进一步展开为：
$$ln p(X ; \pi, \mu, \sum) =\underset{\pi, \mu, \sum}{argmax} \sum_{n=1}^{N} ln \sum_{z} p\left(x_{n} | z ; \mu, \sum\right) p(z ; \pi)$$

## 高斯混合模型的EM求解

### EM算法推导

对数似然函数可转化为：
$$ln p(X | \pi, \mu, \sum)=\sum_{n=1}^{N} ln p\left(x_{n} | z_{n} ; \mu, \sum\right)+ln p\left(z_{n} | \pi\right)$$
- 说明：K-Means可以看作是EM算法的一种特例。它交替执行两步走策略：在第一步（对应E-step），将每个样本分配给最近的聚类中心；在第二步（对应M-step），通过最小化准则函数，重新计算聚类中心。

### E-step（期望步）

计算样本属于每个高斯分布的后验概率$\gamma_{n k}$：
$$\gamma_{n k}^{(t+1)}=\frac{\pi_{k}^{(t)} \mathcal{N}\left(x_{n} | \mu_{k}^{(t)}, \sum_{k}^{(t)}\right)}{\sum_{j=1}^{K} \pi_{j}^{(t)} \mathcal{N}\left(x_{n} | \mu_{j}^{(t)}, \sum_{j}^{(t)}\right)}$$

### M-step（最大化步）

固定$\gamma_{n k}$，调整GMM的模型参数$\pi_k$、$\mu_k$、$\sum_k$：
1. 对$\mu_{k}$求导，得：
$$\mu_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma_{n k} x_{n}$$
其中$N_{k}=\sum_{n=1}^{N} \gamma_{n k}$，$N_k$可以看作第$k$个高斯分布所包含的样本数。
2. 对$\sum_{k}$求导，得：
$$\sum_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma_{n k}\left(x_{n}-\mu_{k}\right)\left(x_{n}-\mu_{k}\right)^{T}$$
3. 对$\pi_k$求导时必须考虑约束条件$\sum_{k=1}^{K} \pi_{k}=1$，需使用拉格朗日乘子法。

### EM算法流程

1. 选取合适的$K$并初始化参数：$\pi^{0}$、$\mu^{0}$、$\sum^{0}$；
2. 交替执行E-step和M-step步骤直至收敛；
3. 收敛条件：模型参数不再发生显著变化或达到最大迭代次数。

### EM过程图示说明

- (a): 初始化高斯混合分布的模型参数；
- (b): E-步计算$\gamma_{n k}$；
- (e): M-步更新$\mu_{k}$、$\sum _{k}$、$\pi_{k}$；
- (d)-(f): 迭代执行E-M过程直至达到停止条件。

## 高斯混合模型的应用

### 不同学院学生的身高分布问题

- 聚类结果：GMM的均值为[174.98, 185.22]，方差为[9.04, 9.69]，混合系数为[0.496, 0.504]。
- 核心逻辑：通过GMM的EM求解，得到每个学生所属院系（高斯分布）的后验概率，实现聚类划分，同时得到各院系身高的高斯分布参数。

### GMM v.s. K-Means

- 对比维度：两者均为聚类算法，K-Means是硬聚类（样本明确属于某一簇），GMM是软聚类（样本以一定概率属于各簇）；K-Means基于距离划分，GMM基于概率分布建模，更适合非球形簇数据。

### 遥感图像分割

#### 应用背景

- 遥感图像分析为资源管理、环境监测、灾害评估、土地利用规划等领域提供了重要信息和决策支持。
- 遥感图像分割是将遥感图像中的像素划分为具有相似特征的区域的过程，通过分割可精确识别和定位地物。
- 数据源：https://www.o-map.cn/

#### 利用GMM进行遥感图像分割的步骤

1. 输入为$q×s×t$的遥感图像，将每个像素点视为一个无标注样本（总共$qs$个），每个样本（像素点）是维度为3的颜色向量；
2. 选取合适的聚类数量$K$，初始化GMM的模型参数$\pi$、$\mu$、$\sum$；
3. 交替执行E-step和M-step，直至满足收敛条件或达到最大迭代次数；
4. 通过计算样本属于各聚类簇的后验概率，将每个样本（像素点）分配给概率最高的聚类簇，得到最终的分割结果。

#### 分割结果示例

- 不同$K$值对应的分割效果：$K=6$可区分水体、草地、建筑、道路、植被、农田；$K=3$可区分建筑、道路、植被等主要地物；$K=4$可进一步细化地物类别。

# 8.4 层次聚类

## 认识层次聚类

- 层次聚类希望在不同层次对数据集进行划分，从而形成树形的聚类结构。可以在不同的尺度（层次）上展示数据集的聚类情况。
- 数据集的划分可以采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略：
  - 自底向上的聚合策略：AGNES、BIRCH、ROCK…
  - 自顶向下的分拆策略：DIANA、Bisecting K-Means、PDDP…

## AGNES聚类算法

### AGNES算法的定义与基本思想

- AGNES（AGglomerative NESting）是一种采用自底向上聚合策略的层次聚类算法。
- 基本思想：先将数据集中的每个样本看作一个初始聚类簇，然后逐步找出距离最近的两个簇进行合并，该过程不断重复，直到只剩下一个簇为止。
- 距离：每个簇是一个样本集合，因此只需采用关于集合的某种距离即可。

### 集合的距离

给定聚类簇 $C_i$ 与 $C_j$，可以定义以下几种距离：
- 最小距离：由两个簇的最近样本决定，公式为：
  $$d_{min}(C_i, C_j) = min_{x \in C_i, z \in C_j} dist(x, z)$$
- 最大距离：由两个簇的最远样本决定，公式为：
  $$d_{max}(C_i, C_j) = max_{x \in C_i, z \in C_j} dist(x, z)$$
- 平均距离：由两个簇的所有样本决定，公式为：
  $$d_{avg}(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i} \sum_{z \in C_j} dist(x, z)$$

- 样本之间的距离 $dist(x, z)$ 可以使用不同的度量方式，如欧几里得距离、曼哈顿距离等。

### AGNES算法的流程

1. 初始化每个样本作为一个聚类簇（$C_1, C_2, \cdots, C_N$）；
2. 计算每两个簇 $C_i$ 和 $C_j$ 之间的距离 $d(C_i, C_j)$；
3. 找出距离最近的两个簇进行合并；
4. 重复步骤2-步骤3，直到只剩下一个簇。

## DIANA聚类算法

- DIANA是采用"自顶向下"分拆策略的层次聚类算法，核心是从整个数据集作为一个初始簇开始，逐步将簇分拆为更小的子簇，直到每个样本成为一个独立的簇。

## 层次聚类的应用

文档中未明确列出层次聚类的具体应用案例，仅提及算法本身相关内容。