# 13.1 什么是决策树？

## 引例

- 引例1：相亲决策场景，基于年龄、长相、收入等属性进行决策。
- 引例2：天气是否适合打网球，决策规则为 (Outlook=Rain ∧ Wind=Weak) ∨ (Outlook=Sunny ∧ Humidity=Normal) ∨ (Outlook=Overcast)。

## 问题背景

### 问题举例

- 根据症状或检查结果分类患者；
- 根据起因或现象分类设备故障；
- 根据拖欠支付的可能性分类贷款申请。

### 分类问题

- 把样例分类到各可能的离散值对应的类别。

### 问题特征

- 实例由"属性-值"对表示；
- 训练数据可以包含缺少属性值的实例；
- 属性可以是连续值或离散值；
- 具有离散的输出值。

## 决策树的概念

- 决策树(Decision Tree)是一种树型结构，由结点和有向边组成。
  - 结点：
    - 根结点对应全部训练样本；
    - 内部结点表示一个属性或特征，对应满足从根结点到该结点所有条件的训练样本；
    - 叶结点代表一种类别。
  - 有向边：代表一个测试输出。

- 基本思想：
  - 采用自顶向下的递归方法，以信息熵为度量构建一棵熵值下降最快的树，到叶结点处的熵值为零，此时每个叶结点中的实例都属于同一类；
  - 决策树具有直观的可视化形式，类似于人类的决策过程，易于理解与解释；
  - 决策树可以看成一个if-then规则集合，根结点到叶结点的每一条路径构建一条规则；
  - 决策树将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布。

## 决策树的构建

### 算法基本流程

1. 训练：从数据中获取知识进行学习，利用训练集建立(并精化)一棵决策树，构建决策树模型；
2. 测试：利用构建的模型对输入数据进行分类，对测试集样本，从根结点依次测试记录的属性值，直至到达某个叶结点，找到该样本所在的类别。

### 决策树构建基本流程

1. 选取一个最佳划分属性作为决策树的根结点，并就该属性所有的取值创建树的分支；
2. 使用决策树对训练数据集进行分类：
   - 如果一个结点的所有实例都属于同一类，则以该类为标记标识此叶结点；
   - 如果所有的叶结点都有类标记，则算法终止；
3. 否则，选取一个从该结点到根结点路径中未出现过的最佳属性作为标记标识该结点；
4. 就该属性所有的取值继续创建树的分支，重复步骤2~4。

### 构建关键

- 选择当前状态下的最佳划分属性，作为分类依据；
- 决策树学习目标：每个分支结点的样本尽可能属于同一类别，即结点的"纯度"(Purity)越来越高；
- 纯度的上升程度记为$\Delta_{P}$，用于确定划分效果的度量标准为：
$$\Delta_{P} = P_{before} - \sum_{v=1}^{V} \frac{|D^v|}{|D|} P_{after}^v$$
其中，$P_{before}$为划分前纯度度量，$P_{after}^v$为划分后纯度度量，$|D|$为总样本数量，$|D^v|$为取值为$a^v$的样本数量，$V$为属性$a$的取值数量。

### 结点纯度度量与对应算法

- 结点纯度度量方式包含：信息熵(Information Entropy)、基尼指数(Gini Index)；
- 决策树学习主要算法：
  - ID3算法【1979年J. R. Quinlan提出】：信息增益(Information Gain)；
  - C4.5算法【1993年J. R. Quinlan提出】：信息增益率(Information Gain Ratio)；
  - CART (Classification And Regression Tree)算法【1984年L. Breiman提出】：基尼指数(Gini Index)。

## 决策树的应用

- 医疗行业：决策树易于理解与解释的特性，使其在复杂的医疗决策中，能够为医患提供有力的指导或支持。例如，术前应用抗菌药物、某项指标≥特定值、术前腹痛时间≥特定时长的患者，出现手术部位感染的概率为特定比例。

# 13.2 ID3算法

## 基本思想

- ID3 (Iterative Dichotomiser 3)迭代二分器算法是一种最经典的决策树学习算法【1979年J. R. Quinlan提出】。
- 基本思想：以信息熵为结点纯度度量，每次优先选取信息增益最大的属性，即能使熵值最小的属性，构建一棵熵值下降最快的决策树，到叶结点的熵值为0，此时对应样本集中的所有样本属于同一类别。

## 信息增益计算

### 信息熵(Information Entropy)

- 信息熵定义了概率密度函数到信息熵值的映射关系，即 $P(X=x_{i}) \to H(X)$。
- 信息熵表示随机变量不确定性的大小，是度量样本集合纯度最常用的一种指标。信息熵越大，随机变量的不确定性越大，样本集合的纯度越低。
- 离散随机变量的信息熵：令一个取有限个值的离散随机变量$X$的概率分布为 $P(X=x_{i})=p_{i}$，则随机变量$X$的信息熵定义为：
$$H(X)=-\sum_{i} p_{i} log _{2} p_{i}$$
- 示例补充（非计算示例）：当随机变量$X$仅有两个取值（如0和1），概率分布为$P(X=0)=p$，$P(X=1)=1-p$，则熵为$H(X)=-p log _{2} p-(1-p) log _{2}(1-p)$；当$p=0$或$p=1$时，熵为0；当$p=0.5$时，熵为1。
- 连续随机变量的信息熵：若$X$为连续随机变量，则概率分布替换为概率密度函数，且求和操作替换为积分操作即可。

### 经验熵(Empirical Entropy)

- 经验熵表示样本集合的纯度的高低，经验熵越小，样本集合的纯度越高。
- 假设当前样本集合$D$中第$k$类样本所占比例为$p_{k}$，则$D$的经验熵定义为：
$$H(D)=-\sum_{k=1}^{K} p_{k} log _{2} p_{k}$$

### 条件熵(Conditional Entropy)

- 条件熵表示在已知随机变量$X$的条件下，随机变量$Y$的不确定性。
- 对于随机变量$(X, Y)$，联合概率分布为$P(X=x_{i}, Y=y_{j})=p_{i j}$，则条件熵定义为：
$$H(Y | X)=-\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)=H(X, Y)-H(X)$$

### 经验条件熵(Empirical Conditional Entropy)

- 经验条件熵表示属性$a$的信息对样本集合$D$的信息的不确定性减少的程度。
- 假设当前样本集合$D$中共有$K$类，每一类有$D_{k}$个样本，属性$a$有$V$个可能的取值$\{a^{1}, a^{2}, \cdots, a^{V}\}$，属性为$a^{v}$的样本数为$D^{v}$，且每一类中包含$D_{k}^{v}$个样本，则$D$的经验条件熵定义为：
$$\begin{aligned} H(D | a) & =-\sum_{v, k} p\left(D_{k}, a^{v}\right) log _{2} P\left(D_{k} | a^{v}\right) \\ & =-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{k}^{v}\right|}{\left|D^{v}\right|} log _{2} \frac{\left|D_{k}^{v}\right|}{\left|D^{v}\right|}=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} H\left(D^{v}\right) \end{aligned}$$

### 信息增益(Information Gain)

- 信息增益表示使用属性$a$进行划分所获得的"纯度"上升程度，信息增益越大，则代表使用属性$a$进行划分所获得的"纯度"上升越快。
- 属性$a$对训练数据集$D$的信息增益记为$G(D, a)$，定义为集合$D$的经验熵$H(D)$与在给定属性$a$的条件下$D$的经验条件熵$H(D | a)$之差，即：
$$\begin{aligned} G(D, a)= & H(D)-H(D | a) \\ & =H(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} H\left(D^{v}\right) \end{aligned}$$
- ID3算法即是以此信息增益为准则，对每次递归的结点属性进行划分的。

## 算法流程

- 输入：训练数据集$D$、属性集$A$。
- 过程：
  1. 若$D$中所有样本属于同一类，则置为单结点树，该类为结点类标记，返回；
  2. 若$A$为空集，则置为单结点树，$D$中样本数最大类为结点类标记，返回；
  3. 否则，计算$A$中各属性对$D$的信息增益，选择信息增益最大的属性$a_*$；
  4. 如果$a_*$的信息增益小于阈值，则置为单结点树，$D$中样本数最大类为该结点类标记，返回；
  5. 否则，对$a_*$的每个可能取值$a_*^v$，构建子结点，对应$D$中属性$a_*$取$a_*^v$的样本子集$D^v$；
  6. 对每个子结点，以$D^v$和$A \setminus \{a_*\}$为输入，递归调用上述步骤，返回决策树。
- 输出：决策树。

## 算法优点

- 能够从一类无序、无规则概念中推理出分类规则；
- 能够将决策树中到达每个叶结点的路径转换为if-then形式的分类规则，比较符合人类的理解方式。

## 算法局限性

- 信息增益偏好取值多的属性（极限趋近于均匀分布）；
- 会受噪声或小样本影响，易出现过拟合问题；
- 无法处理连续值的属性；
- 无法处理属性值不完整的训练数据；
- 无法处理不同代价的属性。

# 13.3 C4.5与CART算法

## C4.5算法

### 基本思想

- C4.5算法由【1993年J. R. Quinlan提出】。
- 核心思想：采用信息增益率(Information Gain Ratio)替代ID3算法中的信息增益，即以信息熵为节点纯度度量，每次优先选取信息增益率最大的属性，缓解信息增益对取值较多属性的偏好。

### 关键公式

- 信息增益率定义：
$$G_{ratio}(D, a)=\frac{G(D, a)}{IV(a)}$$
- 属性$a$的固有值(Intrinsic Value)：
$$IV(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} log _{2} \frac{\left|D^{v}\right|}{|D|}$$
  其中，属性$a$的可能取值$V$越大，固有值通常也越大。

### 算法流程

- 输入：训练数据集$D$、属性集$A$。
- 过程：
  1. 若$D$中所有样本属于同一类，则置为单结点树，该类为结点类标记，返回；
  2. 若$A$为空集，则置为单结点树，$D$中样本数最大类为结点类标记，返回；
  3. 否则，计算$A$中各属性对$D$的信息增益率，选择信息增益率最大的属性$a_*$；
  4. 如果$a_*$的信息增益率小于阈值，则置为单结点树，$D$中样本数最大类为该结点类标记，返回；
  5. 否则，对$a_*$的每个可能取值$a_*^v$，构建子结点，对应$D$中属性$a_*$取$a_*^v$的样本子集$D^v$；
  6. 对每个子结点，以$D^v$和$A \setminus \{a_*\}$为输入，递归调用上述步骤，返回决策树。
- 输出：决策树。

## CART (Classification And Regression Tree)算法

### 基本思想

- CART算法由【1984年L. Breiman提出】，是一种采用基尼指数选择划分属性的二叉决策树。
- 核心优势：相较于ID3与C4.5算法，CART更加高效灵活，可解释性更强。

### 基尼指数定义

- 数据集$D$的基尼指数：直观反映从数据集中随机抽取两个样本，其类别不一致的概率；基尼指数越小，数据集的纯度越高。
$$Gini(D)=\sum_{k=1}^{K} \sum_{k' \neq k} p_{k} p_{k'}=1-\sum_{k=1}^{K} p_{k}^{2}=1-\sum_{k=1}^{K}\left(\frac{\left|D_{k}\right|}{|D|}\right)^{2}$$
- 属性$a$的基尼指数：
$$Gini(D, a)=\sum_{v=1}^{V} \frac{|D^{v}|}{|D|} Gini(D^{v})$$
- 属性$a$特征值$a_v$的基尼指数（二叉划分）：
$$Gini\left(D, a_{v}\right)=\frac{\left|D_{l}\right|}{|D|} Gini\left(D_{l}\right)+\frac{\left|D_{r}\right|}{|D|} Gini\left(D_{r}\right)$$
  其中，$D_l$、$D_r$是以$a_v$为分割点将$D$分割成的两部分。

### 最优属性选择

- 选择基尼指数最小的特征值对应的属性作为最优划分属性：
$$a_{*}^{v}=\underset{a \in A}{\arg \min } Gini(D, a_{v})$$

### 算法流程

- 输入：训练数据集$D$、属性集$A$。
- 过程：
  1. 若$D$中所有样本属于同一类或$A$为空集，则置为单结点树，$D$中样本数最大类为结点类标记，返回；
  2. 否则，对每个属性$a \in A$的每个特征值$a_v$，计算基尼指数$Gini(D, a_v)$；
  3. 选择基尼指数最小的特征值$a_*^v$对应的属性$a_*$作为划分属性，按$a_*^v$将$D$分割为$D_l$和$D_r$，构建左右子结点；
  4. 以$D_l$、$A \setminus \{a_*\}$和$D_r$、$A \setminus \{a_*\}$为输入，递归调用上述步骤，返回二叉决策树。
- 输出：二叉决策树。

# 13.4 剪枝算法

## 过拟合问题

### 过拟合现象

- ID3算法构建的决策树对训练数据有很好的分类能力，但对未知的测试数据未必有好的分类能力，泛化性能弱，即可能发生过拟合现象。

### 过拟合原因

- 训练数据有噪声，决策树同时拟合了数据和噪音，影响分类效果；
- 叶结点数量太多，每个结点的样本太少，易出现耦合的规律性，导致一些与真实数据分布无关的属性恰巧被正确分类。

## 剪枝的目的

- 剪枝是解决决策树过拟合的主要手段，目的是通过剪去部分叶结点，提高决策树的泛化性能，即决策树在测试数据上的分类准确率。

## 预剪枝算法(Pre-pruning)

### 定义

- 预剪枝是在决策树构建过程中，对每个结点在划分前进行估计，若划分不能带来决策树泛化性能提升，则停止划分并将该结点设为叶结点。

### 算法流程

1. 在决策树构建的每个结点划分前，评估该结点划分后的泛化性能；
2. 若划分后的泛化性能优于或等于划分前，则执行划分；
3. 若划分不能带来泛化性能提升，甚至降低泛化性能，则停止划分，将当前结点设为叶结点，标记为该结点样本集中数量最多的类别。

### 特点

- 优势："剪掉"很多没必要展开的分支，降低了过拟合风险，并且显著减少了决策树的训练时间开销和测试时间开销；
- 劣势：有些分支的当前划分有可能不能提高甚至降低泛化性能，但后续划分有可能提高泛化性能；预剪枝禁止这些后续分支的展开，可能会导致欠拟合。

## 后剪枝算法(Post-pruning)

### 定义

- 后剪枝是在决策树建立后，自底向上对非叶结点进行考察，若将该结点对应子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。

### 算法流程

1. 利用训练集完整构建一棵决策树；
2. 自底向上遍历所有非叶结点，逐一评估将该结点对应的子树替换为叶结点后的泛化性能；
3. 若替换后的泛化性能优于替换前，则执行剪枝，将该子树替换为叶结点，标记为该子树样本集中数量最多的类别；
4. 若替换后泛化性能未提升，则保留原分支结构；
5. 重复步骤2-4，直至所有非叶结点均被评估完毕。

### 特点
- 优势：测试了所有分支，比预剪枝决策树保留了更多分支，降低了欠拟合的风险，泛化性能一般优于预剪枝决策树；
- 劣势：后剪枝过程在完全构建决策树后再进行，且要自底向上对所有非叶结点逐一评估；因此，决策树的训练时间开销要高于未剪枝决策树和预剪枝决策树。

# 13.5 对于不同类型属性的处理

## 连续值处理

### 基本思想

- 采用二分法 (Bi-Partition)对连续属性进行离散化处理。

### 信息增益计算

- 对于连续属性$a$，候选划分点集合表示为$T_a$，信息增益表示为：
$$\begin{aligned} G(D, a) & =max _{t \in T_{a}} G(D, a, t) \\ & =max _{t \in T_{a}}\left(H(D)-\sum_{\lambda \in\{-,+\}} \frac{\left|D_{t}^{\lambda}\right|}{|D|} H\left(D_{t}^{\lambda}\right)\right) \end{aligned}$$
- 其中，$G(D, a, t)$是样本集$D$基于划分点$t$二分后的信息增益，需选择使$G(D, a, t)$最大的划分点$t$作为最佳划分属性值。

### 算法流程

- 输入：训练数据集$D$、连续属性$a$。
- 过程：
  1. 对连续属性$a$的所有取值按升序排序；
  2. 生成候选划分点集合$T_a$，对相邻取值的中点作为候选划分点；
  3. 对每个候选划分点$t$，计算二分后的信息增益$G(D, a, t)$；
  4. 选择信息增益最大的候选划分点$t_*$，以$t_*$为界将连续属性$a$离散化为"≤$t_*$"和">$t_*$"两类。
- 输出：离散化后的属性划分结果。

## 缺失值处理

### 形式化定义

- $\tilde{D}$为样本集$D$中在属性$a$上没有缺失值的样本子集；
- 属性$a$有$V$个可能取值；
- $\tilde{D}^{v}$为$\tilde{D}$中在属性$a$上取值为$a^{v}$的样本子集；
- $\tilde{D}_{k}$为$\tilde{D}$中属于第$k$类的样本子集；
- $\omega_{x}$为每个样本$x$的权重。

### 核心问题

1. 在属性值缺失情况下，如何进行划分属性选择？
2. 给定划分属性，若该样本在属性上的值缺失，如何对样本进行划分？

### 基本思想

- 在属性值缺失情况下，仅使用无缺失值样本计算信息增益，并选择最佳划分属性；
- 在给定划分属性情况下，将在该属性上的值缺失的样本以不同的概率划分到不同分支中。

### 关键参数定义

- $\rho=\frac{\sum_{x \in \tilde{D}} \omega_{x}}{\sum_{x \in D} \omega_{x}}$：无缺失值样本所占比例；
- $\bar{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}$：无缺失样本中第$k$类所占比例；
- $\bar{r}_{v}=\frac{\sum_{x \in \tilde{D}^{v}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}$：无缺失样本中属性$a$上取值为$a^{v}$的样本所占比例。

### 处理方法

- 划分属性选择：基于无缺失值样本子集$\tilde{D}$计算各属性的信息增益，选择信息增益最大的属性作为划分属性；
- 样本划分规则：
  1. 若样本$x$在划分属性$a$上的取值已知，则将$x$划入与取值对应的子结点，样本权值保持为$\omega_{x}$；
  2. 若样本$x$在划分属性$a$上的取值未知，则将$x$同时划入所有子结点，样本权值在与属性值$a^{v}$对应的子结点中调整为$\bar{r}_{v} \cdot \omega_{x}$。

## 不同代价属性的处理

### 基本思想

- 在属性筛选度量中考虑属性的不同代价，优先选择低代价属性构建决策树，必要时才依赖高代价属性。

### 属性筛选度量标准

- 标准1：结合信息增益与属性代价，平衡划分效果与测量成本；
- 标准2：$G'_{ratio}(D, a)=\frac{G_{ratio}(D, a)}{cost(a)^{\lambda}}$，其中$cost(a)$为属性$a$的代价，$\lambda \in [0,1]$为常数，决定代价对于信息增益率的相对重要性。