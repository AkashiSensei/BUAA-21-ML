# 6.1 什么是神经网络

神经网络是模拟人脑神经元连接结构与信息处理机制的机器学习模型，通过多层神经元的加权连接与非线性变换处理复杂数据，核心是通过训练调整连接权值优化性能，是模式识别领域的重要模型。

## 神经网络的发展概述

### 三大核心发展阶段

1. **传统神经网络时期**（20世纪40年代-2006年）：以理论研究为主，历经萌芽期（MP模型、Hebb学习规则）、第一次高潮期（感知机模型）、反思期（感知机非线性局限被指出）、第二次高潮期（Hopfield网络、BP算法）、应用探索期（金融预测、手写字符识别等初步应用），代表性工作包括MP模型、感知机、反向传播算法。
2. **深度神经网络时期**（2006-2018年）：模型复杂度与精度大幅提升，2006年Hinton提出“逐层初始化”突破训练难题，2012年AlexNet实现图像分类里程碑突破，2015年ResNet通过跳跃连接缓解梯度消失，代表性工作有AlexNet、ResNet、R-CNN系列。
3. **大模型时期**（2018年至今）：模型参数达数十亿级别，2017年Transformer架构奠定大规模训练基础，2018年BERT等预训练模型问世，2020年ChatGPT推动AI对话系统普及，代表性工作包括Transformer、BERT、GPT、CLIP。

## 神经网络的应用

- 计算机视觉领域：人脸识别与门禁系统（身份精准核验）、自动驾驶（路况感知、障碍物与交通标识识别）；
- 自然语言处理与生成领域：AI对话（智能客服、ChatGPT）、AI作图（文本描述生成图像）；
- 其他领域：金融预测、手写字符识别等。

# 6.2 神经元计算模型

## 神经元的结构

### 生物神经元

- 一个神经元有两种状态：兴奋和抑制。
- 平时处于抑制状态的神经元，其树突和胞体接收其他神经元由突触传来的兴奋电位，多个输入在神经元中以代数和的方式叠加；如果输入兴奋电位总量超过某个阈值，神经元会被激发进入兴奋状态，发出输出脉冲，并由突触传递给其他神经元。
- 神经元被触发后进入不应期，在不应期不能被触发，然后阈值逐渐下降，恢复兴奋性。

数量与连接：
- 人类的大脑皮层大约有14,000,000,000个神经细胞，亦称为神经元。
- 每个神经元有数以千计的通道同其它神经元广泛相互连接，形成复杂的生物神经网络。

### 人工神经元：MP模型

MP模型是一种人工神经元的数学模型，它最早是由美国的McCulloch和Pitts提出的神经元模型，是大多数神经网络模型的基础。

结构：
- 神经元可以有$N$个输入：$x_{1}, x_{2}, ..., x_{N}$。
- 每个输入端与神经元之间有一定的联接权值：$w_{1}, w_{2}, ..., w_{N}$。
- 神经元总的输入为对每个输入的加权求和，同时减去阈值$\theta$代表神经元的活跃值，即神经元状态：
$$a=\sum_{i=1}^{N} w_{i} x_{i}-\theta=\sum_{i=0}^{N} w_{i} x_{i}$$
- 神经元的输出$y$是对$a$的映射：
$$y=f(a)=f\left(\sum_{i=0}^{N} w_{i} x_{i}\right)$$
- 其中$f$称为激活函数（激励函数，输出函数），有很多种形式。

激活函数：
- MP模型采用阶跃函数作为激活函数：
$$f(x)=\left\{\begin{array}{rl} 1, & x \geq 0 \\ -1, & x<0 \end{array}\right. \Rightarrow y=sign\left(\sum_{i=0}^{N} w_{i} x_{i}\right)$$
- 当激活函数$f$为阶跃函数时，神经元就可以看作是一个线性分类器。

问题：
- 梯度为零：函数的输出不随输入变化，导致难以使用梯度法更新权重；
- 非线性问题：无法处理复杂的非线性关系，仅适用于简单的线性可分问题。

## 神经元的计算

### Sigmoid激活函数

- 将输入映射到$(0, 1)$。
- 一个平滑的版本的阶跃函数：
$$sigmoid(x)=\frac{1}{1+exp (-x)}$$

### 双曲正切(tanh)激活函数

- 将输入映射到$(-1, 1)$：
$$tanh (x)=\frac{1-exp (-2 x)}{1+exp (-2 x)}$$

### ReLU激活函数

- 线性修正单元。
- 常用于各种卷积神经网络：
$$ReLU(x)=max(x,0)$$

### GELU激活函数

- 常用于各种Transformer：
$$GELU(x)=xP(X\leq x)=x\Phi (x)$$
$$=x \cdot \frac{1}{2}[1+erf(x / \sqrt{2})]$$
其中$erf(x)=\frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-\eta^{2}} d \eta$。

# 6.3 多层感知机及神经网络

## 感知机

### 什么是感知机?
- 感知机是一种早期的神经网络模型，由美国学者F. Rosenblatt于1957年提出。
- 感知机中第一次引入了学习的概念，使人脑所具备的学习功能在基于符号处理的数学中得到了一定程度的模拟，所以引起了广泛的关注。
- 感知机模型可以通过监督学习来逐步增强模式划分的能力，达到学习的目的。

### 感知机的结构

#### 单输出的感知机(MP模型)

输出层神经元与输入层神经元连接，输出为：
$$y=f(a)=f\left(\sum_{i=0}^{N} w_{i} x_{i}\right)$$

#### 多输出的感知机

输出层神经元与输入层神经元连接，第$j$个输出为：
$$y_{j}=f\left(a_{j}\right)=f\left(\sum_{i=0}^{N} w_{i j} x_{i}\right)$$

### 感知机与感知机准则

- 感知机准则：对随意给定的判别函数初始值，通过样本分类训练过程逐步对其修正直至最终确定。
- 感知机可以作为感知机准则的判别函数。
- 感知机准则可以用于训练感知机，如：$y=f(a)=f\left(\sum_{i=0}^{N} w_{i} x_{i}\right)=sign\left(-2 x_{1}-2 x_{2}+3\right)$，对应的决策面更新为$W_{k+1} x+b=0$（原决策面为$W x+b=0$）。

### 线性不可分问题:感知机模型的局限

- 示例：异或(XOR)问题，单层感知机无法解决。
- 解决方法：多层感知机。
- 局限原因：
  - 单层结构：单层感知机缺乏隐藏层，无法捕捉复杂的非线性关系。
  - 激活函数限制：使用的阶跃函数无法调整非线性决策边界。

## 多层感知机

### 异或问题的解决

通过三层感知机可解决异或问题，具体映射关系：
$$x_{1}^{(1)}=sign\left(x_{1}^{(0)}+x_{2}^{(0)}-0.5\right)$$
$$x_{2}^{(1)}=sign\left(-x_{1}^{(0)}-x_{2}^{(0)}+1.5\right)$$
$$y=sign\left(x_{1}^{(1)}+x_{2}^{(1)}-1.5\right)$$

### 单隐藏层的多层感知机

- 输入：$x \in \mathbb{R}^{n}$
- 隐藏层：$W_{1} \in \mathbb{R}^{m × n}, b_{1} \in \mathbb{R}^{m}$
- 输出层：$W_{2} \in \mathbb{R}^{m × l}, b_{2} \in \mathbb{R}^{l}$
- 隐藏层输出：$h=\sigma\left(W_{1} x+b_{1}\right)$（$\sigma$为非线性激活函数）
- 输出层输出：$y=W_{2}^{T} h+b_{2}$

## 通用近似定理

定理-通用近似定理(Universal Approximation Theorem) [Cybenko,1989,Hornik et al.,1989]：令$\varphi(\cdot)$是一个非常数、有界、单调递增的连续函数，$I_{d}$是一个$d$维的单位超立方体$[0,1]^d$，$C(I_{d})$是定义在$I_{d}$上的连续函数集合。对于任何一个函数$f \in C(I_{d})$，存在一个整数$m$，和一组实数$v_{i}$、$b_{i} \in \mathbb{R}$以及实数向量$w_{i} \in \mathbb{R}^{d}$（$i=1, \cdots, m$），以至于可以定义函数：
$$F(x)=\sum_{i=1}^{m} v_{i} \varphi\left(w_{i}^{\top} x+b_{i}\right)$$
作为函数$f(x)$的近似实现，即：
$$|F(x)-f(x)|<\epsilon, \forall x \in I_{d}$$
其中$\epsilon>0$是一个很小的正数。

- 三层感知机可以实现任意的逻辑运算；
- 只要激活函数满足非线性、连续且几乎处处可导（如：sigmoid、ReLU），一个隐藏层大小足够大的三层感知机就可以逼近任何非线性多元函数。

## 多层感知机为什么需要非线性激活函数?

若激活函数为线性函数，则：
$$y=W_{2}^{T} h+b_{2}=W_{2}^{T} \sigma\left(W_{1} x+b_{1}\right)+b_{2} \Rightarrow y=W_{2}^{T} W_{1} x+b'$$
多层感知机退化为线性模型，无法捕捉非线性关系，因此必须使用非线性激活函数。

## 多层感知机的局限性

- 计算复杂度高：当多层感知机表达能力足够强时，隐藏层的大小较高，使得模型总体参数量较大、计算资源消耗较高；
- 训练困难：层数较深的多层感知机难以训练，容易陷入局部最优；
- 特征提取不足：对复杂数据（如图像、序列）直接使用容易产生过拟合现象，实际应用中效果不佳。

## 其他常见神经网络

### 核心应用导向的神经网络

- 卷积神经网络(CNN)：用于图像和视频处理，提取空间特征；
- 递归神经网络(RNN)：用于处理序列数据，捕捉时间序列中的依赖关系；
- 图神经网络(GNN)：用于处理图结构数据，捕捉节点和边之间的复杂关系。

### Hopfield网络

- 是一种反馈网络。反馈网络具有一般非线性系统的许多性质，如稳定性问题等，在某些情况下还有随机性、不可预测性。因此它比前馈网络的内容复杂。
- 除具有反馈网络的结构和性质外，还满足以下条件：
  - 权值对称，即$w_{i j}=w_{j i}$，权矩阵$W=W^{T}$为对称阵；
  - 无自反馈，即$w_{i i}=0$，矩阵$W$的对角线元素为0。

### 径向基函数神经网络(RBF网络)

- 只有一个隐层，隐层单元采用径向基函数作为其输出函数，输入层到隐层之间的权值均固定为1；输出节点为线性求和单元，隐层到输出节点之间的权值可调，因此，输出为隐层的加权求和，径向基函数为：$h(x)=\exp \left(-\frac{(x-c)^{2}}{r^{2}}\right)$。
- SVM可以使用RBF（径向基函数）核。RBF核在SVM中用于将数据映射到高维空间，处理非线性可分的数据。

### 自适应共振理论神经网络

- 通过反复将输入学习模式由输入向输出层自下而上短时记忆和由输出向输入层自上而下长期记忆和比较来实现。
- 当记忆和比较达到共振时，输出矢量可正确反映输入学习模式的分类，且网络原有记忆不受影响。

# 6.4反向传播算法

## 前馈神经网络

### 前馈神经网络的定义

- 前馈神经网络中，各神经元分层排列，每个神经元只与前一层的神经元相连。
- 每层接收前一层的输出，并输出给下一层，各层间没有反馈。
- 多层感知机是前馈神经网络的一种。

### 前馈神经网络的前向传播

激活函数定义：
$$\sigma(a)=\frac{1}{1+exp (-a)}$$

输出层输出：
$$y_{k}(x, w)=\sigma\left(\sum_{j=1}^{M} w_{k j}^{(2)} h\left(\sum_{i=1}^{D} w_{j i}^{(1)} x_{i}\right)\right)$$

其中，各层计算过程如下：
- 隐藏层输入：$a^{(1)} = W_{1} x + b_{1}$
- 隐藏层输出：$h = h(a^{(1)})$
- 输出层输入：$a^{(2)} = W_{2}^{T} h + b_{2}$
- 输出层输出：$y = \sigma(a^{(2)})$

## 如何训练神经网络?

### 感知机准则训练多层感知机的局限性

- 感知机准则是利用理想输出与实际输出之间的误差作为增量来修正权值。
- 多层感知机只能计算出输出层的误差，中间隐层由于不直接与外界连接，其误差无法估计。

### 反向传播算法的思想

- 从后向前反向逐层传播输出层的误差，以间接计算隐层的误差。
- 算法可以分为两个阶段：
  - 前馈（正向过程）：从输入层经隐层逐层正向计算各单元的输出；
  - 学习（反向过程）：由输出误差逐层反向计算隐层各单元的误差，并用此误差修正前层的权值。

### 反向传播算法（BP）的核心定义

- 定义准则函数（损失函数）：
$$E(w)=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, w\right)-t_{n}\right\}^{2}$$
- 目标：寻找权重$w$，使得$E(w)$最小。
- 计算准则函数对于权重的梯度$\nabla E(w)$，当$\nabla E(w)=0$时，$E(w)$最小；
- 按照梯度方向更新权重：
$$w^{(\tau+1)}=w^{(\tau)}-\eta \nabla E\left(w^{(\tau)}\right)$$

- Geoffrey Hinton（1947-）于1986年首次提出将反向传播算法用于神经网络学习。

## BP反向传播算法

### 单层感知机的BP
- 样本集：$\{x_{n}, t_{n}\}_{n=1}^{N}$，$x \in \mathbb{R}^{D}$，$t \in \mathbb{R}^{3}$
- 输出计算：$y_{k}=\sum_{i=1}^{D} w_{k i} x_{i}$
- 样本损失：$E_{n}(w)=\frac{1}{2} \sum_{k=1}^{3}\left(y_{n k}-t_{n k}\right)^{2}$
- 梯度计算：$\frac{\partial E_{n}}{\partial w_{k i}}=\left(y_{n k}-t_{n k}\right) x_{n i}$

### 多层感知机的BP

- 网络结构：第$i$层（输入层）、第$j$层（隐藏层）、第$k$层（输出层）
- 各层计算关系：$a_{j}=\sum_{i} w_{j i} x_{i}$，$h_{j}=h\left(a_{j}\right)$，$y_{k}=\sum_{j} w_{k j} h_{j}$
- 样本损失：$E_{n}=\frac{1}{2} \sum_{k}\left(y_{k}-t_{k}\right)^{2}$
- 梯度计算：
$$\begin{aligned} \frac{\partial E_{n}}{\partial w_{j i}} & =\sum_{k} \frac{\partial E_{n}}{\partial y_{k}} \frac{\partial y_{k}}{\partial h_{j}} \frac{\partial h_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial w_{j i}} \\ & =\sum_{k}\left(y_{k}-t_{k}\right) \frac{\partial y_{k}}{\partial h_{j}} \frac{\partial h_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial w_{j i}} \\ & =\sum_{k}\left(y_{k}-t_{k}\right) w_{k j} h'\left(a_{j}\right) x_{i} \end{aligned}$$

### 整体算法流程

1. 初始化权重$w_{i j}$；
2. 对于输入的训练样本，求取每个节点输出和最终输出层的输出值；
3. 对于输出层求误差项：$\delta_{k}=y_{k}-t_{k}$；
4. 对于隐藏层求误差项：
$$\delta_{j}=h'\left(a_{j}\right) \sum_{k} w_{k j} \delta_{k}$$
5. 求输出误差对于每个权重的梯度：$\frac{\partial E_{n}}{\partial w_{j i}}=\delta_{j} x_{i}$；
6. 更新权重：$w^{(\tau+1)}=w^{(\tau)}-\eta \nabla E(w^{(\tau)})$。