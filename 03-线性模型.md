# 3.1 什么是线性回归

## “回归”的起源

研究父母身高与子女身高之间关系：若父母身高高于平均身高，则其子女身高倾向于倒退生长，即会比其父母身高矮一些而更接近于大众平均身高；若父母身高小于平均身高，则其子女身高倾向于向上生长，即会更接近于大众平均身高。

此现象被英国生物学家Francis Galton（1822-1911）称之为回归现象，即Regression。

## 回归的概念

### 回归问题

- 回归是监督学习问题中的一种，其目标是将输入向量分配至由一个或多个连续变量组成的输出。
- 数学表示为给定输入数据$x$和一个连续输出$y$，回归任务的目标是找到一个函数$f: x \to y$，使得$f(x)$能够尽可能准确地预测$y$的值。（Christopher M. Bishop, Pattern Recognition and Machine Learning）

## 线性回归的概念

### 问题建模

训练集$(x_i, y_i)$，回归算法通过输入变量（特征）$x$计算输出变量$y$，核心是确定假设函数$f$的形式。

### 线性回归的定义

假设函数$f$为输入$x$的线性函数：

$$\begin{aligned} f(x) & =w_{0}+w_{1} x_{1}+\cdots+w_{m} x_{m} \\ & =w_{0}+\sum_{i=1}^{m} w_{i} x_{i} \\ & =\sum_{i=0}^{m} w_{i} x_{i} \end{aligned}$$

**术语说明**：严格数学意义上，包含常数项 $w_0$ 的函数 $f(x) = w_0 + w_1 x_1 + \cdots$ 是**仿射函数**（不满足线性函数的加法保持性 $f(x+y) = f(x) + f(y)$），但在机器学习和统计学中，由于函数关于参数 $w_i$ 是线性的，通常仍称为"线性模型"或"线性函数"。

### 基函数类型

线性模型的一般形式可以表示为：

$$y(x, w) = \sum_{j=0}^{M-1} w_j \phi_j(x) = w^T \phi(x)$$

其中 $\phi_j(x)$ 称为**基函数**（Basis function），用于将输入 $x$ 映射到特征空间，使得模型能够学习非线性关系。

**基函数的定义**：基函数是一组固定的非线性函数，通过对输入进行变换，将原始输入空间映射到新的特征空间，使得在这个新空间中可以用线性组合来建模复杂的非线性关系。

**常见基函数类型**：

- **最简单的情况下**：$\phi_j(x) = x_j$，即直接使用原始特征，此时模型为标准的线性模型
- **多项式基函数**：$\phi_j(x) = x^j$，通过幂次变换扩展特征空间，适用于建模多项式关系
- **高斯基函数**：$\phi_j(x) = \exp\left\{-\frac{(x-\mu_j)^2}{2s^2}\right\}$，其中 $\mu_j$ 为基函数的中心位置，$s$ 为控制宽度的参数，适用于局部平滑的非线性建模
- **Sigmoid基函数**：$\phi_j(x) = \sigma\left(-\frac{x-\mu_j}{s}\right)$，其中 $\sigma(a) = \frac{1}{1+\exp(-a)}$ 为Sigmoid函数，$\mu_j$ 为基函数的中心位置，$s$ 为控制斜率的参数，适用于S型非线性关系建模

# 3.2 线性回归的求解

## 问题分析

- 问题本质：确定模型中的参数 $w$
- 基本思想：基于训练集最小化预测值 $\hat{y}=f(x)$ 与真实输出值 $y$ 的差异
- 定义目标函数（又叫代价函数）：常见的有基于平均绝对误差、均方误差和均方根误差（详见课程2.2），以基于均方误差的目标函数为例
- 求解方法：标准方程组法、梯度下降法、最大似然估计、贝叶斯估计

## 标准方程组法（Normal Equations）

### 目标函数

$$J(w)=\sum_{i=1}^{N}\left(f\left(x_{i}\right)-y_{i}\right)^{2}=\sum_{i=1}^{N}\left(w^{T} x_{i}-y_{i}\right)^{2}$$

### 目标函数的矩阵形式

$$J(w)=\sum_{i=1}^{N}\left(w^{T} x_{i}-y_{i}\right)^{2}=(X w-y)^{T}(X w-y)$$

其中：

$$X=\left( \begin{array} {c}{x_{1}^{T}}\\ {x_{2}^{T}}\\ {\cdots }\\ {x_{N}^{T}}\end{array} \right) =\left[ \begin{array} {llll}{x_{10}}&{x_{11}}&{\cdots }&{x_{1m}}\\ {x_{20}}&{x_{21}}&{\cdots }&{x_{2m}}\\ &{\vdots }&{\ddots }&{\vdots }\\ {x_{N0}}&{x_{N1}}&{\cdots }&{x_{Nm}}\end{array} \right],\ y=\left( \begin{array} {c}{y_{1}}\\ {y_{2}}\\ {\cdots }\\ {y_{N}}\end{array} \right)$$

### 参数求解

对目标函数直接求导，并令其导数等于0，求得极值：

$$\begin{aligned} \frac{\partial}{\partial w} J(w) & =\frac{\partial}{\partial w}(X w-y)^{T}(X w-y) \\ & =2 X^{T}(X w-y)=0 \\ \Rightarrow X^{T} X w & =X^{T} y \end{aligned}$$

得到模型的参数：

$$w=(X^{T} X)^{-1} X^{T} y$$

## 梯度下降法（Gradient Descent）

梯度下降是一个最优化算法，是求解无约束优化问题最基础的方法之一。

更新参数让目标函数沿负梯度方向下降。

### 一般流程

- 首先对参数 $w$ 赋值
- 更新 $w$ 的值，使得 $J(w)$ 按梯度下降的方向减少直到收敛
- 越接近目标值，搜索步长越小，前进越慢

### 损失函数与目标

- 损失函数：$J(w)=\sum_{i=1}^{N}(f(x_{i})-y_{i})^{2}$
- 目标：$min _{w} J(w)$

### 梯度下降法的步骤

1. 给定初始值 $w^{0}$，这个值可以是随机生成的，也可以是一个全零的向量
2. 更新 $w$ 使得 $J(w)$ 越来越小，$\alpha$ 为学习率或更新步长：
$$w_{j}^{t}=w_{j}^{t-1}-\alpha \frac{\partial}{\partial w_{j}} J(w)=w_{j}^{t-1}-\alpha \sum_{i=1}^{N}\left(f\left(x_{i}\right)-y_{i}\right) \cdot x_{i, j}$$
3. 同时更新函数值的各维 $f(x_{i})=[w^{t}]^{T} x_{i}$

### 批处理梯度下降（Batch Gradient Descent）

- 特点：每次更新都利用所有数据
- 优势：对于凸函数，可以达到全局最优
- 劣势：在大样本条件下，迭代速度很慢

### 随机梯度下降（Stochastic Gradient Descent）

每次只用一个样本 $(x_{r}, y_{r})$

- 更适用于大样本数据情况
- 对于更复杂的优化目标，可以跳出局部最优解

#### 变化形式
- 在线学习（Online Learning）：每次“看”一个样本，对所有样本循环使用多次（一次循环称为一个Epoch）
- 每次可以看一些样本（Mini-Batch）

## 标准方程组 v.s. 梯度下降

|            | 标准方程组                | 梯度下降                      |
|------------|--------------------------|-------------------------------|
| 超参数     | 不需要超参数$\alpha$      | 需要选择$\alpha$               |
| 迭代次数   | 不需要选迭代次数          | 需要选迭代次数                  |
| 归一化     | 无需数据归一化            | 需要数据归一化                  |
| 适用情况   | 样本量大时不适用，需要计算$(X^TX)^{-1}$ | 样本量非常大时也适用           |

- 样本量较小时选用标准方程组求解
- 样本量较大时选用梯度下降法求解

## 从概率视角看回归

第一章的曲线拟合中，目标变量值出现不确定性，可用概率分布表示。假定目标值 $t$ 服从以 $y(x, w)$ 为均值，$\sigma^{2}$ 为方差的高斯分布。

## 最大似然估计（Maximum Likelihood Estimation）

### 核心思想

把待估计的参数 $w$ 看做是确定的量，只是其取值未知。最佳估计就是使得观测到的样本的概率最大的那个值。

### 似然函数

已知训练数据 $x$ 和 $t$，现有新输入变量 $x$，需要预测目标变量 $t$，似然函数表示为：

$$p(t | x, w, \beta)=\prod_{n=1}^{N} N(t_{n} | y(x_{n}, w), \beta^{-1})$$

其中 $N(\cdot|\mu, \sigma^2)$ 表示高斯（正态）分布函数，即

$$
N(a | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{(a - \mu)^2}{2\sigma^2} \right)
$$

在上式中，$N(t_n | y(x_n, w), \beta^{-1})$ 表示在输入 $x_n$ 和参数 $w$ 下，模型输出 $y(x_n, w)$ 为均值，方差为 $\beta^{-1}$ 的高斯分布，$t_n$ 是观测到的真实目标变量。

这里的连乘表示对全部样本点的联合概率，也即假设每个观测$(x_n, t_n)$都是独立同分布（i.i.d.），因此整体的似然概率等于每个样本条件概率的乘积。换句话说，就是所有样本观测值出现的联合概率，体现了参数$w$在给定训练集下拟合数据的能力。

这里的 $x = \{x_1, x_2, ..., x_N\}$ 表示训练集中所有输入样本的特征；$t = \{t_1, t_2, ..., t_N\}$ 表示所有对应的目标变量（即数据的标签或实际观测值）。

“目标变量”通常指待预测的量（例如回归中的房价、分数等，或分类中的标签），在监督学习问题中就是样本的“标签”或者“输出值”。
也就是说，这里的 $t$ 就是我们希望模型能够根据输入 $x$ 正确预测的真实值。

### 对数似然函数

$$\ln p(t | x, w, \beta)=\frac{\beta}{2} \sum_{n=1}^{N}(y(x_{n}, w)-t_{n})^{2}+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)$$

### 参数求解与结果

- 通过最大化对数似然函数解得 $w_{ML}$
- 此时高斯分布的参数：$\frac{1}{\beta_{M L}}=\frac{1}{N} \sum_{n=1}^{N}(y(x_{n}, w_{M L})-t_{n})^{2}$
- 得到概率分布：$p(t | x, w_{M L}, \beta_{M L})=N(t | y(x, w_{M L}), \beta_{M L}^{-1})$
- 等价于以均方误差为损失函数的线性回归的解

## 贝叶斯估计

### 核心思想

把待估计的参数 $w$ 看做是符合某种先验概率分布的随机变量。对样本进行观测的过程，就是把先验概率密度转化为后验概率密度，从而利用样本信息修正了对参数的初始估计值。

### MAP（最大后验估计）：离贝叶斯更近一步
1. 假定 $w$ 的先验分布为高斯分布，其中 $\alpha$ 是分布精度，$M$ 是 $w$ 的阶数
2. 最大化 $\ln p(w | x, t, \alpha, \beta)$ 即最大化对应对数表达式，等价于最小化正则化损失函数
3. 令 $\lambda=\frac{\alpha}{\beta}$ 为正则化系数，令 $\nabla_{w} \tilde{E}(w)=0$ 得解，等价于以正则平方误差为损失函数的线性回归的解

### 贝叶斯估计相关公式
- 预测分布简单表示为两个高斯分布的卷积
- 使用边际高斯和条件高斯的转换可推导相关概率分布

# 3.3 什么是线性分类器?

## 几个问题

- 已知类条件概率密度 $p(x | \omega_{i})$ 和先验概率 $P(\omega_{i})$，通过贝叶斯定理计算后验概率 $p(\omega_{i} | x)$ 进行决策
- 若类条件概率密度参数未知？已知类条件概率密度 $p(x | \omega_{i})$ 的参数表达式，利用样本估计 $p(x | \omega_{i})$ 的未知参数，再利用贝叶斯定理将其转化成后验概率 $p(\omega_{i} | x)$ 进行决策
- 若类条件概率密度形式难以确定？非参数方法估计，需要大量样本

分类问题有三种情况：

1. 理想情况：已知各类别的概率分布和先验概率，可直接用贝叶斯定理计算后验概率进行分类。
2. 参数未知：知道概率分布的形式，但参数未知，需要用样本估计参数，再计算后验概率。
3. 形式未知：不知道概率分布的形式，需要用非参数方法估计，但需要大量样本。

线性分类器属于第2、3种情况的简化：不估计概率分布，直接从样本设计分类器。

| 情况         | 类条件概率密度状态        | 使用的估计方法                              |
|------------|----------------------|-----------------------------------------|
| 第一种情况    | 已知（或已估计好）        | 不需要估计，直接用贝叶斯定理                         |
| 第二种情况    | 形式已知，参数未知        | 最大似然估计或贝叶斯估计                             |
| 第三种情况    | 形式未知               | 非参数估计方法（Parzen窗、k近邻等）                  |

## 线性分类器的概念

线性分类器是利用样本集直接设计分类器，将分类器设计题转化为求准则函数极值的问题。

- 线性回归形式：$y(x)=w^{T} x+w_{0}$，利用样本估计 $w$，对于给定 $x$，计算 $y$
- 准则函数：分类器设计的某些要求的函数形式

## 线性判别函数的一般形式

二分类情况下线性判别函数一般形式为：
$$g(x) = w^{T}x + w_{0}$$

其中：
- $x=\begin{bmatrix}x_{1} & x_{2} & \dots & x_{d}\end{bmatrix}^{T}$ 是特征向量/样本向量
- $w=\begin{bmatrix}w_{1} & w_{2} & \dots & w_{d}\end{bmatrix}^{T}$ 是权向量
- $w_{0}$ 为阈值权（偏置项）

决策规则：
- 若 $g(x) > 0$，则 $x \in \omega_{1}$
- 若 $g(x) < 0$，则 $x \in \omega_{2}$
- 若 $g(x) = 0$，可将 $x$ 分到任意一类或拒绝

$g(x)=0$ 定义了一个决策面，当 $g(x)$ 为线性函数时，决策面就是超平面。

## 线性判别函数的几何理解

1. 法向量特性：如果 $x_{1}$ 和 $x_{2}$ 都在决策面 $H$ 上，则有 $w^{T} x_{1}+w_{0}=w^{T} x_{2}+w_{0}$，即 $w^{T}(x_{1}-x_{2})=0$，说明 $w$ 和超平面 $H$ 上任一向量正交，即 $w$ 是 $H$ 的法向量。
2. 原点位置判断：若 $x$ 为原点，则 $g(x)=w_{0}$；从原点到超平面 $H$ 的距离 $r_{0}=\frac{w_{0}}{\|w\|}$
   - 若 $w_{0}>0$，则原点在 $H$ 的正侧
   - 若 $w_{0}<0$，则原点在 $H$ 的负侧
   - 若 $w_{0}=0$，则 $g(x)$ 具有齐次形式，超平面 $H$ 通过原点
3. 距离度量：判别函数 $g(x)$ 正比于 $x$ 点到超平面的代数距离
   - 当 $x$ 在 $H$ 的正侧时，$g(x)>0$
   - 当 $x$ 在 $H$ 的负侧时，$g(x)<0$

## 广义线性判别函数

1. 适用场景：解决非凸和多连通区域划分问题（线性判别函数不适用于此类场景）
2. 示例（一维样本空间二分类问题）：
   - 分类规则：若 $x<a$ 或 $x>b$，则 $x \in \omega_{1}$；若 $a<x<b$，则 $x \in \omega_{2}$
   - 构造判别函数：$g(x)=(x-a)(x-b)$
   - 决策规则：$\begin{cases}g(x)>0, x \in \omega_{1} \\ g(x)<0, x \in \omega_{2}\end{cases}$
3. 一般形式与转化：
   - 原始形式：$g(x)=c_{0}+c_{1} x+c_{2} x^{2}+\cdots+c_{k} x^{k}$
   - 转化为线性形式：$g(x)=a^{T} y=\sum_{i=1}^{m} a_{i} y_{i}$
     - 其中 $y=\begin{bmatrix}y_{1} & y_{2} & \cdots & y_{m}\end{bmatrix}^{T}=\begin{bmatrix}1 & x & x^{2} & \cdots & x^{k}\end{bmatrix}^{T}$（增广特征向量）
     - $a=\begin{bmatrix}a_{1} & a_{2} & \cdots & a_{m}\end{bmatrix}^{T}=\begin{bmatrix}c_{0} & c_{1} & \cdots & c_{k}\end{bmatrix}^{T}$（广义权向量）
4. 特殊情况：当 $g(x)$ 取二次形式时，也称为二次判别分析（QDA）
5. 局限性：利用线性函数的简单性解决复杂问题，但会导致特征维数大大增加，引发“维数灾难”

*更多内容详见Christopher M. Bishop, Pattern Recognition and Machine Learning. P199-200*

## 线性分类器设计的一般过程

1. 准备数据：具有类别标志的样本集 $X=\{x_{1}, x_{2}, ..., x_{N}\}$ 或其增广样本集 $y$
2. 确定准则函数：需满足
   - 是样本集和 $w$、$w_{0}$ 或 $a$ 的函数
   - 其值反映分类器的性能，极值对应“最好”的决策
3. 优化求解：求解准则函数极值 $w^{*}$、$w_{0}^{*}$ 或 $a^{*}$
4. 生成判别函数：最终得到线性判别函数 $g(x)=w^{* T} x+w_{0}^{*}$ 或 $g(x)=a^{* T} y$，对于未知类别样本 $x_{k}$，计算 $g(x_{k})$ 并通过决策规则判断其类别

## 线性判别函数的齐次化

在特征向量前加一个常数1，将偏置项作为权重向量的第一个分量，使分界面过原点，简化数学处理，将偏置项合并到权重向量中。

1. 转化过程：将线性判别函数 $g(x)=w^{T} x+w_{0}$ 转化为齐次形式 $g(x)=a^{T} y$
   - 增广特征向量：$y=\begin{bmatrix}1 & x_{1} & x_{2} & \dots & x_{d}\end{bmatrix}^{T}=\begin{bmatrix}1 & x\end{bmatrix}^{T}$
   - 增广权向量：$a=\begin{bmatrix}w_{0} & w_{1} & \dots & w_{d}\end{bmatrix}^{T}=\begin{bmatrix}w_{0} & w\end{bmatrix}^{T}$
2. 维度变化：增广后特征空间维度 $\hat{d}=d+1$；$y$ 比 $x$ 增加一维，保持样本空间欧式距离不变，变换后的样本仍全部位于 $d$ 维子空间（原 $X$ 空间）中
3. 决策面等价性：方程 $a^{T} y=0$ 在 $Y$ 空间确定了一个通过原点的超平面 $\hat{H}$，它对 $d$ 维子空间的划分与原决策面 $w^{T} x+w_{0}=0$ 对原 $X$ 空间的划分完全相同
4. 距离公式：$Y$ 空间中任意一点 $y$ 到 $\hat{H}$ 的距离为：
$$r=\frac{g(x)}{\|a\| }=\frac{a^{T} y}{\|a\| }$$

# 3.4 Fisher准则

## Fisher准则的推导

寻找最好投影方向$w^*$：

- 以二分类问题为例，$d$维样本$x_{1}, x_{2}, ..., x_{N}$，其中$N_{1}$个属于$\omega_{1}$类记为子集$X_{1}$，$N_{2}$个属于$\omega_{2}$类记为子集$X_{2}$。
- 在$d$维$X$空间：
  - 各类样本的均值向量$m_{i}$：$m_i = \frac{1}{N_i}\sum_{x \in X_i} x$（$i=1,2$）
  - 样本类内离散度矩阵$S_{i}$：$S_i = \sum_{x \in X_i} (x - m_i)(x - m_i)^T$（$i=1,2$）
  - 总类内离散度矩阵$S_{w}$：$S_w = S_1 + S_2$
  - 样本类间离散度矩阵$S_b$：$S_b = (m_1 - m_2)(m_1 - m_2)^T$
- 在一维$Y$空间：$y_{n}=w^{T} x_{n}$
  - 各类样本均值$\tilde{m}_i$：$\tilde{m}_i = \frac{1}{N_i}\sum_{y \in Y_i} y = \frac{1}{N_i}\sum_{x \in X_i} w^T x = w^T m_i$（$i=1,2$）
  - 样本类内离散度$\tilde{S}_{i}^{2}$：$\tilde{S}_i^2 = \sum_{y \in Y_i} (y - \tilde{m}_i)^2 = \sum_{x \in X_i} (w^T x - w^T m_i)^2 = w^T S_i w$（$i=1,2$）
  - 总类内离散度$\tilde{S}_{w}$：$\tilde{S}_w = \tilde{S}_1^2 + \tilde{S}_2^2 = w^T (S_1 + S_2) w = w^T S_w w$

## Fisher准则的概念

考虑把$d$维空间的样本投影到一条直线上形成一维空间。在一般情况下总可以找到某个方向，使样本在这个方向的直线上的投影分开得最好。

Fisher准则就是要解决：如何根据实际情况找到这条最好的、最易于分类的投影方向的问题。

核心目标：希望投影后在一维$Y$空间中各类样本尽可能分开（即两类均值之差越大越好），同时希望各类样本内部尽量密集（即类内离散度越小越好）。

## Fisher准则的求解

### 准则函数定义

准则定义为类间方差与类内方差之比：
$$J_{F}(w)=\frac{(\tilde{m}_{1}-\tilde{m}_{2})^{2}}{\tilde{S}_{1}^{2}+\tilde{S}_{2}^{2}}$$

### 最优投影方向求解

通过Lagrange乘子法求解$J_{F}(w)$取得最大值的$w^*$，最终得到：

$$w^* = S_w^{-1}(m_1 - m_2)$$

### 分类决策

分类时确定分界阈值$y_{0}$，将投影值$y=w^{* T} x$与$y_0$比较进行决策。

# 3.5 感知机准则

## 感知机准则的概念

感知机准则是一种自学习判别函数生成方法，由于Rosenblatt试图将其用于脑模型感知机，因此得名。

该方法对随意给定的判别函数初始值，通过样本分类训练过程逐步对其修正直至最终确定。

## 几个基本概念

### 线性可分性

一组容量为$N$的样本集$y_{1}, y_{2}, ..., y_{N}$，其中$y_{n}$为$\hat{d}$维增广样本向量，分别来自$\omega_{1}$类和$\omega_{2}$类，如果存在权向量$a$，使得对于任何$y \in \omega_{1}$，都有$a^{T} y>0$，而对于任何$y \in \omega_{2}$，都有$a^{T} y<0$，则称这组样本为线性可分的，反之亦然。

### 样本的规范化

设原样本满足：$a^{T} y_{k}>0, y_{k} \in \omega_{1}$；$a^{T} y_{j}<0, y_{j} \in \omega_{2}$。

定义规范化增广样本向量：
$$y_{n}' = \begin{cases} y_{k}, & y_{k} \in \omega_{1} \\ -y_{j}, & y_{j} \in \omega_{2} \end{cases}$$

规范化后，解向量均满足$a^{T} y_{n}'>0, n=1,2, ..., N$，解区并没有变化。

### 解向量和解区
满足$a^{T} y_{n}'>0$（$n=1,2,...,N$）的权向量$a$称为解向量，所有解向量构成的集合称为解区。

### 对解区的限制
为使解向量更可靠，需满足$a^{T} y_{n} ≥b>0$，避免解收敛到解区边界的某点上。

## 感知机准则函数的构建
### 核心目标
寻找解向量$a^*$，使得对于规范化增广样本向量$y_{1}, y_{2}, ..., y_{N}$，满足$a^{T} y_{n}>0, n=1,2, ..., N$。

### 准则函数定义
设$\eta_{k}$是被权向量$a$错分的样本集合，即当$y \in \eta_{k}$时，有$a^{T} y ≤0$。

对于线性可分问题，构造准则函数：
$$J_{P}(a)=\sum_{y \in \eta_{k}}(-a^{T} y)$$

准则函数性质：$J_{P}(a) ≥0$，仅当$a$为解向量或在解区边界时$J_{P}(a)=0$；当且仅当$\eta_{k}$为空集时，$J_{P}^{*}(a)=min J_{P}(a)=0$，此时无错分样本，$a$即为解向量$a^*$。

## 感知机准则函数的求解
### 求解目标
求使$J_{P}(a)$达到最小值的权向量$a^*$。

### 求解方法
采用梯度下降法求解$J_{P}(a)=\sum_{y \in \eta_{k}}(-a^{T} y)$。

### 梯度下降法迭代公式
基于准则函数的梯度计算，逐步更新权向量，直至准则函数收敛到最小值0。

## 感知机准则的收敛性与局限性
### 收敛性
- 在线性可分情形下，感知机准则一定收敛，但收敛结果不唯一。
- 在线性不可分情况下，感知机准则不收敛。

# 3.6 最小二乘准则

## 最小二乘准则的概念

最小二乘法（最小平方误差法）通过最小化误差的平方和寻找数据的最佳函数匹配，即可以使求得的数据与实际数据之间误差的平方和最小。

最小二乘可以用于拟合数据点，找到一条直线（或曲线）使预测值与真实值的误差最小。

在分类问题中，最小二乘用于寻找分类线（决策边界），使分类器的输出与目标值的误差最小。

即使数据不线性可分，也能找到误差平方和最小的解。

## 核心目标与模型形式

### 目标

寻找最优投影方向$a^*$。

### 方程组形式

$$Y a = b$$

其中：

- $y_n$ 是规范化增广向量样本；
- $Y$ 是$N×\hat{d}$维矩阵，通常$N>\hat{d}$，一般为列满秩阵；
- $b$ 是$N$维向量，$b_{n}>0$（$n=1,2, ..., N$），$b_n$是任意给定的正常数；
- 该方程组为方程数多于未知数的矛盾方程组，通常没有精确解。

### 误差向量与准则函数

- 定义误差向量：$e=Y a - b$
- 平方误差准则函数：$J_{SE}(a)=\|e\|^2=\|Y a - b\|^2=\sum_{n=1}^{N}(Y_n a - b_n)^2$

## MSE（均方误差）解与贝叶斯判别函数的关系

MSE线性判别函数与Bayes判别误差最小，即$e^2=\int[a^T x - g_0(x)]^2 p(x) dx$最小。

MSE解$a^*=Y^+ b$使$\frac{1}{N} J_{SE}(a)$最小化，因而也使$e^2$最小化，所以MSE解以最小均方误差逼近贝叶斯判别函数。

## 最小二乘准则的求解

### 求解目标

求使$J_{SE}(a)$最小的$a^*$，即最小二乘近似解/伪逆解/均方误差（MSE）解。

### 方法一：解析法（伪逆解）

对$J_{SE}(a)$求导并令导数为0：
$$\nabla J_{SE}(a)=0$$

解得：

$$Y^T Y a^* = Y^T b$$
$$a^*=(Y^T Y)^{-1} Y^T b = Y^+ b$$

其中：

- $\hat{d}×N$矩阵$Y^+=(Y^T Y)^{-1} Y^T$是$Y$的左逆矩阵；
- 矩阵$Y^T Y$是$\hat{d}×\hat{d}$方阵，一般非奇异，解唯一；
- 当$b$按特定方式选择（$N_1$个对应$\omega_1$类，$N_2$个对应$\omega_2$类），$a^*$等价于Fisher解。

### 方法二：梯度下降法

#### 适用场景

- 避免$Y^T Y$非奇异的要求；
- 减少计算量（仅计算$\hat{d}×\hat{d}$方阵$Y^T Y$，而非$\hat{d}×N$阵$Y^+$）。

#### 求解步骤

1. 初始化权向量$a(1)$，可设为随机值；
2. 迭代更新权向量，学习率$\rho_k=\frac{\rho_1}{k}$（$\rho_1$为任意常数）：
$$a(k+1) = a(k) - \frac{\rho_k}{2} \nabla J_{SE}(a(k))$$

#### 收敛性

该算法权向量收敛于使$\nabla J_{SE}(a)=2 Y^T(Y a - b)=0$的权向量$a^*$。

## 最小二乘准则的局限性

- 对异常值（Outlier）非常敏感；
- 平方误差准则函数会惩罚那些“太正确”的预测，因为它们在决策边界的正确一侧距离太远。

# 3.7 扩展与讨论

## 逻辑回归（Logistic Regression）

### 核心定位

将回归问题转为分类问题，是概率型非线性回归，但其本质是线性回归，只是在特征到结果的映射中加入了一层Sigmoid函数映射。

### 核心原理

1. 映射过程：先把特征线性求和，再使用Sigmoid函数$\sigma$预测。
2. 概率公式：
$$p=\frac{1}{1+e^{-w^{T} x}}$$
3. 对数几率解释：
   - 几率（odds）：相对可能性，若将$p$视为样本$x$作为正例的可能性，则几率为$\frac{p}{1-p}$。
   - 对数几率（log odds，亦称logit）：$\ln \frac{p}{1-p}$，即用线性回归模型的预测结果去逼近真实标记的对数几率，故逻辑回归又称对数几率回归。

### Sigmoid函数与Softmax函数

- Sigmoid函数：$\sigma(z)=\frac{1}{1+\exp(-z)}$，常用于二元回归/分类问题。
- Softmax函数：$\sigma(a)_k=\frac{\exp(a_k)}{\sum_{j=1}^{K}\exp(a_j)}$，其中$a_k=w_k^T x$，是平滑的max函数，适用于多元回归/分类问题。
- 关系：Sigmoid函数是Softmax函数在$K=2$（二分类）时的特殊情形。

### 逻辑回归的求解

1. 数据集：$\{x_{n}, t_{n}\}$，$t_{n} \in \{0,1\}$，$n=1,2, ..., N$。
2. 误差函数：交叉熵（Cross-Entropy）误差函数
$$E(w)=-\ln p(t | w)=-\sum_{n=1}^{N}\left[t_{n} \ln p_n + (1 - t_n) \ln(1 - p_n)\right]$$
其中$p_n=\frac{1}{1+e^{-w^{T} x_n}}$。
3. 梯度与求解：令导数等于零求解参数$w$，梯度表达式为
$$\nabla_{w} \ln p(t | w)=\sum_{n=1}^{N}(t_{n}-p_{n}) x_{n}=0$$
其中 $p_n = \frac{1}{1+e^{-w^T x_n}}$ 是预测概率。

## 二分类和多分类

### 多分类问题求解策略

多分类问题可分解为多个二分类问题求解，常用分解策略：
- 一对一（1 vs. 1）：每次选取两类样本训练一个二分类器，最终通过投票确定类别。
- 一对其余（1 vs. (N-1)）：每次选取一类作为正例，其余所有类作为负例训练二分类器，通过各类器输出确定类别。

## 生成式模型和判别式模型

### 生成式模型（Generative Model）

#### 核心思想

分别对各类的类条件密度$p(x | C_{k})$和先验概率$p(C_{k})$进行建模，之后利用贝叶斯定理计算后验概率：
$$p(C_k | x)=\frac{p(x | C_k)p(C_k)}{\sum_{j=1}^{K}p(x | C_j)p(C_j)}$$

#### 代表算法

- Naive Bayes（朴素贝叶斯）
- Mixtures of Gaussians（高斯混合模型）
- Hidden Markov Models（隐马尔可夫模型）
- Bayesian Networks（贝叶斯网络）
- Deep Belief Network（深度信念网络）

#### 优缺点

- 优点：信息丰富、单类问题灵活性强、支持增量学习、可合成缺失数据。
- 缺点：学习过程复杂、为建模分布牺牲分类性能。

### 判别式模型（Discriminative Model）

#### 核心思想

- 直接对后验概率$p(C_{k} | x)$建模；
- 或直接建模决策函数$f(x)$（无需先验和类条件密度）；
- 或对联合分布$p(x, C_{k})$建模间接得到后验概率。

#### 代表算法

- Linear & Logistic Regression（线性回归与逻辑回归）
- Support Vector Machine（支持向量机）
- Nearest Neighbor（近邻算法）
- Conditional Random Fields（条件随机场）
- Linear-chain CRF（线性链条件随机场）

#### 优缺点

- 优点：类间差异清晰、分类边界灵活、学习过程简单、分类性能较好。
- 缺点：不能反映数据生成特性、需要全部数据进行学习。

### 两者关系

由生成模型可以得到判别式模型，但由判别式模型得不到生成模型。